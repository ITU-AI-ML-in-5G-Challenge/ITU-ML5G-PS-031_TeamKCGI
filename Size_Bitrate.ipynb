{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Size_Bitrate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5I0o9aVbNYk",
        "outputId": "465c84f7-cbc9-4c02-a6ab-4138cb964c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 27 12:56:39 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g_kN59z8xjs",
        "outputId": "03703e43-539b-4e4c-e605-21b4e358ecec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MojrUBy9Nxj",
        "outputId": "9fb0d06d-850a-43a0-b1b7-eb0844543a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGcHfVma9R1G",
        "outputId": "3fd85e09-4252-4b04-fb77-8e55de379925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "#load the dataset\n",
        "vid = pd.read_csv(\"/content/drive/My Drive/video/Training.csv\")\n",
        "#show the data\n",
        "vid.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>ori_file</th>\n",
              "      <th>size</th>\n",
              "      <th>bitrate</th>\n",
              "      <th>size_diff</th>\n",
              "      <th>bitrate_diff</th>\n",
              "      <th>sizediff/ori</th>\n",
              "      <th>bitratediff/ori</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Cls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_0001.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>36.4</td>\n",
              "      <td>1381</td>\n",
              "      <td>11.9</td>\n",
              "      <td>321</td>\n",
              "      <td>0.246377</td>\n",
              "      <td>0.188602</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_001.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>37.3</td>\n",
              "      <td>1414</td>\n",
              "      <td>11.0</td>\n",
              "      <td>288</td>\n",
              "      <td>0.227743</td>\n",
              "      <td>0.169213</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_0025.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>37.5</td>\n",
              "      <td>1419</td>\n",
              "      <td>10.8</td>\n",
              "      <td>283</td>\n",
              "      <td>0.223602</td>\n",
              "      <td>0.166275</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_005.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1440</td>\n",
              "      <td>10.3</td>\n",
              "      <td>262</td>\n",
              "      <td>0.213251</td>\n",
              "      <td>0.153937</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_01.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>37.2</td>\n",
              "      <td>1411</td>\n",
              "      <td>11.1</td>\n",
              "      <td>291</td>\n",
              "      <td>0.229814</td>\n",
              "      <td>0.170975</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            name     ori_file  ...  Condition  Cls\n",
              "0  0GHpTnbnTZs_1100kbps_0001.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "1   0GHpTnbnTZs_1100kbps_001.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "2  0GHpTnbnTZs_1100kbps_0025.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "3   0GHpTnbnTZs_1100kbps_005.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "4    0GHpTnbnTZs_1100kbps_01.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GijQlESk-XS2",
        "outputId": "fae9300a-aae4-48fe-9302-82d6553c9d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split as split\n",
        "train, test = split(vid, test_size = 0.20)\n",
        "print (train.shape)\n",
        "print (test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(288, 10)\n",
            "(72, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGDjZbAh-qQT"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "X_train = train[['size','bitrate','size_diff', 'bitrate_diff']]\n",
        "y_train = train.Cls\n",
        "X_test = test[['size','bitrate','size_diff', 'bitrate_diff']]\n",
        "y_test = test.Cls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYkHMCpfADgp"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=4,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='RMSProp',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31ynfqd-B9xB",
        "outputId": "ac3d7f69-5865-403d-826a-5db5640c7052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = [\n",
        "    ModelCheckpoint('model_best.hdf5',\n",
        "        monitor='val_accuracy', \n",
        "        save_best_only=True, \n",
        "        mode='max',\n",
        "        verbose=1)\n",
        "]\n",
        "hist=model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[checkpoint],epochs=250, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 17.7957 - accuracy: 0.1325\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.13889, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 16.2614 - accuracy: 0.1389 - val_loss: 5.0494 - val_accuracy: 0.1389\n",
            "Epoch 2/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 4.2103 - accuracy: 0.1326\n",
            "Epoch 00002: val_accuracy did not improve from 0.13889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 4.1622 - accuracy: 0.1319 - val_loss: 3.5187 - val_accuracy: 0.1389\n",
            "Epoch 3/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 3.1788 - accuracy: 0.1350\n",
            "Epoch 00003: val_accuracy did not improve from 0.13889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 3.0766 - accuracy: 0.1319 - val_loss: 3.3151 - val_accuracy: 0.0972\n",
            "Epoch 4/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.8920 - accuracy: 0.1151\n",
            "Epoch 00004: val_accuracy did not improve from 0.13889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.8394 - accuracy: 0.1181 - val_loss: 2.5179 - val_accuracy: 0.1250\n",
            "Epoch 5/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 2.6271 - accuracy: 0.1446\n",
            "Epoch 00005: val_accuracy did not improve from 0.13889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.5844 - accuracy: 0.1424 - val_loss: 2.4390 - val_accuracy: 0.1250\n",
            "Epoch 6/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.3309 - accuracy: 0.1250\n",
            "Epoch 00006: val_accuracy improved from 0.13889 to 0.20833, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.3481 - accuracy: 0.1181 - val_loss: 2.2579 - val_accuracy: 0.2083\n",
            "Epoch 7/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.3230 - accuracy: 0.1373\n",
            "Epoch 00007: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.3408 - accuracy: 0.1389 - val_loss: 2.4317 - val_accuracy: 0.1111\n",
            "Epoch 8/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.2893 - accuracy: 0.1629\n",
            "Epoch 00008: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2944 - accuracy: 0.1667 - val_loss: 2.1908 - val_accuracy: 0.0694\n",
            "Epoch 9/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 2.2662 - accuracy: 0.1011\n",
            "Epoch 00009: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2666 - accuracy: 0.1076 - val_loss: 2.1590 - val_accuracy: 0.1111\n",
            "Epoch 10/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.2009 - accuracy: 0.1402\n",
            "Epoch 00010: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1994 - accuracy: 0.1319 - val_loss: 2.2116 - val_accuracy: 0.1250\n",
            "Epoch 11/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.2013 - accuracy: 0.1629\n",
            "Epoch 00011: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1994 - accuracy: 0.1667 - val_loss: 2.2058 - val_accuracy: 0.1111\n",
            "Epoch 12/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.1690 - accuracy: 0.1705\n",
            "Epoch 00012: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1659 - accuracy: 0.1736 - val_loss: 2.1570 - val_accuracy: 0.1806\n",
            "Epoch 13/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 2.1396 - accuracy: 0.1985\n",
            "Epoch 00013: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1367 - accuracy: 0.1979 - val_loss: 2.2494 - val_accuracy: 0.0694\n",
            "Epoch 14/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 2.1671 - accuracy: 0.1605\n",
            "Epoch 00014: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1717 - accuracy: 0.1667 - val_loss: 2.2260 - val_accuracy: 0.1944\n",
            "Epoch 15/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.1690 - accuracy: 0.1608\n",
            "Epoch 00015: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1595 - accuracy: 0.1597 - val_loss: 2.1924 - val_accuracy: 0.1250\n",
            "Epoch 16/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 2.1082 - accuracy: 0.2097\n",
            "Epoch 00016: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1231 - accuracy: 0.1979 - val_loss: 2.1717 - val_accuracy: 0.1111\n",
            "Epoch 17/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.1174 - accuracy: 0.1894\n",
            "Epoch 00017: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1228 - accuracy: 0.1944 - val_loss: 2.1560 - val_accuracy: 0.1111\n",
            "Epoch 18/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.1071 - accuracy: 0.1724\n",
            "Epoch 00018: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1238 - accuracy: 0.1632 - val_loss: 2.2084 - val_accuracy: 0.1389\n",
            "Epoch 19/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.1035 - accuracy: 0.1705\n",
            "Epoch 00019: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0890 - accuracy: 0.1736 - val_loss: 2.2167 - val_accuracy: 0.0972\n",
            "Epoch 20/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.1244 - accuracy: 0.1705\n",
            "Epoch 00020: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1156 - accuracy: 0.1806 - val_loss: 2.3638 - val_accuracy: 0.0833\n",
            "Epoch 21/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.1281 - accuracy: 0.1364\n",
            "Epoch 00021: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1247 - accuracy: 0.1354 - val_loss: 2.1929 - val_accuracy: 0.1250\n",
            "Epoch 22/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.0941 - accuracy: 0.1553\n",
            "Epoch 00022: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1052 - accuracy: 0.1597 - val_loss: 2.1544 - val_accuracy: 0.1250\n",
            "Epoch 23/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.0686 - accuracy: 0.1860\n",
            "Epoch 00023: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0940 - accuracy: 0.1701 - val_loss: 2.1504 - val_accuracy: 0.1250\n",
            "Epoch 24/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.1203 - accuracy: 0.1494\n",
            "Epoch 00024: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1026 - accuracy: 0.1597 - val_loss: 2.1799 - val_accuracy: 0.1111\n",
            "Epoch 25/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 2.0819 - accuracy: 0.1829\n",
            "Epoch 00025: val_accuracy did not improve from 0.20833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0851 - accuracy: 0.1736 - val_loss: 2.1411 - val_accuracy: 0.0694\n",
            "Epoch 26/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.0721 - accuracy: 0.1992\n",
            "Epoch 00026: val_accuracy improved from 0.20833 to 0.23611, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0643 - accuracy: 0.1944 - val_loss: 2.1524 - val_accuracy: 0.2361\n",
            "Epoch 27/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.0416 - accuracy: 0.1801\n",
            "Epoch 00027: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0318 - accuracy: 0.1944 - val_loss: 2.5081 - val_accuracy: 0.0833\n",
            "Epoch 28/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 2.0531 - accuracy: 0.2073\n",
            "Epoch 00028: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0627 - accuracy: 0.2014 - val_loss: 2.1400 - val_accuracy: 0.1944\n",
            "Epoch 29/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.0831 - accuracy: 0.1922\n",
            "Epoch 00029: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0776 - accuracy: 0.1979 - val_loss: 2.1767 - val_accuracy: 0.1111\n",
            "Epoch 30/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.0322 - accuracy: 0.2146\n",
            "Epoch 00030: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0579 - accuracy: 0.2049 - val_loss: 2.4844 - val_accuracy: 0.1111\n",
            "Epoch 31/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.1030 - accuracy: 0.2008\n",
            "Epoch 00031: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0879 - accuracy: 0.2118 - val_loss: 2.1820 - val_accuracy: 0.0972\n",
            "Epoch 32/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 2.0913 - accuracy: 0.1687\n",
            "Epoch 00032: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0881 - accuracy: 0.1736 - val_loss: 2.1724 - val_accuracy: 0.1111\n",
            "Epoch 33/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.0324 - accuracy: 0.2222\n",
            "Epoch 00033: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0435 - accuracy: 0.2188 - val_loss: 2.1297 - val_accuracy: 0.1111\n",
            "Epoch 34/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.0926 - accuracy: 0.2045\n",
            "Epoch 00034: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0859 - accuracy: 0.2083 - val_loss: 2.1649 - val_accuracy: 0.1250\n",
            "Epoch 35/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.0124 - accuracy: 0.2381\n",
            "Epoch 00035: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0359 - accuracy: 0.2257 - val_loss: 2.2006 - val_accuracy: 0.1528\n",
            "Epoch 36/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.9921 - accuracy: 0.2302\n",
            "Epoch 00036: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9882 - accuracy: 0.2257 - val_loss: 2.3953 - val_accuracy: 0.1389\n",
            "Epoch 37/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.0622 - accuracy: 0.2045\n",
            "Epoch 00037: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0502 - accuracy: 0.2083 - val_loss: 2.1125 - val_accuracy: 0.1528\n",
            "Epoch 38/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.0622 - accuracy: 0.1843\n",
            "Epoch 00038: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0523 - accuracy: 0.1910 - val_loss: 2.0641 - val_accuracy: 0.1250\n",
            "Epoch 39/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.0267 - accuracy: 0.2235\n",
            "Epoch 00039: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0122 - accuracy: 0.2222 - val_loss: 2.1113 - val_accuracy: 0.1528\n",
            "Epoch 40/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 2.0511 - accuracy: 0.2135\n",
            "Epoch 00040: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0415 - accuracy: 0.2222 - val_loss: 2.1662 - val_accuracy: 0.1528\n",
            "Epoch 41/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.0898 - accuracy: 0.2016\n",
            "Epoch 00041: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0492 - accuracy: 0.2188 - val_loss: 2.0844 - val_accuracy: 0.1667\n",
            "Epoch 42/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.9446 - accuracy: 0.2348\n",
            "Epoch 00042: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9667 - accuracy: 0.2257 - val_loss: 2.2176 - val_accuracy: 0.1667\n",
            "Epoch 43/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 2.0360 - accuracy: 0.2305\n",
            "Epoch 00043: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0311 - accuracy: 0.2326 - val_loss: 2.0661 - val_accuracy: 0.2083\n",
            "Epoch 44/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.9667 - accuracy: 0.2519\n",
            "Epoch 00044: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9625 - accuracy: 0.2431 - val_loss: 2.4223 - val_accuracy: 0.1250\n",
            "Epoch 45/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 2.0475 - accuracy: 0.2135\n",
            "Epoch 00045: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0378 - accuracy: 0.2118 - val_loss: 2.0644 - val_accuracy: 0.2222\n",
            "Epoch 46/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.9861 - accuracy: 0.2287\n",
            "Epoch 00046: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0012 - accuracy: 0.2326 - val_loss: 2.1080 - val_accuracy: 0.0972\n",
            "Epoch 47/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 2.0022 - accuracy: 0.2265\n",
            "Epoch 00047: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9945 - accuracy: 0.2326 - val_loss: 2.0712 - val_accuracy: 0.1528\n",
            "Epoch 48/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.9817 - accuracy: 0.2263\n",
            "Epoch 00048: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0294 - accuracy: 0.2222 - val_loss: 2.0423 - val_accuracy: 0.1806\n",
            "Epoch 49/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.9718 - accuracy: 0.2583\n",
            "Epoch 00049: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9364 - accuracy: 0.2639 - val_loss: 2.1945 - val_accuracy: 0.1528\n",
            "Epoch 50/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.9933 - accuracy: 0.2439\n",
            "Epoch 00050: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9878 - accuracy: 0.2431 - val_loss: 2.2287 - val_accuracy: 0.1111\n",
            "Epoch 51/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.9869 - accuracy: 0.2045\n",
            "Epoch 00051: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9993 - accuracy: 0.1979 - val_loss: 2.0984 - val_accuracy: 0.1250\n",
            "Epoch 52/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.9782 - accuracy: 0.2167\n",
            "Epoch 00052: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9544 - accuracy: 0.2257 - val_loss: 2.0409 - val_accuracy: 0.1667\n",
            "Epoch 53/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.9622 - accuracy: 0.2248\n",
            "Epoch 00053: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9442 - accuracy: 0.2326 - val_loss: 2.0360 - val_accuracy: 0.1667\n",
            "Epoch 54/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.9985 - accuracy: 0.2249\n",
            "Epoch 00054: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0213 - accuracy: 0.2153 - val_loss: 2.0514 - val_accuracy: 0.1806\n",
            "Epoch 55/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.9416 - accuracy: 0.2460\n",
            "Epoch 00055: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9856 - accuracy: 0.2431 - val_loss: 2.0437 - val_accuracy: 0.1389\n",
            "Epoch 56/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.9806 - accuracy: 0.2584\n",
            "Epoch 00056: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9824 - accuracy: 0.2500 - val_loss: 2.1416 - val_accuracy: 0.1806\n",
            "Epoch 57/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.0224 - accuracy: 0.1865\n",
            "Epoch 00057: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9868 - accuracy: 0.1979 - val_loss: 2.0765 - val_accuracy: 0.1111\n",
            "Epoch 58/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.9770 - accuracy: 0.2299\n",
            "Epoch 00058: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9709 - accuracy: 0.2326 - val_loss: 2.0848 - val_accuracy: 0.1667\n",
            "Epoch 59/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.9545 - accuracy: 0.2734\n",
            "Epoch 00059: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9455 - accuracy: 0.2778 - val_loss: 2.3027 - val_accuracy: 0.1806\n",
            "Epoch 60/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.9083 - accuracy: 0.2549\n",
            "Epoch 00060: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9250 - accuracy: 0.2431 - val_loss: 2.0747 - val_accuracy: 0.1250\n",
            "Epoch 61/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.9558 - accuracy: 0.2364\n",
            "Epoch 00061: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9391 - accuracy: 0.2431 - val_loss: 2.1019 - val_accuracy: 0.1111\n",
            "Epoch 62/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.9395 - accuracy: 0.2584\n",
            "Epoch 00062: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9494 - accuracy: 0.2465 - val_loss: 2.1990 - val_accuracy: 0.1528\n",
            "Epoch 63/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.9957 - accuracy: 0.2146\n",
            "Epoch 00063: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9738 - accuracy: 0.2257 - val_loss: 2.0659 - val_accuracy: 0.1667\n",
            "Epoch 64/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.9003 - accuracy: 0.2458\n",
            "Epoch 00064: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9105 - accuracy: 0.2465 - val_loss: 2.0735 - val_accuracy: 0.1389\n",
            "Epoch 65/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.9344 - accuracy: 0.2353\n",
            "Epoch 00065: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9841 - accuracy: 0.2361 - val_loss: 2.0707 - val_accuracy: 0.1528\n",
            "Epoch 66/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.0065 - accuracy: 0.2576\n",
            "Epoch 00066: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9949 - accuracy: 0.2535 - val_loss: 2.0318 - val_accuracy: 0.1667\n",
            "Epoch 67/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.8810 - accuracy: 0.2519\n",
            "Epoch 00067: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8961 - accuracy: 0.2431 - val_loss: 2.0304 - val_accuracy: 0.1667\n",
            "Epoch 68/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.9503 - accuracy: 0.2549\n",
            "Epoch 00068: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9347 - accuracy: 0.2396 - val_loss: 1.9822 - val_accuracy: 0.1806\n",
            "Epoch 69/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.9074 - accuracy: 0.2442\n",
            "Epoch 00069: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9221 - accuracy: 0.2361 - val_loss: 2.0032 - val_accuracy: 0.1944\n",
            "Epoch 70/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.9053 - accuracy: 0.2614\n",
            "Epoch 00070: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9579 - accuracy: 0.2604 - val_loss: 1.9999 - val_accuracy: 0.1528\n",
            "Epoch 71/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.8927 - accuracy: 0.2519\n",
            "Epoch 00071: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8913 - accuracy: 0.2639 - val_loss: 2.0029 - val_accuracy: 0.1667\n",
            "Epoch 72/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.9398 - accuracy: 0.2341\n",
            "Epoch 00072: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9102 - accuracy: 0.2396 - val_loss: 2.0548 - val_accuracy: 0.1389\n",
            "Epoch 73/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.9500 - accuracy: 0.2490\n",
            "Epoch 00073: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9424 - accuracy: 0.2535 - val_loss: 2.0343 - val_accuracy: 0.1250\n",
            "Epoch 74/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 1.9074 - accuracy: 0.2747\n",
            "Epoch 00074: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9080 - accuracy: 0.2674 - val_loss: 1.9888 - val_accuracy: 0.2083\n",
            "Epoch 75/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.8128 - accuracy: 0.2950\n",
            "Epoch 00075: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8182 - accuracy: 0.2882 - val_loss: 1.9604 - val_accuracy: 0.1944\n",
            "Epoch 76/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.8279 - accuracy: 0.2902\n",
            "Epoch 00076: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8275 - accuracy: 0.2917 - val_loss: 1.9833 - val_accuracy: 0.2222\n",
            "Epoch 77/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 1.8892 - accuracy: 0.2754\n",
            "Epoch 00077: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8935 - accuracy: 0.2708 - val_loss: 1.9957 - val_accuracy: 0.1389\n",
            "Epoch 78/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.8397 - accuracy: 0.2846\n",
            "Epoch 00078: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8152 - accuracy: 0.2812 - val_loss: 2.0066 - val_accuracy: 0.2222\n",
            "Epoch 79/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.7180 - accuracy: 0.2745\n",
            "Epoch 00079: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7309 - accuracy: 0.2639 - val_loss: 1.7492 - val_accuracy: 0.2222\n",
            "Epoch 80/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.7149 - accuracy: 0.3409\n",
            "Epoch 00080: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7426 - accuracy: 0.3299 - val_loss: 1.8282 - val_accuracy: 0.1528\n",
            "Epoch 81/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.7835 - accuracy: 0.3371\n",
            "Epoch 00081: val_accuracy did not improve from 0.23611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7903 - accuracy: 0.3333 - val_loss: 1.8356 - val_accuracy: 0.1806\n",
            "Epoch 82/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.7063 - accuracy: 0.3068\n",
            "Epoch 00082: val_accuracy improved from 0.23611 to 0.26389, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7050 - accuracy: 0.2951 - val_loss: 1.8837 - val_accuracy: 0.2639\n",
            "Epoch 83/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.8209 - accuracy: 0.3292\n",
            "Epoch 00083: val_accuracy improved from 0.26389 to 0.29167, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7873 - accuracy: 0.3299 - val_loss: 1.7144 - val_accuracy: 0.2917\n",
            "Epoch 84/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.6839 - accuracy: 0.3446\n",
            "Epoch 00084: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7137 - accuracy: 0.3403 - val_loss: 2.0493 - val_accuracy: 0.2222\n",
            "Epoch 85/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.6996 - accuracy: 0.3333\n",
            "Epoch 00085: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.6734 - accuracy: 0.3368 - val_loss: 1.6940 - val_accuracy: 0.2222\n",
            "Epoch 86/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.6818 - accuracy: 0.3527\n",
            "Epoch 00086: val_accuracy improved from 0.29167 to 0.34722, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.6555 - accuracy: 0.3576 - val_loss: 1.6227 - val_accuracy: 0.3472\n",
            "Epoch 87/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.6063 - accuracy: 0.3372\n",
            "Epoch 00087: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.6068 - accuracy: 0.3299 - val_loss: 1.7014 - val_accuracy: 0.3194\n",
            "Epoch 88/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 1.7010 - accuracy: 0.3261\n",
            "Epoch 00088: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.6782 - accuracy: 0.3333 - val_loss: 1.6110 - val_accuracy: 0.3472\n",
            "Epoch 89/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.6197 - accuracy: 0.3574\n",
            "Epoch 00089: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5911 - accuracy: 0.3611 - val_loss: 1.5761 - val_accuracy: 0.3472\n",
            "Epoch 90/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.6079 - accuracy: 0.3485\n",
            "Epoch 00090: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5851 - accuracy: 0.3576 - val_loss: 1.7126 - val_accuracy: 0.2500\n",
            "Epoch 91/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.5545 - accuracy: 0.3446\n",
            "Epoch 00091: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5483 - accuracy: 0.3611 - val_loss: 1.6162 - val_accuracy: 0.2917\n",
            "Epoch 92/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.5188 - accuracy: 0.3561\n",
            "Epoch 00092: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5252 - accuracy: 0.3542 - val_loss: 1.5591 - val_accuracy: 0.3333\n",
            "Epoch 93/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.5680 - accuracy: 0.3760\n",
            "Epoch 00093: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5691 - accuracy: 0.3750 - val_loss: 1.5387 - val_accuracy: 0.3056\n",
            "Epoch 94/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.5644 - accuracy: 0.4000\n",
            "Epoch 00094: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5495 - accuracy: 0.4097 - val_loss: 2.0458 - val_accuracy: 0.2778\n",
            "Epoch 95/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.5554 - accuracy: 0.3633\n",
            "Epoch 00095: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5341 - accuracy: 0.3715 - val_loss: 1.6523 - val_accuracy: 0.2778\n",
            "Epoch 96/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.4721 - accuracy: 0.3870\n",
            "Epoch 00096: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4574 - accuracy: 0.3958 - val_loss: 1.6318 - val_accuracy: 0.3194\n",
            "Epoch 97/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.4540 - accuracy: 0.4023\n",
            "Epoch 00097: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4726 - accuracy: 0.3924 - val_loss: 1.8168 - val_accuracy: 0.2639\n",
            "Epoch 98/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.5449 - accuracy: 0.3855\n",
            "Epoch 00098: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5800 - accuracy: 0.3715 - val_loss: 1.5319 - val_accuracy: 0.3472\n",
            "Epoch 99/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.4438 - accuracy: 0.4345\n",
            "Epoch 00099: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4380 - accuracy: 0.4514 - val_loss: 1.7526 - val_accuracy: 0.3056\n",
            "Epoch 100/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.4986 - accuracy: 0.4157\n",
            "Epoch 00100: val_accuracy improved from 0.34722 to 0.37500, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5026 - accuracy: 0.4028 - val_loss: 1.6974 - val_accuracy: 0.3750\n",
            "Epoch 101/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.6038 - accuracy: 0.3775\n",
            "Epoch 00101: val_accuracy improved from 0.37500 to 0.41667, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5448 - accuracy: 0.3958 - val_loss: 1.5079 - val_accuracy: 0.4167\n",
            "Epoch 102/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.5008 - accuracy: 0.4382\n",
            "Epoch 00102: val_accuracy did not improve from 0.41667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4905 - accuracy: 0.4410 - val_loss: 1.4031 - val_accuracy: 0.3750\n",
            "Epoch 103/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.4293 - accuracy: 0.4356\n",
            "Epoch 00103: val_accuracy did not improve from 0.41667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4023 - accuracy: 0.4514 - val_loss: 1.6179 - val_accuracy: 0.3611\n",
            "Epoch 104/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.3957 - accuracy: 0.4264\n",
            "Epoch 00104: val_accuracy did not improve from 0.41667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4046 - accuracy: 0.4167 - val_loss: 1.5560 - val_accuracy: 0.3472\n",
            "Epoch 105/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.4798 - accuracy: 0.4302\n",
            "Epoch 00105: val_accuracy improved from 0.41667 to 0.45833, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4719 - accuracy: 0.4306 - val_loss: 1.4582 - val_accuracy: 0.4583\n",
            "Epoch 106/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.3940 - accuracy: 0.4519\n",
            "Epoch 00106: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3490 - accuracy: 0.4722 - val_loss: 1.5018 - val_accuracy: 0.3750\n",
            "Epoch 107/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.5042 - accuracy: 0.4330\n",
            "Epoch 00107: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4754 - accuracy: 0.4410 - val_loss: 1.8055 - val_accuracy: 0.2361\n",
            "Epoch 108/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.3800 - accuracy: 0.4621\n",
            "Epoch 00108: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3766 - accuracy: 0.4618 - val_loss: 1.6636 - val_accuracy: 0.2500\n",
            "Epoch 109/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.4239 - accuracy: 0.4494\n",
            "Epoch 00109: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4143 - accuracy: 0.4514 - val_loss: 1.4983 - val_accuracy: 0.4028\n",
            "Epoch 110/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.3219 - accuracy: 0.4667\n",
            "Epoch 00110: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3283 - accuracy: 0.4549 - val_loss: 1.7913 - val_accuracy: 0.3750\n",
            "Epoch 111/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.3957 - accuracy: 0.4378\n",
            "Epoch 00111: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4651 - accuracy: 0.4340 - val_loss: 1.5326 - val_accuracy: 0.3750\n",
            "Epoch 112/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.4422 - accuracy: 0.4444\n",
            "Epoch 00112: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4436 - accuracy: 0.4410 - val_loss: 1.3902 - val_accuracy: 0.4444\n",
            "Epoch 113/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.3505 - accuracy: 0.4365\n",
            "Epoch 00113: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3390 - accuracy: 0.4444 - val_loss: 1.5285 - val_accuracy: 0.4028\n",
            "Epoch 114/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.3704 - accuracy: 0.4583\n",
            "Epoch 00114: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3549 - accuracy: 0.4653 - val_loss: 1.5280 - val_accuracy: 0.3333\n",
            "Epoch 115/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.4363 - accuracy: 0.4904\n",
            "Epoch 00115: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4396 - accuracy: 0.4792 - val_loss: 1.5923 - val_accuracy: 0.3472\n",
            "Epoch 116/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.3407 - accuracy: 0.4729\n",
            "Epoch 00116: val_accuracy did not improve from 0.45833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3337 - accuracy: 0.4757 - val_loss: 1.4765 - val_accuracy: 0.3889\n",
            "Epoch 117/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.4283 - accuracy: 0.4682\n",
            "Epoch 00117: val_accuracy improved from 0.45833 to 0.48611, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4168 - accuracy: 0.4688 - val_loss: 1.3947 - val_accuracy: 0.4861\n",
            "Epoch 118/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.3129 - accuracy: 0.4943\n",
            "Epoch 00118: val_accuracy did not improve from 0.48611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3370 - accuracy: 0.4861 - val_loss: 1.9266 - val_accuracy: 0.2778\n",
            "Epoch 119/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.3096 - accuracy: 0.4521\n",
            "Epoch 00119: val_accuracy did not improve from 0.48611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3470 - accuracy: 0.4444 - val_loss: 1.4624 - val_accuracy: 0.4167\n",
            "Epoch 120/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.4068 - accuracy: 0.4508\n",
            "Epoch 00120: val_accuracy did not improve from 0.48611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4325 - accuracy: 0.4410 - val_loss: 1.3905 - val_accuracy: 0.3611\n",
            "Epoch 121/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.2898 - accuracy: 0.5076\n",
            "Epoch 00121: val_accuracy improved from 0.48611 to 0.51389, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2944 - accuracy: 0.5035 - val_loss: 1.2691 - val_accuracy: 0.5139\n",
            "Epoch 122/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.3846 - accuracy: 0.4535\n",
            "Epoch 00122: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3896 - accuracy: 0.4514 - val_loss: 1.3996 - val_accuracy: 0.4167\n",
            "Epoch 123/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.3520 - accuracy: 0.4510\n",
            "Epoch 00123: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3306 - accuracy: 0.4583 - val_loss: 1.4388 - val_accuracy: 0.4028\n",
            "Epoch 124/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.3844 - accuracy: 0.4559\n",
            "Epoch 00124: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4208 - accuracy: 0.4514 - val_loss: 1.6087 - val_accuracy: 0.3194\n",
            "Epoch 125/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.4281 - accuracy: 0.4719\n",
            "Epoch 00125: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3860 - accuracy: 0.4896 - val_loss: 1.4308 - val_accuracy: 0.4028\n",
            "Epoch 126/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.3809 - accuracy: 0.4733\n",
            "Epoch 00126: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3012 - accuracy: 0.4965 - val_loss: 1.3632 - val_accuracy: 0.4167\n",
            "Epoch 127/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.3210 - accuracy: 0.4961\n",
            "Epoch 00127: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2998 - accuracy: 0.4965 - val_loss: 1.3944 - val_accuracy: 0.4028\n",
            "Epoch 128/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.3144 - accuracy: 0.4904\n",
            "Epoch 00128: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3481 - accuracy: 0.4722 - val_loss: 1.5473 - val_accuracy: 0.3750\n",
            "Epoch 129/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.2446 - accuracy: 0.5296\n",
            "Epoch 00129: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2543 - accuracy: 0.5208 - val_loss: 2.2547 - val_accuracy: 0.2917\n",
            "Epoch 130/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.2990 - accuracy: 0.4963\n",
            "Epoch 00130: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3032 - accuracy: 0.4896 - val_loss: 1.2247 - val_accuracy: 0.5000\n",
            "Epoch 131/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.2869 - accuracy: 0.5000\n",
            "Epoch 00131: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2930 - accuracy: 0.4931 - val_loss: 1.2864 - val_accuracy: 0.4167\n",
            "Epoch 132/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.2916 - accuracy: 0.5185\n",
            "Epoch 00132: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2898 - accuracy: 0.5174 - val_loss: 3.5234 - val_accuracy: 0.2639\n",
            "Epoch 133/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.3396 - accuracy: 0.5059\n",
            "Epoch 00133: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3113 - accuracy: 0.5104 - val_loss: 1.2190 - val_accuracy: 0.4722\n",
            "Epoch 134/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.3371 - accuracy: 0.4651\n",
            "Epoch 00134: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3475 - accuracy: 0.4653 - val_loss: 1.2841 - val_accuracy: 0.4722\n",
            "Epoch 135/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.2324 - accuracy: 0.4778\n",
            "Epoch 00135: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2558 - accuracy: 0.4826 - val_loss: 1.3595 - val_accuracy: 0.4583\n",
            "Epoch 136/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.3071 - accuracy: 0.4719\n",
            "Epoch 00136: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2990 - accuracy: 0.4688 - val_loss: 1.2467 - val_accuracy: 0.4444\n",
            "Epoch 137/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 1.2123 - accuracy: 0.5604\n",
            "Epoch 00137: val_accuracy did not improve from 0.51389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2390 - accuracy: 0.5451 - val_loss: 1.4597 - val_accuracy: 0.3472\n",
            "Epoch 138/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.3697 - accuracy: 0.5181\n",
            "Epoch 00138: val_accuracy improved from 0.51389 to 0.52778, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3297 - accuracy: 0.5243 - val_loss: 1.0845 - val_accuracy: 0.5278\n",
            "Epoch 139/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.2907 - accuracy: 0.5056\n",
            "Epoch 00139: val_accuracy did not improve from 0.52778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2831 - accuracy: 0.5069 - val_loss: 1.2933 - val_accuracy: 0.5278\n",
            "Epoch 140/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.2965 - accuracy: 0.5038\n",
            "Epoch 00140: val_accuracy did not improve from 0.52778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3045 - accuracy: 0.5000 - val_loss: 1.2376 - val_accuracy: 0.4583\n",
            "Epoch 141/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.2406 - accuracy: 0.5172\n",
            "Epoch 00141: val_accuracy did not improve from 0.52778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2213 - accuracy: 0.5243 - val_loss: 1.1593 - val_accuracy: 0.5139\n",
            "Epoch 142/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.1360 - accuracy: 0.5198\n",
            "Epoch 00142: val_accuracy did not improve from 0.52778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1155 - accuracy: 0.5278 - val_loss: 1.7853 - val_accuracy: 0.3750\n",
            "Epoch 143/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.3125 - accuracy: 0.4884\n",
            "Epoch 00143: val_accuracy did not improve from 0.52778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3160 - accuracy: 0.4896 - val_loss: 2.5208 - val_accuracy: 0.2500\n",
            "Epoch 144/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.1620 - accuracy: 0.5287\n",
            "Epoch 00144: val_accuracy improved from 0.52778 to 0.58333, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1260 - accuracy: 0.5382 - val_loss: 1.0616 - val_accuracy: 0.5833\n",
            "Epoch 145/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.1715 - accuracy: 0.5078\n",
            "Epoch 00145: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1818 - accuracy: 0.5000 - val_loss: 2.0648 - val_accuracy: 0.3333\n",
            "Epoch 146/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.3793 - accuracy: 0.5019\n",
            "Epoch 00146: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3428 - accuracy: 0.5208 - val_loss: 1.4276 - val_accuracy: 0.3750\n",
            "Epoch 147/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.1608 - accuracy: 0.5519\n",
            "Epoch 00147: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1539 - accuracy: 0.5625 - val_loss: 1.8210 - val_accuracy: 0.3333\n",
            "Epoch 148/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.0704 - accuracy: 0.5437\n",
            "Epoch 00148: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0920 - accuracy: 0.5451 - val_loss: 1.5258 - val_accuracy: 0.4583\n",
            "Epoch 149/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.3172 - accuracy: 0.5152\n",
            "Epoch 00149: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2937 - accuracy: 0.5208 - val_loss: 1.3025 - val_accuracy: 0.4861\n",
            "Epoch 150/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.0924 - accuracy: 0.5594\n",
            "Epoch 00150: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0989 - accuracy: 0.5625 - val_loss: 1.2124 - val_accuracy: 0.4722\n",
            "Epoch 151/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.0830 - accuracy: 0.5426\n",
            "Epoch 00151: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0941 - accuracy: 0.5382 - val_loss: 1.3354 - val_accuracy: 0.4167\n",
            "Epoch 152/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.1195 - accuracy: 0.5543\n",
            "Epoch 00152: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1382 - accuracy: 0.5417 - val_loss: 1.6068 - val_accuracy: 0.4028\n",
            "Epoch 153/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.3037 - accuracy: 0.5853\n",
            "Epoch 00153: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2597 - accuracy: 0.5972 - val_loss: 1.2449 - val_accuracy: 0.4861\n",
            "Epoch 154/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.1086 - accuracy: 0.5824\n",
            "Epoch 00154: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1098 - accuracy: 0.5903 - val_loss: 0.9761 - val_accuracy: 0.5833\n",
            "Epoch 155/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.1024 - accuracy: 0.5373\n",
            "Epoch 00155: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1042 - accuracy: 0.5451 - val_loss: 1.5734 - val_accuracy: 0.3889\n",
            "Epoch 156/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.1651 - accuracy: 0.5326\n",
            "Epoch 00156: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1557 - accuracy: 0.5382 - val_loss: 1.8379 - val_accuracy: 0.4028\n",
            "Epoch 157/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.1334 - accuracy: 0.5556\n",
            "Epoch 00157: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1558 - accuracy: 0.5486 - val_loss: 1.0086 - val_accuracy: 0.5417\n",
            "Epoch 158/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 1.1390 - accuracy: 0.5678\n",
            "Epoch 00158: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1132 - accuracy: 0.5764 - val_loss: 0.9073 - val_accuracy: 0.5694\n",
            "Epoch 159/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.1343 - accuracy: 0.5516\n",
            "Epoch 00159: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0861 - accuracy: 0.5694 - val_loss: 1.0621 - val_accuracy: 0.5417\n",
            "Epoch 160/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.0797 - accuracy: 0.5930\n",
            "Epoch 00160: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1317 - accuracy: 0.5694 - val_loss: 1.5091 - val_accuracy: 0.4722\n",
            "Epoch 161/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.0170 - accuracy: 0.6367\n",
            "Epoch 00161: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0138 - accuracy: 0.6285 - val_loss: 0.9411 - val_accuracy: 0.5556\n",
            "Epoch 162/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.0788 - accuracy: 0.5871\n",
            "Epoch 00162: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0926 - accuracy: 0.5868 - val_loss: 1.2210 - val_accuracy: 0.5556\n",
            "Epoch 163/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.0245 - accuracy: 0.5853\n",
            "Epoch 00163: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0382 - accuracy: 0.5833 - val_loss: 0.9748 - val_accuracy: 0.5139\n",
            "Epoch 164/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.0332 - accuracy: 0.5693\n",
            "Epoch 00164: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0577 - accuracy: 0.5660 - val_loss: 1.7877 - val_accuracy: 0.4583\n",
            "Epoch 165/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 1.1484 - accuracy: 0.5725\n",
            "Epoch 00165: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1325 - accuracy: 0.5799 - val_loss: 1.2845 - val_accuracy: 0.5139\n",
            "Epoch 166/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.0559 - accuracy: 0.6092\n",
            "Epoch 00166: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0933 - accuracy: 0.6007 - val_loss: 1.8095 - val_accuracy: 0.5139\n",
            "Epoch 167/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.0519 - accuracy: 0.5900\n",
            "Epoch 00167: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0839 - accuracy: 0.5833 - val_loss: 1.2580 - val_accuracy: 0.5278\n",
            "Epoch 168/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.9545 - accuracy: 0.6111\n",
            "Epoch 00168: val_accuracy did not improve from 0.58333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9654 - accuracy: 0.6111 - val_loss: 1.3340 - val_accuracy: 0.4028\n",
            "Epoch 169/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.0653 - accuracy: 0.5725\n",
            "Epoch 00169: val_accuracy improved from 0.58333 to 0.61111, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0045 - accuracy: 0.5903 - val_loss: 0.9414 - val_accuracy: 0.6111\n",
            "Epoch 170/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.0798 - accuracy: 0.6061\n",
            "Epoch 00170: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0862 - accuracy: 0.5972 - val_loss: 1.2468 - val_accuracy: 0.5694\n",
            "Epoch 171/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.1988 - accuracy: 0.6015\n",
            "Epoch 00171: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1671 - accuracy: 0.5972 - val_loss: 0.9064 - val_accuracy: 0.5833\n",
            "Epoch 172/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.0663 - accuracy: 0.6105\n",
            "Epoch 00172: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0771 - accuracy: 0.5938 - val_loss: 1.1278 - val_accuracy: 0.5278\n",
            "Epoch 173/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.0622 - accuracy: 0.6202\n",
            "Epoch 00173: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0792 - accuracy: 0.6042 - val_loss: 1.4069 - val_accuracy: 0.5694\n",
            "Epoch 174/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 1.0051 - accuracy: 0.5824\n",
            "Epoch 00174: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9758 - accuracy: 0.5972 - val_loss: 1.4975 - val_accuracy: 0.5278\n",
            "Epoch 175/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 1.0994 - accuracy: 0.5824\n",
            "Epoch 00175: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1666 - accuracy: 0.5660 - val_loss: 1.6504 - val_accuracy: 0.4306\n",
            "Epoch 176/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.0297 - accuracy: 0.5852\n",
            "Epoch 00176: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0136 - accuracy: 0.5938 - val_loss: 1.6817 - val_accuracy: 0.4306\n",
            "Epoch 177/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.0267 - accuracy: 0.6000\n",
            "Epoch 00177: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0288 - accuracy: 0.6007 - val_loss: 1.3944 - val_accuracy: 0.4722\n",
            "Epoch 178/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.1349 - accuracy: 0.6163\n",
            "Epoch 00178: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0838 - accuracy: 0.6250 - val_loss: 1.2549 - val_accuracy: 0.5833\n",
            "Epoch 179/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.0599 - accuracy: 0.5926\n",
            "Epoch 00179: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0417 - accuracy: 0.6007 - val_loss: 1.9972 - val_accuracy: 0.4306\n",
            "Epoch 180/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.9582 - accuracy: 0.6074\n",
            "Epoch 00180: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9753 - accuracy: 0.6042 - val_loss: 1.5591 - val_accuracy: 0.4861\n",
            "Epoch 181/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.9771 - accuracy: 0.6123\n",
            "Epoch 00181: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9810 - accuracy: 0.6111 - val_loss: 1.0444 - val_accuracy: 0.5556\n",
            "Epoch 182/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.9488 - accuracy: 0.6240\n",
            "Epoch 00182: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0184 - accuracy: 0.6007 - val_loss: 1.4955 - val_accuracy: 0.4444\n",
            "Epoch 183/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.8766 - accuracy: 0.6741\n",
            "Epoch 00183: val_accuracy improved from 0.61111 to 0.66667, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8885 - accuracy: 0.6597 - val_loss: 0.7770 - val_accuracy: 0.6667\n",
            "Epoch 184/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.0003 - accuracy: 0.5843\n",
            "Epoch 00184: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9479 - accuracy: 0.6111 - val_loss: 0.9078 - val_accuracy: 0.6111\n",
            "Epoch 185/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.9606 - accuracy: 0.6416\n",
            "Epoch 00185: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9491 - accuracy: 0.6424 - val_loss: 1.9796 - val_accuracy: 0.4722\n",
            "Epoch 186/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.0210 - accuracy: 0.6552\n",
            "Epoch 00186: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0048 - accuracy: 0.6528 - val_loss: 1.3420 - val_accuracy: 0.5139\n",
            "Epoch 187/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.1091 - accuracy: 0.5862\n",
            "Epoch 00187: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0894 - accuracy: 0.5903 - val_loss: 0.8868 - val_accuracy: 0.5972\n",
            "Epoch 188/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.9477 - accuracy: 0.6402\n",
            "Epoch 00188: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9394 - accuracy: 0.6319 - val_loss: 1.7736 - val_accuracy: 0.4861\n",
            "Epoch 189/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 1.0177 - accuracy: 0.5942\n",
            "Epoch 00189: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0057 - accuracy: 0.5972 - val_loss: 0.7426 - val_accuracy: 0.6111\n",
            "Epoch 190/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 1.0987 - accuracy: 0.6447\n",
            "Epoch 00190: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0933 - accuracy: 0.6458 - val_loss: 1.3437 - val_accuracy: 0.5278\n",
            "Epoch 191/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.9588 - accuracy: 0.6442\n",
            "Epoch 00191: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9380 - accuracy: 0.6493 - val_loss: 0.8894 - val_accuracy: 0.6389\n",
            "Epoch 192/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.0580 - accuracy: 0.5843\n",
            "Epoch 00192: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0263 - accuracy: 0.6042 - val_loss: 1.2003 - val_accuracy: 0.5694\n",
            "Epoch 193/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.9447 - accuracy: 0.6863\n",
            "Epoch 00193: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9370 - accuracy: 0.6736 - val_loss: 2.2885 - val_accuracy: 0.4028\n",
            "Epoch 194/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.9679 - accuracy: 0.6292\n",
            "Epoch 00194: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9593 - accuracy: 0.6250 - val_loss: 1.2801 - val_accuracy: 0.5139\n",
            "Epoch 195/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.9893 - accuracy: 0.6471\n",
            "Epoch 00195: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9994 - accuracy: 0.6389 - val_loss: 0.8923 - val_accuracy: 0.6250\n",
            "Epoch 196/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.8541 - accuracy: 0.6439\n",
            "Epoch 00196: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8695 - accuracy: 0.6458 - val_loss: 1.2813 - val_accuracy: 0.5139\n",
            "Epoch 197/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.9499 - accuracy: 0.6202\n",
            "Epoch 00197: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9545 - accuracy: 0.6111 - val_loss: 1.3536 - val_accuracy: 0.5694\n",
            "Epoch 198/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.7965 - accuracy: 0.6970\n",
            "Epoch 00198: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8628 - accuracy: 0.6910 - val_loss: 1.8030 - val_accuracy: 0.4306\n",
            "Epoch 199/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.9702 - accuracy: 0.6371\n",
            "Epoch 00199: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0017 - accuracy: 0.6389 - val_loss: 1.8251 - val_accuracy: 0.4444\n",
            "Epoch 200/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8794 - accuracy: 0.6557\n",
            "Epoch 00200: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8598 - accuracy: 0.6632 - val_loss: 0.9600 - val_accuracy: 0.6389\n",
            "Epoch 201/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8589 - accuracy: 0.6749\n",
            "Epoch 00201: val_accuracy improved from 0.66667 to 0.70833, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8554 - accuracy: 0.6806 - val_loss: 0.6184 - val_accuracy: 0.7083\n",
            "Epoch 202/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.9589 - accuracy: 0.6493\n",
            "Epoch 00202: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9589 - accuracy: 0.6493 - val_loss: 1.0590 - val_accuracy: 0.6250\n",
            "Epoch 203/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.9196 - accuracy: 0.6173\n",
            "Epoch 00203: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9192 - accuracy: 0.6250 - val_loss: 1.1914 - val_accuracy: 0.5972\n",
            "Epoch 204/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.9484 - accuracy: 0.6751\n",
            "Epoch 00204: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9144 - accuracy: 0.6875 - val_loss: 1.2089 - val_accuracy: 0.4722\n",
            "Epoch 205/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8645 - accuracy: 0.6625\n",
            "Epoch 00205: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8403 - accuracy: 0.6667 - val_loss: 1.3631 - val_accuracy: 0.5000\n",
            "Epoch 206/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8851 - accuracy: 0.6502\n",
            "Epoch 00206: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9637 - accuracy: 0.6354 - val_loss: 0.8706 - val_accuracy: 0.5556\n",
            "Epoch 207/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.9060 - accuracy: 0.6543\n",
            "Epoch 00207: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9583 - accuracy: 0.6389 - val_loss: 0.8343 - val_accuracy: 0.6111\n",
            "Epoch 208/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.9020 - accuracy: 0.6752\n",
            "Epoch 00208: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9415 - accuracy: 0.6632 - val_loss: 0.8633 - val_accuracy: 0.6667\n",
            "Epoch 209/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8876 - accuracy: 0.6749\n",
            "Epoch 00209: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8870 - accuracy: 0.6667 - val_loss: 1.1186 - val_accuracy: 0.5833\n",
            "Epoch 210/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.8562 - accuracy: 0.6710\n",
            "Epoch 00210: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8777 - accuracy: 0.6632 - val_loss: 0.8131 - val_accuracy: 0.6806\n",
            "Epoch 211/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.7624 - accuracy: 0.7284\n",
            "Epoch 00211: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7532 - accuracy: 0.7257 - val_loss: 1.9765 - val_accuracy: 0.4306\n",
            "Epoch 212/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.8600 - accuracy: 0.6407\n",
            "Epoch 00212: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8602 - accuracy: 0.6493 - val_loss: 0.9267 - val_accuracy: 0.6111\n",
            "Epoch 213/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.8992 - accuracy: 0.6416\n",
            "Epoch 00213: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9024 - accuracy: 0.6424 - val_loss: 1.0673 - val_accuracy: 0.6667\n",
            "Epoch 214/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 1.0398 - accuracy: 0.6371\n",
            "Epoch 00214: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9898 - accuracy: 0.6424 - val_loss: 0.9110 - val_accuracy: 0.5972\n",
            "Epoch 215/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.7899 - accuracy: 0.6838\n",
            "Epoch 00215: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8348 - accuracy: 0.6771 - val_loss: 0.8562 - val_accuracy: 0.6528\n",
            "Epoch 216/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.9203 - accuracy: 0.7033\n",
            "Epoch 00216: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9481 - accuracy: 0.6806 - val_loss: 1.0563 - val_accuracy: 0.5972\n",
            "Epoch 217/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.9133 - accuracy: 0.6424\n",
            "Epoch 00217: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9133 - accuracy: 0.6424 - val_loss: 1.3507 - val_accuracy: 0.5139\n",
            "Epoch 218/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8895 - accuracy: 0.6914\n",
            "Epoch 00218: val_accuracy improved from 0.70833 to 0.72222, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8795 - accuracy: 0.6944 - val_loss: 0.7736 - val_accuracy: 0.7222\n",
            "Epoch 219/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8064 - accuracy: 0.6792\n",
            "Epoch 00219: val_accuracy did not improve from 0.72222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8236 - accuracy: 0.6736 - val_loss: 1.0549 - val_accuracy: 0.5417\n",
            "Epoch 220/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.8630 - accuracy: 0.6458\n",
            "Epoch 00220: val_accuracy did not improve from 0.72222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8630 - accuracy: 0.6458 - val_loss: 0.6862 - val_accuracy: 0.6389\n",
            "Epoch 221/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8051 - accuracy: 0.6917\n",
            "Epoch 00221: val_accuracy improved from 0.72222 to 0.73611, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8437 - accuracy: 0.6875 - val_loss: 0.5598 - val_accuracy: 0.7361\n",
            "Epoch 222/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.7926 - accuracy: 0.7000\n",
            "Epoch 00222: val_accuracy did not improve from 0.73611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8316 - accuracy: 0.6806 - val_loss: 3.8005 - val_accuracy: 0.4583\n",
            "Epoch 223/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.9536 - accuracy: 0.6583\n",
            "Epoch 00223: val_accuracy did not improve from 0.73611\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9374 - accuracy: 0.6528 - val_loss: 0.8372 - val_accuracy: 0.6389\n",
            "Epoch 224/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.8503 - accuracy: 0.6526\n",
            "Epoch 00224: val_accuracy improved from 0.73611 to 0.75000, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8472 - accuracy: 0.6528 - val_loss: 0.5736 - val_accuracy: 0.7500\n",
            "Epoch 225/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.8377 - accuracy: 0.6835\n",
            "Epoch 00225: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8188 - accuracy: 0.6840 - val_loss: 1.4766 - val_accuracy: 0.6667\n",
            "Epoch 226/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8868 - accuracy: 0.6625\n",
            "Epoch 00226: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9287 - accuracy: 0.6736 - val_loss: 1.2191 - val_accuracy: 0.5694\n",
            "Epoch 227/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.8612 - accuracy: 0.6703\n",
            "Epoch 00227: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8705 - accuracy: 0.6667 - val_loss: 0.9074 - val_accuracy: 0.5972\n",
            "Epoch 228/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8394 - accuracy: 0.6625\n",
            "Epoch 00228: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8124 - accuracy: 0.6771 - val_loss: 0.6022 - val_accuracy: 0.7361\n",
            "Epoch 229/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8161 - accuracy: 0.6543\n",
            "Epoch 00229: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8428 - accuracy: 0.6493 - val_loss: 1.2392 - val_accuracy: 0.6389\n",
            "Epoch 230/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.8197 - accuracy: 0.6701\n",
            "Epoch 00230: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8197 - accuracy: 0.6701 - val_loss: 1.5924 - val_accuracy: 0.5000\n",
            "Epoch 231/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.8300 - accuracy: 0.6748\n",
            "Epoch 00231: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8539 - accuracy: 0.6667 - val_loss: 0.8223 - val_accuracy: 0.6250\n",
            "Epoch 232/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.7962 - accuracy: 0.6856\n",
            "Epoch 00232: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8132 - accuracy: 0.6910 - val_loss: 1.5338 - val_accuracy: 0.5278\n",
            "Epoch 233/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.7850 - accuracy: 0.7004\n",
            "Epoch 00233: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8013 - accuracy: 0.7014 - val_loss: 0.6005 - val_accuracy: 0.6944\n",
            "Epoch 234/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.8186 - accuracy: 0.7126\n",
            "Epoch 00234: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7888 - accuracy: 0.7188 - val_loss: 1.5055 - val_accuracy: 0.5556\n",
            "Epoch 235/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.9161 - accuracy: 0.6706\n",
            "Epoch 00235: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9798 - accuracy: 0.6667 - val_loss: 0.5321 - val_accuracy: 0.7222\n",
            "Epoch 236/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.9459 - accuracy: 0.6850\n",
            "Epoch 00236: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9515 - accuracy: 0.6806 - val_loss: 0.6968 - val_accuracy: 0.6667\n",
            "Epoch 237/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.7483 - accuracy: 0.7165\n",
            "Epoch 00237: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7848 - accuracy: 0.7118 - val_loss: 1.6644 - val_accuracy: 0.5139\n",
            "Epoch 238/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.8237 - accuracy: 0.6816\n",
            "Epoch 00238: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8168 - accuracy: 0.6875 - val_loss: 1.1703 - val_accuracy: 0.6250\n",
            "Epoch 239/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.8888 - accuracy: 0.7093\n",
            "Epoch 00239: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9212 - accuracy: 0.6944 - val_loss: 0.8722 - val_accuracy: 0.5833\n",
            "Epoch 240/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7303 - accuracy: 0.6960\n",
            "Epoch 00240: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7507 - accuracy: 0.6944 - val_loss: 1.9567 - val_accuracy: 0.3056\n",
            "Epoch 241/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7780 - accuracy: 0.6744\n",
            "Epoch 00241: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7982 - accuracy: 0.6632 - val_loss: 1.0016 - val_accuracy: 0.5556\n",
            "Epoch 242/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7516 - accuracy: 0.6923\n",
            "Epoch 00242: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7731 - accuracy: 0.6910 - val_loss: 0.9899 - val_accuracy: 0.5833\n",
            "Epoch 243/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.8552 - accuracy: 0.6977\n",
            "Epoch 00243: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8629 - accuracy: 0.6910 - val_loss: 2.0714 - val_accuracy: 0.3889\n",
            "Epoch 244/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.8128 - accuracy: 0.6894\n",
            "Epoch 00244: val_accuracy improved from 0.75000 to 0.83333, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7785 - accuracy: 0.7014 - val_loss: 0.5349 - val_accuracy: 0.8333\n",
            "Epoch 245/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8110 - accuracy: 0.6630\n",
            "Epoch 00245: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7886 - accuracy: 0.6701 - val_loss: 0.5974 - val_accuracy: 0.7222\n",
            "Epoch 246/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.9252 - accuracy: 0.7248\n",
            "Epoch 00246: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9165 - accuracy: 0.7153 - val_loss: 1.2262 - val_accuracy: 0.6389\n",
            "Epoch 247/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.9130 - accuracy: 0.7037\n",
            "Epoch 00247: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9074 - accuracy: 0.7049 - val_loss: 0.6316 - val_accuracy: 0.7083\n",
            "Epoch 248/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.7472 - accuracy: 0.6970\n",
            "Epoch 00248: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7244 - accuracy: 0.7118 - val_loss: 0.6513 - val_accuracy: 0.6667\n",
            "Epoch 249/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.7749 - accuracy: 0.6973\n",
            "Epoch 00249: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7946 - accuracy: 0.6910 - val_loss: 0.9036 - val_accuracy: 0.7361\n",
            "Epoch 250/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.9112 - accuracy: 0.6667\n",
            "Epoch 00250: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9032 - accuracy: 0.6701 - val_loss: 0.9446 - val_accuracy: 0.5833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1sUpso8dlkY",
        "outputId": "3887bd03-78bc-476d-8f26-194f354b02cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "#model.load_weights('model_best.hdf5')\n",
        "#model.compile(optimizer='RMSProp',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "stop = EarlyStopping(monitor='val_accuracy',  patience=70, verbose=0, mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=25, verbose=1,\n",
        "    factor=0.5,\n",
        "    min_lr = 0.00001,\n",
        "    cooldown=0\n",
        ")\n",
        "checkpoint = [ModelCheckpoint('model_best.hdf5',monitor='val_accuracy', \n",
        "        save_best_only=True, \n",
        "        mode='max',\n",
        "        verbose=1)\n",
        "]\n",
        "hist=model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[checkpoint,reduce_lr,stop],epochs=500, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.6995 - accuracy: 0.6935\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.68056, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7139 - accuracy: 0.6910 - val_loss: 0.9799 - val_accuracy: 0.6806\n",
            "Epoch 2/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8903 - accuracy: 0.6996\n",
            "Epoch 00002: val_accuracy improved from 0.68056 to 0.70833, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8920 - accuracy: 0.6910 - val_loss: 0.7394 - val_accuracy: 0.7083\n",
            "Epoch 3/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.7865 - accuracy: 0.7000\n",
            "Epoch 00003: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7693 - accuracy: 0.7083 - val_loss: 1.0952 - val_accuracy: 0.5972\n",
            "Epoch 4/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.8588 - accuracy: 0.7008\n",
            "Epoch 00004: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8192 - accuracy: 0.7083 - val_loss: 1.1705 - val_accuracy: 0.6111\n",
            "Epoch 5/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.7553 - accuracy: 0.7370\n",
            "Epoch 00005: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7408 - accuracy: 0.7326 - val_loss: 0.7431 - val_accuracy: 0.6111\n",
            "Epoch 6/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.8025 - accuracy: 0.6891\n",
            "Epoch 00006: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7876 - accuracy: 0.6910 - val_loss: 0.9908 - val_accuracy: 0.7083\n",
            "Epoch 7/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.8189 - accuracy: 0.7216\n",
            "Epoch 00007: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9202 - accuracy: 0.7257 - val_loss: 0.9865 - val_accuracy: 0.6111\n",
            "Epoch 8/500\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8854 - accuracy: 0.7042\n",
            "Epoch 00008: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9157 - accuracy: 0.6910 - val_loss: 1.3034 - val_accuracy: 0.6528\n",
            "Epoch 9/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.8587 - accuracy: 0.7008\n",
            "Epoch 00009: val_accuracy did not improve from 0.70833\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8507 - accuracy: 0.6944 - val_loss: 1.8954 - val_accuracy: 0.4722\n",
            "Epoch 10/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.9488 - accuracy: 0.6587\n",
            "Epoch 00010: val_accuracy improved from 0.70833 to 0.75000, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9115 - accuracy: 0.6632 - val_loss: 0.6809 - val_accuracy: 0.7500\n",
            "Epoch 11/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7689 - accuracy: 0.7216\n",
            "Epoch 00011: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7718 - accuracy: 0.7153 - val_loss: 0.7592 - val_accuracy: 0.6667\n",
            "Epoch 12/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.8252 - accuracy: 0.7165\n",
            "Epoch 00012: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7884 - accuracy: 0.7326 - val_loss: 1.6825 - val_accuracy: 0.5278\n",
            "Epoch 13/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.8165 - accuracy: 0.7029\n",
            "Epoch 00013: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8154 - accuracy: 0.6979 - val_loss: 0.6479 - val_accuracy: 0.7500\n",
            "Epoch 14/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7161 - accuracy: 0.7093\n",
            "Epoch 00014: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.7118 - val_loss: 0.9862 - val_accuracy: 0.7222\n",
            "Epoch 15/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.7992 - accuracy: 0.7101\n",
            "Epoch 00015: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7956 - accuracy: 0.7118 - val_loss: 0.7079 - val_accuracy: 0.6806\n",
            "Epoch 16/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.7510 - accuracy: 0.6775\n",
            "Epoch 00016: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8098 - accuracy: 0.6667 - val_loss: 0.8431 - val_accuracy: 0.6111\n",
            "Epoch 17/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7924 - accuracy: 0.7143\n",
            "Epoch 00017: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7821 - accuracy: 0.7118 - val_loss: 0.5295 - val_accuracy: 0.7222\n",
            "Epoch 18/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8007 - accuracy: 0.7143\n",
            "Epoch 00018: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8085 - accuracy: 0.7118 - val_loss: 2.8639 - val_accuracy: 0.4306\n",
            "Epoch 19/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.7842 - accuracy: 0.7165\n",
            "Epoch 00019: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7679 - accuracy: 0.7222 - val_loss: 1.5035 - val_accuracy: 0.5278\n",
            "Epoch 20/500\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.6700 - accuracy: 0.7312\n",
            "Epoch 00020: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6674 - accuracy: 0.7326 - val_loss: 1.1617 - val_accuracy: 0.5694\n",
            "Epoch 21/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.6717 - accuracy: 0.7510\n",
            "Epoch 00021: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.7465 - val_loss: 0.9147 - val_accuracy: 0.7083\n",
            "Epoch 22/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.8707 - accuracy: 0.7333\n",
            "Epoch 00022: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8863 - accuracy: 0.7257 - val_loss: 1.1211 - val_accuracy: 0.6111\n",
            "Epoch 23/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.6719 - accuracy: 0.7222\n",
            "Epoch 00023: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7422 - accuracy: 0.7326 - val_loss: 1.5069 - val_accuracy: 0.6250\n",
            "Epoch 24/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.9570 - accuracy: 0.6593\n",
            "Epoch 00024: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9525 - accuracy: 0.6562 - val_loss: 1.6656 - val_accuracy: 0.5694\n",
            "Epoch 25/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.8390 - accuracy: 0.7033\n",
            "Epoch 00025: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8768 - accuracy: 0.6840 - val_loss: 1.4253 - val_accuracy: 0.5694\n",
            "Epoch 26/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.7363 - accuracy: 0.7370\n",
            "Epoch 00026: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7413 - accuracy: 0.7326 - val_loss: 0.6168 - val_accuracy: 0.7222\n",
            "Epoch 27/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.8712 - accuracy: 0.7296\n",
            "Epoch 00027: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8742 - accuracy: 0.7257 - val_loss: 1.6974 - val_accuracy: 0.4861\n",
            "Epoch 28/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.8227 - accuracy: 0.6479\n",
            "Epoch 00028: val_accuracy improved from 0.75000 to 0.76389, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8178 - accuracy: 0.6562 - val_loss: 0.6345 - val_accuracy: 0.7639\n",
            "Epoch 29/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.7858 - accuracy: 0.7294\n",
            "Epoch 00029: val_accuracy did not improve from 0.76389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8114 - accuracy: 0.7014 - val_loss: 1.4531 - val_accuracy: 0.5000\n",
            "Epoch 30/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.7376 - accuracy: 0.7020\n",
            "Epoch 00030: val_accuracy did not improve from 0.76389\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8039 - accuracy: 0.6632 - val_loss: 0.9678 - val_accuracy: 0.5833\n",
            "Epoch 31/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.6492 - accuracy: 0.7395\n",
            "Epoch 00031: val_accuracy improved from 0.76389 to 0.81944, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.7396 - val_loss: 0.4125 - val_accuracy: 0.8194\n",
            "Epoch 32/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.8649 - accuracy: 0.6593\n",
            "Epoch 00032: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8443 - accuracy: 0.6632 - val_loss: 1.3581 - val_accuracy: 0.6111\n",
            "Epoch 33/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8397 - accuracy: 0.6850\n",
            "Epoch 00033: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8355 - accuracy: 0.6910 - val_loss: 0.9367 - val_accuracy: 0.6667\n",
            "Epoch 34/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.7424 - accuracy: 0.7510\n",
            "Epoch 00034: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8277 - accuracy: 0.7292 - val_loss: 1.1381 - val_accuracy: 0.7083\n",
            "Epoch 35/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.8182 - accuracy: 0.7083\n",
            "Epoch 00035: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8292 - accuracy: 0.7049 - val_loss: 0.4564 - val_accuracy: 0.8056\n",
            "Epoch 36/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.6912 - accuracy: 0.7216\n",
            "Epoch 00036: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.7222 - val_loss: 0.5171 - val_accuracy: 0.7778\n",
            "Epoch 37/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.6910 - accuracy: 0.7273\n",
            "Epoch 00037: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7111 - accuracy: 0.7361 - val_loss: 0.8039 - val_accuracy: 0.7222\n",
            "Epoch 38/500\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.7613 - accuracy: 0.7240\n",
            "Epoch 00038: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7709 - accuracy: 0.7188 - val_loss: 0.7942 - val_accuracy: 0.7083\n",
            "Epoch 39/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.9099 - accuracy: 0.7165\n",
            "Epoch 00039: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9242 - accuracy: 0.7188 - val_loss: 1.3668 - val_accuracy: 0.5833\n",
            "Epoch 40/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.8063 - accuracy: 0.6938\n",
            "Epoch 00040: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7772 - accuracy: 0.7049 - val_loss: 0.9724 - val_accuracy: 0.5972\n",
            "Epoch 41/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.7354 - accuracy: 0.7197\n",
            "Epoch 00041: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7520 - accuracy: 0.7083 - val_loss: 0.6710 - val_accuracy: 0.7500\n",
            "Epoch 42/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.8801 - accuracy: 0.7074\n",
            "Epoch 00042: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8416 - accuracy: 0.7188 - val_loss: 0.9713 - val_accuracy: 0.6806\n",
            "Epoch 43/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7585 - accuracy: 0.7093\n",
            "Epoch 00043: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7572 - accuracy: 0.7014 - val_loss: 1.0012 - val_accuracy: 0.6250\n",
            "Epoch 44/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8830 - accuracy: 0.6960\n",
            "Epoch 00044: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8617 - accuracy: 0.7083 - val_loss: 0.7619 - val_accuracy: 0.7222\n",
            "Epoch 45/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.7998 - accuracy: 0.7280\n",
            "Epoch 00045: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8059 - accuracy: 0.7222 - val_loss: 0.8984 - val_accuracy: 0.6944\n",
            "Epoch 46/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7689 - accuracy: 0.7179\n",
            "Epoch 00046: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7918 - accuracy: 0.7014 - val_loss: 0.6285 - val_accuracy: 0.8194\n",
            "Epoch 47/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.7075 - accuracy: 0.6818\n",
            "Epoch 00047: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6846 - accuracy: 0.6944 - val_loss: 0.8247 - val_accuracy: 0.7083\n",
            "Epoch 48/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.8919 - accuracy: 0.7428\n",
            "Epoch 00048: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9260 - accuracy: 0.7326 - val_loss: 1.2432 - val_accuracy: 0.6806\n",
            "Epoch 49/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.7162 - accuracy: 0.7356\n",
            "Epoch 00049: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7587 - accuracy: 0.7361 - val_loss: 0.8701 - val_accuracy: 0.7222\n",
            "Epoch 50/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.7051 - accuracy: 0.7079\n",
            "Epoch 00050: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7211 - accuracy: 0.6979 - val_loss: 0.7895 - val_accuracy: 0.6806\n",
            "Epoch 51/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.7157 - accuracy: 0.7439\n",
            "Epoch 00051: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7931 - accuracy: 0.7292 - val_loss: 1.2540 - val_accuracy: 0.6250\n",
            "Epoch 52/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.7729 - accuracy: 0.7103\n",
            "Epoch 00052: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7496 - accuracy: 0.7188 - val_loss: 1.0204 - val_accuracy: 0.5694\n",
            "Epoch 53/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7642 - accuracy: 0.7143\n",
            "Epoch 00053: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7466 - accuracy: 0.7257 - val_loss: 0.9309 - val_accuracy: 0.7083\n",
            "Epoch 54/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.8863 - accuracy: 0.7273\n",
            "Epoch 00054: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9496 - accuracy: 0.7222 - val_loss: 0.6824 - val_accuracy: 0.7500\n",
            "Epoch 55/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.6373 - accuracy: 0.7678\n",
            "Epoch 00055: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6289 - accuracy: 0.7674 - val_loss: 0.8147 - val_accuracy: 0.6944\n",
            "Epoch 56/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.7739 - accuracy: 0.7294\n",
            "Epoch 00056: val_accuracy did not improve from 0.81944\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7519 - accuracy: 0.7431 - val_loss: 1.1871 - val_accuracy: 0.6250\n",
            "Epoch 57/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3895 - accuracy: 0.8391\n",
            "Epoch 00057: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8403 - val_loss: 0.7520 - val_accuracy: 0.7639\n",
            "Epoch 58/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.3819 - accuracy: 0.8256\n",
            "Epoch 00058: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3938 - accuracy: 0.8090 - val_loss: 0.9032 - val_accuracy: 0.6667\n",
            "Epoch 59/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3604 - accuracy: 0.8544\n",
            "Epoch 00059: val_accuracy did not improve from 0.81944\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4004 - accuracy: 0.8438 - val_loss: 0.6249 - val_accuracy: 0.7639\n",
            "Epoch 60/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.3630 - accuracy: 0.8333\n",
            "Epoch 00060: val_accuracy improved from 0.81944 to 0.83333, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3660 - accuracy: 0.8264 - val_loss: 0.4900 - val_accuracy: 0.8333\n",
            "Epoch 61/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3851 - accuracy: 0.8144\n",
            "Epoch 00061: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8090 - val_loss: 0.4449 - val_accuracy: 0.7917\n",
            "Epoch 62/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3700 - accuracy: 0.8373\n",
            "Epoch 00062: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3410 - accuracy: 0.8472 - val_loss: 0.3325 - val_accuracy: 0.8194\n",
            "Epoch 63/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3801 - accuracy: 0.8849\n",
            "Epoch 00063: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3586 - accuracy: 0.8854 - val_loss: 0.4430 - val_accuracy: 0.8056\n",
            "Epoch 64/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.4188 - accuracy: 0.8008\n",
            "Epoch 00064: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8160 - val_loss: 0.3333 - val_accuracy: 0.8333\n",
            "Epoch 65/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3710 - accuracy: 0.8371\n",
            "Epoch 00065: val_accuracy did not improve from 0.83333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3580 - accuracy: 0.8472 - val_loss: 0.6700 - val_accuracy: 0.7361\n",
            "Epoch 66/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3664 - accuracy: 0.8429\n",
            "Epoch 00066: val_accuracy improved from 0.83333 to 0.84722, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8542 - val_loss: 0.2983 - val_accuracy: 0.8472\n",
            "Epoch 67/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3643 - accuracy: 0.8391\n",
            "Epoch 00067: val_accuracy did not improve from 0.84722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.8368 - val_loss: 0.6361 - val_accuracy: 0.7778\n",
            "Epoch 68/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.3612 - accuracy: 0.8370\n",
            "Epoch 00068: val_accuracy did not improve from 0.84722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8438 - val_loss: 0.4740 - val_accuracy: 0.8194\n",
            "Epoch 69/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3549 - accuracy: 0.8294\n",
            "Epoch 00069: val_accuracy did not improve from 0.84722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8299 - val_loss: 0.6963 - val_accuracy: 0.8056\n",
            "Epoch 70/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3887 - accuracy: 0.8506\n",
            "Epoch 00070: val_accuracy did not improve from 0.84722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8576 - val_loss: 0.5405 - val_accuracy: 0.8056\n",
            "Epoch 71/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.3533 - accuracy: 0.8502\n",
            "Epoch 00071: val_accuracy did not improve from 0.84722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8576 - val_loss: 0.5004 - val_accuracy: 0.8194\n",
            "Epoch 72/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.3785 - accuracy: 0.8614\n",
            "Epoch 00072: val_accuracy did not improve from 0.84722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8507 - val_loss: 0.4381 - val_accuracy: 0.7917\n",
            "Epoch 73/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.3179 - accuracy: 0.8374\n",
            "Epoch 00073: val_accuracy did not improve from 0.84722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8403 - val_loss: 0.4213 - val_accuracy: 0.8333\n",
            "Epoch 74/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.3720 - accuracy: 0.8514\n",
            "Epoch 00074: val_accuracy improved from 0.84722 to 0.86111, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8368 - val_loss: 0.4706 - val_accuracy: 0.8611\n",
            "Epoch 75/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3280 - accuracy: 0.8674\n",
            "Epoch 00075: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8507 - val_loss: 0.4911 - val_accuracy: 0.8333\n",
            "Epoch 76/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.3323 - accuracy: 0.8741\n",
            "Epoch 00076: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8646 - val_loss: 0.5513 - val_accuracy: 0.7639\n",
            "Epoch 77/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.3765 - accuracy: 0.8295\n",
            "Epoch 00077: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8333 - val_loss: 0.4259 - val_accuracy: 0.8333\n",
            "Epoch 78/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2917 - accuracy: 0.8876\n",
            "Epoch 00078: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8889 - val_loss: 0.4357 - val_accuracy: 0.8333\n",
            "Epoch 79/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.3630 - accuracy: 0.8444\n",
            "Epoch 00079: val_accuracy improved from 0.86111 to 0.87500, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8472 - val_loss: 0.4757 - val_accuracy: 0.8750\n",
            "Epoch 80/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3329 - accuracy: 0.8651\n",
            "Epoch 00080: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8715 - val_loss: 0.3910 - val_accuracy: 0.8472\n",
            "Epoch 81/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.3428 - accuracy: 0.8372\n",
            "Epoch 00081: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.8403 - val_loss: 0.2955 - val_accuracy: 0.8472\n",
            "Epoch 82/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3631 - accuracy: 0.8561\n",
            "Epoch 00082: val_accuracy improved from 0.87500 to 0.88889, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3868 - accuracy: 0.8507 - val_loss: 0.3056 - val_accuracy: 0.8889\n",
            "Epoch 83/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3423 - accuracy: 0.8523\n",
            "Epoch 00083: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8576 - val_loss: 0.4124 - val_accuracy: 0.8472\n",
            "Epoch 84/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3342 - accuracy: 0.8544\n",
            "Epoch 00084: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8611 - val_loss: 0.3485 - val_accuracy: 0.8750\n",
            "Epoch 85/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3797 - accuracy: 0.8333\n",
            "Epoch 00085: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8472 - val_loss: 0.4896 - val_accuracy: 0.8194\n",
            "Epoch 86/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3625 - accuracy: 0.8429\n",
            "Epoch 00086: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8507 - val_loss: 0.4704 - val_accuracy: 0.8056\n",
            "Epoch 87/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3890 - accuracy: 0.8373\n",
            "Epoch 00087: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8403 - val_loss: 1.5284 - val_accuracy: 0.6528\n",
            "Epoch 88/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3493 - accuracy: 0.8485\n",
            "Epoch 00088: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.8368 - val_loss: 0.8052 - val_accuracy: 0.6806\n",
            "Epoch 89/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.3403 - accuracy: 0.8798\n",
            "Epoch 00089: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8646 - val_loss: 0.3451 - val_accuracy: 0.8194\n",
            "Epoch 90/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.3517 - accuracy: 0.8539\n",
            "Epoch 00090: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.8507 - val_loss: 0.3693 - val_accuracy: 0.8472\n",
            "Epoch 91/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.3377 - accuracy: 0.8431\n",
            "Epoch 00091: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8403 - val_loss: 0.3849 - val_accuracy: 0.8750\n",
            "Epoch 92/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.3472 - accuracy: 0.8682\n",
            "Epoch 00092: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3444 - accuracy: 0.8646 - val_loss: 0.3734 - val_accuracy: 0.8056\n",
            "Epoch 93/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3544 - accuracy: 0.8598\n",
            "Epoch 00093: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3421 - accuracy: 0.8611 - val_loss: 0.3823 - val_accuracy: 0.8194\n",
            "Epoch 94/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2899 - accuracy: 0.8750\n",
            "Epoch 00094: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.8819 - val_loss: 0.3504 - val_accuracy: 0.8194\n",
            "Epoch 95/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.3767 - accuracy: 0.8577\n",
            "Epoch 00095: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3732 - accuracy: 0.8542 - val_loss: 0.4610 - val_accuracy: 0.8333\n",
            "Epoch 96/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2999 - accuracy: 0.8801\n",
            "Epoch 00096: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3112 - accuracy: 0.8715 - val_loss: 0.4558 - val_accuracy: 0.8194\n",
            "Epoch 97/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.3971 - accuracy: 0.8593\n",
            "Epoch 00097: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8507 - val_loss: 0.4407 - val_accuracy: 0.8611\n",
            "Epoch 98/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2716 - accuracy: 0.9015\n",
            "Epoch 00098: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.8993 - val_loss: 0.4222 - val_accuracy: 0.8611\n",
            "Epoch 99/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.3674 - accuracy: 0.8425\n",
            "Epoch 00099: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8472 - val_loss: 0.4267 - val_accuracy: 0.8611\n",
            "Epoch 100/500\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.8423\n",
            "Epoch 00100: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3889 - accuracy: 0.8472 - val_loss: 0.3119 - val_accuracy: 0.8611\n",
            "Epoch 101/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.4051 - accuracy: 0.8577\n",
            "Epoch 00101: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8542 - val_loss: 0.6497 - val_accuracy: 0.7917\n",
            "Epoch 102/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3135 - accuracy: 0.8788\n",
            "Epoch 00102: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8750 - val_loss: 0.3682 - val_accuracy: 0.8472\n",
            "Epoch 103/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3438 - accuracy: 0.8730\n",
            "Epoch 00103: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8646 - val_loss: 0.2798 - val_accuracy: 0.8889\n",
            "Epoch 104/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.3239 - accuracy: 0.8519\n",
            "Epoch 00104: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8542 - val_loss: 0.5912 - val_accuracy: 0.8611\n",
            "Epoch 105/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.3540 - accuracy: 0.8645\n",
            "Epoch 00105: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8646 - val_loss: 0.2303 - val_accuracy: 0.8611\n",
            "Epoch 106/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3769 - accuracy: 0.8429\n",
            "Epoch 00106: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8438 - val_loss: 0.8063 - val_accuracy: 0.7500\n",
            "Epoch 107/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3223 - accuracy: 0.8429\n",
            "Epoch 00107: val_accuracy did not improve from 0.88889\n",
            "\n",
            "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8403 - val_loss: 0.5051 - val_accuracy: 0.8333\n",
            "Epoch 108/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.2189 - accuracy: 0.9080\n",
            "Epoch 00108: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.9097 - val_loss: 0.4448 - val_accuracy: 0.7639\n",
            "Epoch 109/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.2075 - accuracy: 0.9111\n",
            "Epoch 00109: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9097 - val_loss: 0.2777 - val_accuracy: 0.8889\n",
            "Epoch 110/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2024 - accuracy: 0.8977\n",
            "Epoch 00110: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.8924 - val_loss: 0.3668 - val_accuracy: 0.8750\n",
            "Epoch 111/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.2154 - accuracy: 0.8986\n",
            "Epoch 00111: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.8993 - val_loss: 0.3749 - val_accuracy: 0.8750\n",
            "Epoch 112/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2136 - accuracy: 0.9139\n",
            "Epoch 00112: val_accuracy improved from 0.88889 to 0.90278, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9201 - val_loss: 0.3328 - val_accuracy: 0.9028\n",
            "Epoch 113/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2187 - accuracy: 0.9091\n",
            "Epoch 00113: val_accuracy did not improve from 0.90278\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.9028 - val_loss: 0.2484 - val_accuracy: 0.9028\n",
            "Epoch 114/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2231 - accuracy: 0.8839\n",
            "Epoch 00114: val_accuracy improved from 0.90278 to 0.91667, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.8924 - val_loss: 0.2158 - val_accuracy: 0.9167\n",
            "Epoch 115/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2011 - accuracy: 0.9101\n",
            "Epoch 00115: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2025 - accuracy: 0.9028 - val_loss: 0.4238 - val_accuracy: 0.8056\n",
            "Epoch 116/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.2003 - accuracy: 0.9084\n",
            "Epoch 00116: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9062 - val_loss: 0.1817 - val_accuracy: 0.9028\n",
            "Epoch 117/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1809 - accuracy: 0.9267\n",
            "Epoch 00117: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.9236 - val_loss: 0.2835 - val_accuracy: 0.8889\n",
            "Epoch 118/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.2086 - accuracy: 0.8966\n",
            "Epoch 00118: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.8993 - val_loss: 0.2836 - val_accuracy: 0.8889\n",
            "Epoch 119/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.2147 - accuracy: 0.8974\n",
            "Epoch 00119: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2225 - accuracy: 0.8924 - val_loss: 0.1800 - val_accuracy: 0.9028\n",
            "Epoch 120/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.2141 - accuracy: 0.9111\n",
            "Epoch 00120: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9062 - val_loss: 0.3022 - val_accuracy: 0.9028\n",
            "Epoch 121/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.2173 - accuracy: 0.9186\n",
            "Epoch 00121: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9167 - val_loss: 0.4029 - val_accuracy: 0.8472\n",
            "Epoch 122/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2222 - accuracy: 0.9026\n",
            "Epoch 00122: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9062 - val_loss: 0.2021 - val_accuracy: 0.9028\n",
            "Epoch 123/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1868 - accuracy: 0.9185\n",
            "Epoch 00123: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9097 - val_loss: 0.7080 - val_accuracy: 0.7778\n",
            "Epoch 124/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.2354 - accuracy: 0.9111\n",
            "Epoch 00124: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2218 - accuracy: 0.9167 - val_loss: 0.3738 - val_accuracy: 0.8611\n",
            "Epoch 125/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.2291 - accuracy: 0.8877\n",
            "Epoch 00125: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.8924 - val_loss: 0.2533 - val_accuracy: 0.8889\n",
            "Epoch 126/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2292 - accuracy: 0.9091\n",
            "Epoch 00126: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2306 - accuracy: 0.9132 - val_loss: 0.2157 - val_accuracy: 0.8750\n",
            "Epoch 127/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1932 - accuracy: 0.9064\n",
            "Epoch 00127: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9062 - val_loss: 0.2127 - val_accuracy: 0.8889\n",
            "Epoch 128/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.2121 - accuracy: 0.8889\n",
            "Epoch 00128: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.8958 - val_loss: 0.1943 - val_accuracy: 0.8889\n",
            "Epoch 129/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2215 - accuracy: 0.9091\n",
            "Epoch 00129: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2076 - accuracy: 0.9132 - val_loss: 0.1809 - val_accuracy: 0.9028\n",
            "Epoch 130/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1720 - accuracy: 0.9326\n",
            "Epoch 00130: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9201 - val_loss: 0.1936 - val_accuracy: 0.8889\n",
            "Epoch 131/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.1900 - accuracy: 0.9109\n",
            "Epoch 00131: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9062 - val_loss: 0.2614 - val_accuracy: 0.8750\n",
            "Epoch 132/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2098 - accuracy: 0.9053\n",
            "Epoch 00132: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9097 - val_loss: 0.4031 - val_accuracy: 0.8889\n",
            "Epoch 133/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1927 - accuracy: 0.9091\n",
            "Epoch 00133: val_accuracy did not improve from 0.91667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.9132 - val_loss: 0.4327 - val_accuracy: 0.8194\n",
            "Epoch 134/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.2100 - accuracy: 0.9011\n",
            "Epoch 00134: val_accuracy improved from 0.91667 to 0.93056, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9028 - val_loss: 0.1765 - val_accuracy: 0.9306\n",
            "Epoch 135/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1993 - accuracy: 0.9139\n",
            "Epoch 00135: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1952 - accuracy: 0.9167 - val_loss: 0.1952 - val_accuracy: 0.9306\n",
            "Epoch 136/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2137 - accuracy: 0.9242\n",
            "Epoch 00136: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9201 - val_loss: 0.2394 - val_accuracy: 0.9028\n",
            "Epoch 137/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1827 - accuracy: 0.9101\n",
            "Epoch 00137: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9062 - val_loss: 0.2130 - val_accuracy: 0.8889\n",
            "Epoch 138/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1792 - accuracy: 0.8949\n",
            "Epoch 00138: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.8993 - val_loss: 0.4238 - val_accuracy: 0.8472\n",
            "Epoch 139/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.2181 - accuracy: 0.9267\n",
            "Epoch 00139: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.9271 - val_loss: 0.5769 - val_accuracy: 0.7778\n",
            "Epoch 140/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1966 - accuracy: 0.9020\n",
            "Epoch 00140: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.8924 - val_loss: 0.3346 - val_accuracy: 0.8611\n",
            "Epoch 141/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1949 - accuracy: 0.8989\n",
            "Epoch 00141: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.8993 - val_loss: 0.4753 - val_accuracy: 0.8750\n",
            "Epoch 142/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.2056 - accuracy: 0.9167\n",
            "Epoch 00142: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.9167 - val_loss: 0.4753 - val_accuracy: 0.8056\n",
            "Epoch 143/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2187 - accuracy: 0.9091\n",
            "Epoch 00143: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9132 - val_loss: 0.2008 - val_accuracy: 0.8750\n",
            "Epoch 144/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2010 - accuracy: 0.9129\n",
            "Epoch 00144: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1992 - accuracy: 0.9132 - val_loss: 0.2674 - val_accuracy: 0.8611\n",
            "Epoch 145/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.2052 - accuracy: 0.9048\n",
            "Epoch 00145: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2031 - accuracy: 0.9062 - val_loss: 0.2000 - val_accuracy: 0.9028\n",
            "Epoch 146/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.1982 - accuracy: 0.9264\n",
            "Epoch 00146: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2060 - accuracy: 0.9167 - val_loss: 0.2552 - val_accuracy: 0.9028\n",
            "Epoch 147/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1906 - accuracy: 0.9195\n",
            "Epoch 00147: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9167 - val_loss: 0.3722 - val_accuracy: 0.8750\n",
            "Epoch 148/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1961 - accuracy: 0.9205\n",
            "Epoch 00148: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9097 - val_loss: 0.3839 - val_accuracy: 0.8472\n",
            "Epoch 149/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2106 - accuracy: 0.9139\n",
            "Epoch 00149: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.9097 - val_loss: 0.2053 - val_accuracy: 0.9028\n",
            "Epoch 150/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.2099 - accuracy: 0.9048\n",
            "Epoch 00150: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2004 - accuracy: 0.9097 - val_loss: 0.2636 - val_accuracy: 0.9167\n",
            "Epoch 151/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1927 - accuracy: 0.9053\n",
            "Epoch 00151: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9062 - val_loss: 0.2337 - val_accuracy: 0.9167\n",
            "Epoch 152/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.2097 - accuracy: 0.9064\n",
            "Epoch 00152: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1981 - accuracy: 0.9132 - val_loss: 0.1596 - val_accuracy: 0.9028\n",
            "Epoch 153/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.2146 - accuracy: 0.8963\n",
            "Epoch 00153: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9028 - val_loss: 0.3993 - val_accuracy: 0.8889\n",
            "Epoch 154/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1864 - accuracy: 0.9129\n",
            "Epoch 00154: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9062 - val_loss: 0.2400 - val_accuracy: 0.9167\n",
            "Epoch 155/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1745 - accuracy: 0.9213\n",
            "Epoch 00155: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9271 - val_loss: 0.2107 - val_accuracy: 0.8889\n",
            "Epoch 156/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1729 - accuracy: 0.9333\n",
            "Epoch 00156: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9306 - val_loss: 0.4757 - val_accuracy: 0.8194\n",
            "Epoch 157/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.2211 - accuracy: 0.8986\n",
            "Epoch 00157: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.8993 - val_loss: 0.2199 - val_accuracy: 0.8750\n",
            "Epoch 158/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.2046 - accuracy: 0.9080\n",
            "Epoch 00158: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1927 - accuracy: 0.9132 - val_loss: 0.2481 - val_accuracy: 0.8889\n",
            "Epoch 159/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.2180 - accuracy: 0.9234\n",
            "Epoch 00159: val_accuracy did not improve from 0.93056\n",
            "\n",
            "Epoch 00159: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.9236 - val_loss: 0.2046 - val_accuracy: 0.8889\n",
            "Epoch 160/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1621 - accuracy: 0.9288\n",
            "Epoch 00160: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9201 - val_loss: 0.1853 - val_accuracy: 0.9028\n",
            "Epoch 161/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1583 - accuracy: 0.9275\n",
            "Epoch 00161: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9236 - val_loss: 0.1843 - val_accuracy: 0.9028\n",
            "Epoch 162/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1567 - accuracy: 0.9195\n",
            "Epoch 00162: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.9201 - val_loss: 0.1516 - val_accuracy: 0.8889\n",
            "Epoch 163/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1439 - accuracy: 0.9333\n",
            "Epoch 00163: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9340 - val_loss: 0.2541 - val_accuracy: 0.9028\n",
            "Epoch 164/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1684 - accuracy: 0.9194\n",
            "Epoch 00164: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9236 - val_loss: 0.2274 - val_accuracy: 0.9028\n",
            "Epoch 165/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1612 - accuracy: 0.9058\n",
            "Epoch 00165: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.8993 - val_loss: 0.2269 - val_accuracy: 0.9028\n",
            "Epoch 166/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1428 - accuracy: 0.9325\n",
            "Epoch 00166: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1454 - accuracy: 0.9375 - val_loss: 0.2256 - val_accuracy: 0.9028\n",
            "Epoch 167/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1598 - accuracy: 0.9349\n",
            "Epoch 00167: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9340 - val_loss: 0.2116 - val_accuracy: 0.9028\n",
            "Epoch 168/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1453 - accuracy: 0.9333\n",
            "Epoch 00168: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9375 - val_loss: 0.1979 - val_accuracy: 0.9028\n",
            "Epoch 169/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1710 - accuracy: 0.9231\n",
            "Epoch 00169: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9236 - val_loss: 0.3037 - val_accuracy: 0.8889\n",
            "Epoch 170/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1362 - accuracy: 0.9310\n",
            "Epoch 00170: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9340 - val_loss: 0.2939 - val_accuracy: 0.9028\n",
            "Epoch 171/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1624 - accuracy: 0.9203\n",
            "Epoch 00171: val_accuracy improved from 0.93056 to 0.94444, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.9236 - val_loss: 0.1777 - val_accuracy: 0.9444\n",
            "Epoch 172/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1740 - accuracy: 0.9037\n",
            "Epoch 00172: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9028 - val_loss: 0.2005 - val_accuracy: 0.9028\n",
            "Epoch 173/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1604 - accuracy: 0.9272\n",
            "Epoch 00173: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9340 - val_loss: 0.2516 - val_accuracy: 0.9028\n",
            "Epoch 174/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1445 - accuracy: 0.9370\n",
            "Epoch 00174: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1479 - accuracy: 0.9375 - val_loss: 0.4141 - val_accuracy: 0.9028\n",
            "Epoch 175/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1526 - accuracy: 0.9370\n",
            "Epoch 00175: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.9375 - val_loss: 0.2613 - val_accuracy: 0.9028\n",
            "Epoch 176/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1479 - accuracy: 0.9363\n",
            "Epoch 00176: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1476 - accuracy: 0.9375 - val_loss: 0.2878 - val_accuracy: 0.8750\n",
            "Epoch 177/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1624 - accuracy: 0.9242\n",
            "Epoch 00177: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.9167 - val_loss: 0.1982 - val_accuracy: 0.9028\n",
            "Epoch 178/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1328 - accuracy: 0.9333\n",
            "Epoch 00178: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9271 - val_loss: 0.2032 - val_accuracy: 0.9306\n",
            "Epoch 179/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1447 - accuracy: 0.9387\n",
            "Epoch 00179: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.9340 - val_loss: 0.2297 - val_accuracy: 0.9306\n",
            "Epoch 180/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1592 - accuracy: 0.9185\n",
            "Epoch 00180: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1629 - accuracy: 0.9201 - val_loss: 0.1910 - val_accuracy: 0.9028\n",
            "Epoch 181/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1509 - accuracy: 0.9246\n",
            "Epoch 00181: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.9167 - val_loss: 0.2548 - val_accuracy: 0.9028\n",
            "Epoch 182/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1504 - accuracy: 0.9394\n",
            "Epoch 00182: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1514 - accuracy: 0.9375 - val_loss: 0.2801 - val_accuracy: 0.8889\n",
            "Epoch 183/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1479 - accuracy: 0.9377\n",
            "Epoch 00183: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9410 - val_loss: 0.2651 - val_accuracy: 0.8750\n",
            "Epoch 184/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1638 - accuracy: 0.9294\n",
            "Epoch 00184: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1633 - accuracy: 0.9306 - val_loss: 0.2323 - val_accuracy: 0.9306\n",
            "Epoch 185/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1721 - accuracy: 0.9176\n",
            "Epoch 00185: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9167 - val_loss: 0.2022 - val_accuracy: 0.9167\n",
            "Epoch 186/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1530 - accuracy: 0.9148\n",
            "Epoch 00186: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1546 - accuracy: 0.9167 - val_loss: 0.1771 - val_accuracy: 0.9028\n",
            "Epoch 187/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1606 - accuracy: 0.9259\n",
            "Epoch 00187: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9306 - val_loss: 0.1902 - val_accuracy: 0.9306\n",
            "Epoch 188/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1632 - accuracy: 0.8980\n",
            "Epoch 00188: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.8993 - val_loss: 0.2100 - val_accuracy: 0.9167\n",
            "Epoch 189/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1567 - accuracy: 0.9326\n",
            "Epoch 00189: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1555 - accuracy: 0.9306 - val_loss: 0.2468 - val_accuracy: 0.8889\n",
            "Epoch 190/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1655 - accuracy: 0.9333\n",
            "Epoch 00190: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9340 - val_loss: 0.1652 - val_accuracy: 0.9028\n",
            "Epoch 191/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1360 - accuracy: 0.9519\n",
            "Epoch 00191: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9410 - val_loss: 0.2416 - val_accuracy: 0.9028\n",
            "Epoch 192/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.1614 - accuracy: 0.9147\n",
            "Epoch 00192: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1545 - accuracy: 0.9201 - val_loss: 0.2158 - val_accuracy: 0.9167\n",
            "Epoch 193/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1504 - accuracy: 0.9326\n",
            "Epoch 00193: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9340 - val_loss: 0.2093 - val_accuracy: 0.8889\n",
            "Epoch 194/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1508 - accuracy: 0.9341\n",
            "Epoch 00194: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1627 - accuracy: 0.9271 - val_loss: 0.2733 - val_accuracy: 0.8750\n",
            "Epoch 195/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1614 - accuracy: 0.9267\n",
            "Epoch 00195: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 0.9271 - val_loss: 0.1734 - val_accuracy: 0.9028\n",
            "Epoch 196/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1504 - accuracy: 0.9272\n",
            "Epoch 00196: val_accuracy did not improve from 0.94444\n",
            "\n",
            "Epoch 00196: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.9271 - val_loss: 0.2584 - val_accuracy: 0.9167\n",
            "Epoch 197/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1403 - accuracy: 0.9333\n",
            "Epoch 00197: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.9306 - val_loss: 0.1691 - val_accuracy: 0.9167\n",
            "Epoch 198/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1435 - accuracy: 0.9231\n",
            "Epoch 00198: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.9271 - val_loss: 0.1710 - val_accuracy: 0.9167\n",
            "Epoch 199/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1426 - accuracy: 0.9365\n",
            "Epoch 00199: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9375 - val_loss: 0.1343 - val_accuracy: 0.9306\n",
            "Epoch 200/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1431 - accuracy: 0.9414\n",
            "Epoch 00200: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1428 - accuracy: 0.9410 - val_loss: 0.1507 - val_accuracy: 0.9028\n",
            "Epoch 201/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.9304\n",
            "Epoch 00201: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1351 - accuracy: 0.9306 - val_loss: 0.1545 - val_accuracy: 0.9167\n",
            "Epoch 202/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.9341\n",
            "Epoch 00202: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1379 - accuracy: 0.9306 - val_loss: 0.1531 - val_accuracy: 0.9306\n",
            "Epoch 203/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9157\n",
            "Epoch 00203: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1378 - accuracy: 0.9236 - val_loss: 0.1361 - val_accuracy: 0.9306\n",
            "Epoch 204/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1374 - accuracy: 0.9326\n",
            "Epoch 00204: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9340 - val_loss: 0.2001 - val_accuracy: 0.9306\n",
            "Epoch 205/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1318 - accuracy: 0.9377\n",
            "Epoch 00205: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1379 - accuracy: 0.9375 - val_loss: 0.1387 - val_accuracy: 0.9028\n",
            "Epoch 206/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1317 - accuracy: 0.9470\n",
            "Epoch 00206: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1319 - accuracy: 0.9444 - val_loss: 0.1823 - val_accuracy: 0.8889\n",
            "Epoch 207/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1334 - accuracy: 0.9444\n",
            "Epoch 00207: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.9410 - val_loss: 0.2124 - val_accuracy: 0.9028\n",
            "Epoch 208/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1427 - accuracy: 0.9377\n",
            "Epoch 00208: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1425 - accuracy: 0.9375 - val_loss: 0.1613 - val_accuracy: 0.9306\n",
            "Epoch 209/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1343 - accuracy: 0.9487\n",
            "Epoch 00209: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 0.9479 - val_loss: 0.1585 - val_accuracy: 0.9167\n",
            "Epoch 210/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1358 - accuracy: 0.9363\n",
            "Epoch 00210: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9410 - val_loss: 0.1865 - val_accuracy: 0.8889\n",
            "Epoch 211/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1502 - accuracy: 0.9213\n",
            "Epoch 00211: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9271 - val_loss: 0.1453 - val_accuracy: 0.9167\n",
            "Epoch 212/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1359 - accuracy: 0.9444\n",
            "Epoch 00212: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9444 - val_loss: 0.1542 - val_accuracy: 0.9167\n",
            "Epoch 213/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1363 - accuracy: 0.9377\n",
            "Epoch 00213: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9375 - val_loss: 0.1485 - val_accuracy: 0.9028\n",
            "Epoch 214/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1143 - accuracy: 0.9294\n",
            "Epoch 00214: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9201 - val_loss: 0.1722 - val_accuracy: 0.9167\n",
            "Epoch 215/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1328 - accuracy: 0.9414\n",
            "Epoch 00215: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1301 - accuracy: 0.9410 - val_loss: 0.1454 - val_accuracy: 0.9028\n",
            "Epoch 216/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1277 - accuracy: 0.9341\n",
            "Epoch 00216: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1284 - accuracy: 0.9340 - val_loss: 0.2144 - val_accuracy: 0.9028\n",
            "Epoch 217/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1417 - accuracy: 0.9451\n",
            "Epoch 00217: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9479 - val_loss: 0.1752 - val_accuracy: 0.9167\n",
            "Epoch 218/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1296 - accuracy: 0.9444\n",
            "Epoch 00218: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9444 - val_loss: 0.1795 - val_accuracy: 0.9167\n",
            "Epoch 219/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.0985 - accuracy: 0.9579\n",
            "Epoch 00219: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9340 - val_loss: 0.1805 - val_accuracy: 0.9028\n",
            "Epoch 220/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1362 - accuracy: 0.9333\n",
            "Epoch 00220: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1323 - accuracy: 0.9375 - val_loss: 0.1897 - val_accuracy: 0.8889\n",
            "Epoch 221/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1305 - accuracy: 0.9401\n",
            "Epoch 00221: val_accuracy did not improve from 0.94444\n",
            "\n",
            "Epoch 00221: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1315 - accuracy: 0.9410 - val_loss: 0.1394 - val_accuracy: 0.9306\n",
            "Epoch 222/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1085 - accuracy: 0.9438\n",
            "Epoch 00222: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9340 - val_loss: 0.1426 - val_accuracy: 0.9306\n",
            "Epoch 223/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1242 - accuracy: 0.9384\n",
            "Epoch 00223: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1260 - accuracy: 0.9375 - val_loss: 0.1429 - val_accuracy: 0.9167\n",
            "Epoch 224/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1314 - accuracy: 0.9363\n",
            "Epoch 00224: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9410 - val_loss: 0.1602 - val_accuracy: 0.9306\n",
            "Epoch 225/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1270 - accuracy: 0.9394\n",
            "Epoch 00225: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9410 - val_loss: 0.1533 - val_accuracy: 0.9167\n",
            "Epoch 226/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1158 - accuracy: 0.9412\n",
            "Epoch 00226: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9410 - val_loss: 0.1557 - val_accuracy: 0.9167\n",
            "Epoch 227/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1204 - accuracy: 0.9357\n",
            "Epoch 00227: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.9340 - val_loss: 0.1799 - val_accuracy: 0.9306\n",
            "Epoch 228/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.1318 - accuracy: 0.9425\n",
            "Epoch 00228: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1267 - accuracy: 0.9410 - val_loss: 0.1598 - val_accuracy: 0.9028\n",
            "Epoch 229/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1284 - accuracy: 0.9357\n",
            "Epoch 00229: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9375 - val_loss: 0.1483 - val_accuracy: 0.9167\n",
            "Epoch 230/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.1237 - accuracy: 0.9451\n",
            "Epoch 00230: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1246 - accuracy: 0.9410 - val_loss: 0.1403 - val_accuracy: 0.9306\n",
            "Epoch 231/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1305 - accuracy: 0.9407\n",
            "Epoch 00231: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.9444 - val_loss: 0.1391 - val_accuracy: 0.9306\n",
            "Epoch 232/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1048 - accuracy: 0.9519\n",
            "Epoch 00232: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9444 - val_loss: 0.1650 - val_accuracy: 0.9028\n",
            "Epoch 233/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1189 - accuracy: 0.9405\n",
            "Epoch 00233: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.9375 - val_loss: 0.1621 - val_accuracy: 0.9167\n",
            "Epoch 234/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9420\n",
            "Epoch 00234: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1211 - accuracy: 0.9444 - val_loss: 0.1837 - val_accuracy: 0.9306\n",
            "Epoch 235/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1287 - accuracy: 0.9457\n",
            "Epoch 00235: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.9444 - val_loss: 0.1396 - val_accuracy: 0.9167\n",
            "Epoch 236/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1274 - accuracy: 0.9356\n",
            "Epoch 00236: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1224 - accuracy: 0.9375 - val_loss: 0.1356 - val_accuracy: 0.9306\n",
            "Epoch 237/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1302 - accuracy: 0.9444\n",
            "Epoch 00237: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1228 - accuracy: 0.9479 - val_loss: 0.1509 - val_accuracy: 0.9167\n",
            "Epoch 238/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.1259 - accuracy: 0.9363\n",
            "Epoch 00238: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1211 - accuracy: 0.9375 - val_loss: 0.1322 - val_accuracy: 0.9167\n",
            "Epoch 239/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1129 - accuracy: 0.9519\n",
            "Epoch 00239: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9479 - val_loss: 0.1497 - val_accuracy: 0.9167\n",
            "Epoch 240/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1190 - accuracy: 0.9365\n",
            "Epoch 00240: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.9410 - val_loss: 0.1641 - val_accuracy: 0.9167\n",
            "Epoch 241/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9481\n",
            "Epoch 00241: val_accuracy did not improve from 0.94444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.9479 - val_loss: 0.1432 - val_accuracy: 0.9306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQZz5YXCuY1",
        "outputId": "2750073c-30fd-4f2c-cbaa-01fb20a680fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.title('Train Accuracy vs Val Accuracy')\n",
        "plt.plot(hist.history['accuracy'], label='Train Accuracy', color='black')\n",
        "plt.plot(hist.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXxc0ZzL5AT"
      },
      "source": [
        "x = vid[['size','bitrate','size_diff', 'bitrate_diff']]\n",
        "y = vid.Cls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H94RyPxDIpU2"
      },
      "source": [
        "model.load_weights('model_best.hdf5')\n",
        "model.compile(optimizer='RMSprop',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "pr = model.predict(x, batch_size=3, verbose=0, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRFLZh5aI4sf"
      },
      "source": [
        "pp = pr.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy0J9LcDI5za"
      },
      "source": [
        "yp = []\n",
        "for i in pp:\n",
        "  yp.append(1100+i*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lazGPcapJ2ty",
        "outputId": "5ab03244-e8a3-46d8-fcd5-eebf32e0324d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(yp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1500, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1700, 1600, 1600, 1600, 1600, 1600, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1800, 1700, 1700, 1700, 1700, 1700, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1700, 1800, 1800, 1800, 1800, 1900, 1800, 1800, 1800, 1800, 1800, 1800, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1800, 1900, 2000, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1800, 1800, 1800, 1800, 2000, 2000, 1900, 1900, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz8Cwy8FJ4UT"
      },
      "source": [
        "yt = []\n",
        "for i in y:\n",
        "  yt.append(1100+i*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veCuTd5-KVU2",
        "outputId": "d758fb17-e09e-4c4b-b757-b41dfdc9e903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(yt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuoOmt6TKWvH",
        "outputId": "c17567ff-faf3-42c1-e236-b7e099d07d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(yt, yp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.444444444444445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBEudwyFtDgd"
      },
      "source": [
        "issue = pd.read_csv(\"/content/received.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yBxFtasyltA",
        "outputId": "2549fbe0-3f5c-425a-f089-483593d33506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "issue.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>FileSize</th>\n",
              "      <th>BitRate</th>\n",
              "      <th>size_diff</th>\n",
              "      <th>B_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.mp4</td>\n",
              "      <td>33.9</td>\n",
              "      <td>1388</td>\n",
              "      <td>7.3</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.mp4</td>\n",
              "      <td>35.2</td>\n",
              "      <td>1441</td>\n",
              "      <td>6.0</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.mp4</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1473</td>\n",
              "      <td>5.2</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.mp4</td>\n",
              "      <td>35.9</td>\n",
              "      <td>1469</td>\n",
              "      <td>5.3</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.mp4</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1473</td>\n",
              "      <td>5.2</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    name  FileSize  BitRate  size_diff  B_diff\n",
              "0  0.mp4      33.9     1388        7.3     168\n",
              "1  1.mp4      35.2     1441        6.0     115\n",
              "2  2.mp4      36.0     1473        5.2      83\n",
              "3  3.mp4      35.9     1469        5.3      87\n",
              "4  4.mp4      36.0     1473        5.2      83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoeRnqb6ypLO"
      },
      "source": [
        "X_issue = issue[['FileSize','BitRate','size_diff', 'B_diff']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYPBNQtstAxd"
      },
      "source": [
        "pr = model.predict(X_issue, batch_size=3, verbose=0, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-ek1n-yztr"
      },
      "source": [
        "pp = pr.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypJx-GeTy0nE",
        "outputId": "75292a05-02bb-47ab-a6ba-c7f9e531afbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 2, 2, 0, 1, 4, 5, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSwJfQk5y4vF",
        "outputId": "4fe394cf-3971-4d04-875d-30c473c0a2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yp = []\n",
        "for i in pp:\n",
        "  yp.append(1100+i*100)\n",
        "yp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1200, 1300, 1300, 1300, 1300, 1100, 1200, 1500, 1600, 1300]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}