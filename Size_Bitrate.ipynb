{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Size_Bitrate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g_kN59z8xjs"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MojrUBy9Nxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42041533-1236-4645-cf6c-bad52f4c86f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGcHfVma9R1G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d928cb20-8f0a-4eb5-d472-6c3ec2eb3405"
      },
      "source": [
        "#load the dataset\n",
        "vid = pd.read_csv(\"/content/drive/My Drive/video/Training.csv\")\n",
        "#show the data\n",
        "vid.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>ori_file</th>\n",
              "      <th>size</th>\n",
              "      <th>bitrate</th>\n",
              "      <th>size_diff</th>\n",
              "      <th>bitrate_diff</th>\n",
              "      <th>sizediff/ori</th>\n",
              "      <th>bitratediff/ori</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Cls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_0001.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>36.4</td>\n",
              "      <td>1381</td>\n",
              "      <td>11.9</td>\n",
              "      <td>321</td>\n",
              "      <td>0.246377</td>\n",
              "      <td>0.188602</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_001.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>37.3</td>\n",
              "      <td>1414</td>\n",
              "      <td>11.0</td>\n",
              "      <td>288</td>\n",
              "      <td>0.227743</td>\n",
              "      <td>0.169213</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_0025.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>37.5</td>\n",
              "      <td>1419</td>\n",
              "      <td>10.8</td>\n",
              "      <td>283</td>\n",
              "      <td>0.223602</td>\n",
              "      <td>0.166275</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_005.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1440</td>\n",
              "      <td>10.3</td>\n",
              "      <td>262</td>\n",
              "      <td>0.213251</td>\n",
              "      <td>0.153937</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0GHpTnbnTZs_1100kbps_01.mp4</td>\n",
              "      <td>0GHpTnbnTZs</td>\n",
              "      <td>37.2</td>\n",
              "      <td>1411</td>\n",
              "      <td>11.1</td>\n",
              "      <td>291</td>\n",
              "      <td>0.229814</td>\n",
              "      <td>0.170975</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            name     ori_file  ...  Condition  Cls\n",
              "0  0GHpTnbnTZs_1100kbps_0001.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "1   0GHpTnbnTZs_1100kbps_001.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "2  0GHpTnbnTZs_1100kbps_0025.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "3   0GHpTnbnTZs_1100kbps_005.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "4    0GHpTnbnTZs_1100kbps_01.mp4  0GHpTnbnTZs  ...       1100    0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GijQlESk-XS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfd76fe-fa0c-415f-99bb-f709fa320bcc"
      },
      "source": [
        "#train_test_split\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "train, test = split(vid, test_size = 0.20)\n",
        "print (train.shape)\n",
        "print (test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(288, 10)\n",
            "(72, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGDjZbAh-qQT"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "X_train = train[['size','bitrate','size_diff', 'bitrate_diff']]\n",
        "y_train = train.Cls\n",
        "X_test = test[['size','bitrate','size_diff', 'bitrate_diff']]\n",
        "y_test = test.Cls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYkHMCpfADgp"
      },
      "source": [
        "#network construction\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=4,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='RMSProp',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31ynfqd-B9xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90879b5a-b083-41c6-bbe6-1b7a1a11e7a5"
      },
      "source": [
        "#training roughly\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = [\n",
        "    ModelCheckpoint('model_best.hdf5',\n",
        "        monitor='val_accuracy', \n",
        "        save_best_only=True, \n",
        "        mode='max',\n",
        "        verbose=1)\n",
        "]\n",
        "hist=model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[checkpoint],epochs=250, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 10.4839 - accuracy: 0.1303\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.18056, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 9.8891 - accuracy: 0.1319 - val_loss: 3.3691 - val_accuracy: 0.1806\n",
            "Epoch 2/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 3.1137 - accuracy: 0.1088\n",
            "Epoch 00002: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 3.1092 - accuracy: 0.1076 - val_loss: 2.2902 - val_accuracy: 0.1250\n",
            "Epoch 3/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 2.4973 - accuracy: 0.1688\n",
            "Epoch 00003: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.4870 - accuracy: 0.1562 - val_loss: 2.3907 - val_accuracy: 0.1389\n",
            "Epoch 4/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 2.3323 - accuracy: 0.1165\n",
            "Epoch 00004: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.3389 - accuracy: 0.1146 - val_loss: 2.2757 - val_accuracy: 0.1389\n",
            "Epoch 5/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 2.3670 - accuracy: 0.1375\n",
            "Epoch 00005: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.3458 - accuracy: 0.1354 - val_loss: 2.2279 - val_accuracy: 0.1528\n",
            "Epoch 6/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.2629 - accuracy: 0.1515\n",
            "Epoch 00006: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2721 - accuracy: 0.1424 - val_loss: 2.2322 - val_accuracy: 0.1111\n",
            "Epoch 7/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.3142 - accuracy: 0.1512\n",
            "Epoch 00007: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.3077 - accuracy: 0.1424 - val_loss: 2.2497 - val_accuracy: 0.0972\n",
            "Epoch 8/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.2259 - accuracy: 0.1059\n",
            "Epoch 00008: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2332 - accuracy: 0.0972 - val_loss: 2.2188 - val_accuracy: 0.1389\n",
            "Epoch 9/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.2887 - accuracy: 0.1439\n",
            "Epoch 00009: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2861 - accuracy: 0.1389 - val_loss: 2.7596 - val_accuracy: 0.0694\n",
            "Epoch 10/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 2.2264 - accuracy: 0.1594\n",
            "Epoch 00010: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2364 - accuracy: 0.1562 - val_loss: 2.2023 - val_accuracy: 0.1111\n",
            "Epoch 11/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 2.2420 - accuracy: 0.1444\n",
            "Epoch 00011: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2449 - accuracy: 0.1424 - val_loss: 2.1718 - val_accuracy: 0.1250\n",
            "Epoch 12/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 2.2217 - accuracy: 0.1815\n",
            "Epoch 00012: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2217 - accuracy: 0.1806 - val_loss: 2.4752 - val_accuracy: 0.1806\n",
            "Epoch 13/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.2402 - accuracy: 0.1647\n",
            "Epoch 00013: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2330 - accuracy: 0.1701 - val_loss: 2.8122 - val_accuracy: 0.1389\n",
            "Epoch 14/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.2097 - accuracy: 0.1279\n",
            "Epoch 00014: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.2244 - accuracy: 0.1319 - val_loss: 2.4988 - val_accuracy: 0.1250\n",
            "Epoch 15/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.1713 - accuracy: 0.1473\n",
            "Epoch 00015: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1696 - accuracy: 0.1562 - val_loss: 2.2391 - val_accuracy: 0.1389\n",
            "Epoch 16/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.1550 - accuracy: 0.1746\n",
            "Epoch 00016: val_accuracy did not improve from 0.18056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1680 - accuracy: 0.1667 - val_loss: 2.1244 - val_accuracy: 0.1389\n",
            "Epoch 17/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 2.1690 - accuracy: 0.1593\n",
            "Epoch 00017: val_accuracy improved from 0.18056 to 0.19444, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1744 - accuracy: 0.1493 - val_loss: 2.1759 - val_accuracy: 0.1944\n",
            "Epoch 18/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.1875 - accuracy: 0.1373\n",
            "Epoch 00018: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1897 - accuracy: 0.1389 - val_loss: 2.1191 - val_accuracy: 0.1389\n",
            "Epoch 19/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.1931 - accuracy: 0.1724\n",
            "Epoch 00019: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1976 - accuracy: 0.1667 - val_loss: 2.1429 - val_accuracy: 0.1111\n",
            "Epoch 20/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 2.1425 - accuracy: 0.1648\n",
            "Epoch 00020: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1513 - accuracy: 0.1667 - val_loss: 2.1774 - val_accuracy: 0.1528\n",
            "Epoch 21/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.1139 - accuracy: 0.1667\n",
            "Epoch 00021: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 2.1227 - accuracy: 0.1528 - val_loss: 2.1761 - val_accuracy: 0.1528\n",
            "Epoch 22/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 2.1136 - accuracy: 0.1723\n",
            "Epoch 00022: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1271 - accuracy: 0.1667 - val_loss: 2.1068 - val_accuracy: 0.1806\n",
            "Epoch 23/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.0912 - accuracy: 0.1667\n",
            "Epoch 00023: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0987 - accuracy: 0.1632 - val_loss: 2.1484 - val_accuracy: 0.1944\n",
            "Epoch 24/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.1178 - accuracy: 0.2054\n",
            "Epoch 00024: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1252 - accuracy: 0.1944 - val_loss: 2.1112 - val_accuracy: 0.1250\n",
            "Epoch 25/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.1300 - accuracy: 0.1434\n",
            "Epoch 00025: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1326 - accuracy: 0.1424 - val_loss: 2.0643 - val_accuracy: 0.1528\n",
            "Epoch 26/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.1126 - accuracy: 0.1515\n",
            "Epoch 00026: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1114 - accuracy: 0.1493 - val_loss: 2.1638 - val_accuracy: 0.1250\n",
            "Epoch 27/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.1065 - accuracy: 0.2121\n",
            "Epoch 00027: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1046 - accuracy: 0.2049 - val_loss: 2.1858 - val_accuracy: 0.1528\n",
            "Epoch 28/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.1370 - accuracy: 0.1932\n",
            "Epoch 00028: val_accuracy did not improve from 0.19444\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1422 - accuracy: 0.1944 - val_loss: 2.1105 - val_accuracy: 0.1667\n",
            "Epoch 29/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.1358 - accuracy: 0.1825\n",
            "Epoch 00029: val_accuracy improved from 0.19444 to 0.22222, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1303 - accuracy: 0.1840 - val_loss: 2.0541 - val_accuracy: 0.2222\n",
            "Epoch 30/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 2.1075 - accuracy: 0.1847\n",
            "Epoch 00030: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1299 - accuracy: 0.1875 - val_loss: 2.0228 - val_accuracy: 0.1944\n",
            "Epoch 31/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.0774 - accuracy: 0.1569\n",
            "Epoch 00031: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0738 - accuracy: 0.1562 - val_loss: 2.0896 - val_accuracy: 0.1667\n",
            "Epoch 32/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 2.0957 - accuracy: 0.1926\n",
            "Epoch 00032: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.1000 - accuracy: 0.1875 - val_loss: 2.0188 - val_accuracy: 0.1667\n",
            "Epoch 33/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 2.0604 - accuracy: 0.1905\n",
            "Epoch 00033: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0522 - accuracy: 0.2014 - val_loss: 2.0709 - val_accuracy: 0.1389\n",
            "Epoch 34/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.0508 - accuracy: 0.1822\n",
            "Epoch 00034: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0738 - accuracy: 0.1736 - val_loss: 2.0835 - val_accuracy: 0.1806\n",
            "Epoch 35/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.0637 - accuracy: 0.2184\n",
            "Epoch 00035: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0478 - accuracy: 0.2188 - val_loss: 2.1583 - val_accuracy: 0.1389\n",
            "Epoch 36/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.0507 - accuracy: 0.1961\n",
            "Epoch 00036: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0546 - accuracy: 0.1910 - val_loss: 1.9913 - val_accuracy: 0.2083\n",
            "Epoch 37/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.0264 - accuracy: 0.2353\n",
            "Epoch 00037: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0206 - accuracy: 0.2257 - val_loss: 2.2222 - val_accuracy: 0.1944\n",
            "Epoch 38/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.0786 - accuracy: 0.1977\n",
            "Epoch 00038: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0604 - accuracy: 0.1979 - val_loss: 2.0431 - val_accuracy: 0.1944\n",
            "Epoch 39/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 2.0246 - accuracy: 0.1818\n",
            "Epoch 00039: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0242 - accuracy: 0.1979 - val_loss: 2.1945 - val_accuracy: 0.1944\n",
            "Epoch 40/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 2.0195 - accuracy: 0.2414\n",
            "Epoch 00040: val_accuracy did not improve from 0.22222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0396 - accuracy: 0.2292 - val_loss: 1.9653 - val_accuracy: 0.2083\n",
            "Epoch 41/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 2.0089 - accuracy: 0.2195\n",
            "Epoch 00041: val_accuracy improved from 0.22222 to 0.27778, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0148 - accuracy: 0.2153 - val_loss: 1.9826 - val_accuracy: 0.2778\n",
            "Epoch 42/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.9816 - accuracy: 0.2370\n",
            "Epoch 00042: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9918 - accuracy: 0.2326 - val_loss: 2.1389 - val_accuracy: 0.1944\n",
            "Epoch 43/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 2.0181 - accuracy: 0.1888\n",
            "Epoch 00043: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0360 - accuracy: 0.2014 - val_loss: 2.0344 - val_accuracy: 0.2361\n",
            "Epoch 44/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 2.0191 - accuracy: 0.1984\n",
            "Epoch 00044: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0129 - accuracy: 0.2083 - val_loss: 2.2109 - val_accuracy: 0.2222\n",
            "Epoch 45/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.9454 - accuracy: 0.2364\n",
            "Epoch 00045: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9596 - accuracy: 0.2396 - val_loss: 2.1765 - val_accuracy: 0.2361\n",
            "Epoch 46/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.9865 - accuracy: 0.2397\n",
            "Epoch 00046: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0036 - accuracy: 0.2257 - val_loss: 1.9854 - val_accuracy: 0.2083\n",
            "Epoch 47/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.9850 - accuracy: 0.2048\n",
            "Epoch 00047: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9801 - accuracy: 0.2083 - val_loss: 2.0399 - val_accuracy: 0.1667\n",
            "Epoch 48/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 2.0351 - accuracy: 0.2211\n",
            "Epoch 00048: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0331 - accuracy: 0.2257 - val_loss: 2.0107 - val_accuracy: 0.1389\n",
            "Epoch 49/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.9752 - accuracy: 0.2540\n",
            "Epoch 00049: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9980 - accuracy: 0.2500 - val_loss: 1.9312 - val_accuracy: 0.2639\n",
            "Epoch 50/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 2.0479 - accuracy: 0.2110\n",
            "Epoch 00050: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0159 - accuracy: 0.2326 - val_loss: 2.3480 - val_accuracy: 0.1667\n",
            "Epoch 51/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 2.0281 - accuracy: 0.2392\n",
            "Epoch 00051: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0038 - accuracy: 0.2361 - val_loss: 2.0143 - val_accuracy: 0.2222\n",
            "Epoch 52/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.9592 - accuracy: 0.2697\n",
            "Epoch 00052: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9702 - accuracy: 0.2743 - val_loss: 1.9549 - val_accuracy: 0.2222\n",
            "Epoch 53/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 2.0097 - accuracy: 0.2519\n",
            "Epoch 00053: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 2.0043 - accuracy: 0.2465 - val_loss: 2.0844 - val_accuracy: 0.2361\n",
            "Epoch 54/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.9029 - accuracy: 0.3052\n",
            "Epoch 00054: val_accuracy did not improve from 0.27778\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9303 - accuracy: 0.2847 - val_loss: 1.9592 - val_accuracy: 0.2361\n",
            "Epoch 55/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.9819 - accuracy: 0.2538\n",
            "Epoch 00055: val_accuracy improved from 0.27778 to 0.29167, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9663 - accuracy: 0.2604 - val_loss: 1.9523 - val_accuracy: 0.2917\n",
            "Epoch 56/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.9184 - accuracy: 0.2644\n",
            "Epoch 00056: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9258 - accuracy: 0.2639 - val_loss: 1.9599 - val_accuracy: 0.2222\n",
            "Epoch 57/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.9235 - accuracy: 0.2490\n",
            "Epoch 00057: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9474 - accuracy: 0.2396 - val_loss: 2.0778 - val_accuracy: 0.2083\n",
            "Epoch 58/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.9465 - accuracy: 0.2450\n",
            "Epoch 00058: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9409 - accuracy: 0.2500 - val_loss: 1.9242 - val_accuracy: 0.2500\n",
            "Epoch 59/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.9042 - accuracy: 0.2868\n",
            "Epoch 00059: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9153 - accuracy: 0.2847 - val_loss: 2.0402 - val_accuracy: 0.2361\n",
            "Epoch 60/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.8997 - accuracy: 0.2857\n",
            "Epoch 00060: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9117 - accuracy: 0.2812 - val_loss: 2.0729 - val_accuracy: 0.2639\n",
            "Epoch 61/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.8590 - accuracy: 0.2593\n",
            "Epoch 00061: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8778 - accuracy: 0.2604 - val_loss: 2.0667 - val_accuracy: 0.2361\n",
            "Epoch 62/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.8764 - accuracy: 0.2428\n",
            "Epoch 00062: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8820 - accuracy: 0.2396 - val_loss: 2.1516 - val_accuracy: 0.2083\n",
            "Epoch 63/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.9508 - accuracy: 0.2261\n",
            "Epoch 00063: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9428 - accuracy: 0.2465 - val_loss: 2.1903 - val_accuracy: 0.1806\n",
            "Epoch 64/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.8893 - accuracy: 0.2897\n",
            "Epoch 00064: val_accuracy did not improve from 0.29167\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.8633 - accuracy: 0.2812 - val_loss: 2.3131 - val_accuracy: 0.2361\n",
            "Epoch 65/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.9101 - accuracy: 0.2209\n",
            "Epoch 00065: val_accuracy improved from 0.29167 to 0.30556, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.9207 - accuracy: 0.2292 - val_loss: 1.8069 - val_accuracy: 0.3056\n",
            "Epoch 66/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.8248 - accuracy: 0.3125\n",
            "Epoch 00066: val_accuracy did not improve from 0.30556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.8552 - accuracy: 0.2882 - val_loss: 1.8849 - val_accuracy: 0.2917\n",
            "Epoch 67/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.8248 - accuracy: 0.2764\n",
            "Epoch 00067: val_accuracy improved from 0.30556 to 0.33333, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.8254 - accuracy: 0.2639 - val_loss: 1.8855 - val_accuracy: 0.3333\n",
            "Epoch 68/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.8148 - accuracy: 0.2706\n",
            "Epoch 00068: val_accuracy did not improve from 0.33333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7818 - accuracy: 0.2847 - val_loss: 2.3963 - val_accuracy: 0.2361\n",
            "Epoch 69/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.7973 - accuracy: 0.2980\n",
            "Epoch 00069: val_accuracy did not improve from 0.33333\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7889 - accuracy: 0.2917 - val_loss: 1.7115 - val_accuracy: 0.1944\n",
            "Epoch 70/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.8111 - accuracy: 0.2759\n",
            "Epoch 00070: val_accuracy improved from 0.33333 to 0.34722, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7926 - accuracy: 0.2847 - val_loss: 1.7602 - val_accuracy: 0.3472\n",
            "Epoch 71/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.7939 - accuracy: 0.2833\n",
            "Epoch 00071: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7978 - accuracy: 0.2708 - val_loss: 1.8159 - val_accuracy: 0.2222\n",
            "Epoch 72/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.8114 - accuracy: 0.3101\n",
            "Epoch 00072: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7886 - accuracy: 0.3160 - val_loss: 1.7681 - val_accuracy: 0.2639\n",
            "Epoch 73/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.7165 - accuracy: 0.3098\n",
            "Epoch 00073: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7292 - accuracy: 0.3021 - val_loss: 1.8446 - val_accuracy: 0.2222\n",
            "Epoch 74/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.7191 - accuracy: 0.2708\n",
            "Epoch 00074: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7448 - accuracy: 0.2882 - val_loss: 1.9827 - val_accuracy: 0.2361\n",
            "Epoch 75/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.7071 - accuracy: 0.2809\n",
            "Epoch 00075: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.7094 - accuracy: 0.2847 - val_loss: 1.7872 - val_accuracy: 0.1944\n",
            "Epoch 76/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 1.6902 - accuracy: 0.2982\n",
            "Epoch 00076: val_accuracy did not improve from 0.34722\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.6919 - accuracy: 0.2986 - val_loss: 1.8232 - val_accuracy: 0.2917\n",
            "Epoch 77/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.6914 - accuracy: 0.3496\n",
            "Epoch 00077: val_accuracy improved from 0.34722 to 0.40278, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.6683 - accuracy: 0.3438 - val_loss: 1.6171 - val_accuracy: 0.4028\n",
            "Epoch 78/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.7335 - accuracy: 0.2772\n",
            "Epoch 00078: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.7258 - accuracy: 0.2778 - val_loss: 1.6688 - val_accuracy: 0.3194\n",
            "Epoch 79/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.6293 - accuracy: 0.3293\n",
            "Epoch 00079: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.6526 - accuracy: 0.3229 - val_loss: 1.6765 - val_accuracy: 0.2917\n",
            "Epoch 80/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.5190 - accuracy: 0.3444\n",
            "Epoch 00080: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5447 - accuracy: 0.3403 - val_loss: 1.7571 - val_accuracy: 0.2639\n",
            "Epoch 81/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.5336 - accuracy: 0.3699\n",
            "Epoch 00081: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.5397 - accuracy: 0.3438 - val_loss: 1.5673 - val_accuracy: 0.3750\n",
            "Epoch 82/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.6688 - accuracy: 0.3571\n",
            "Epoch 00082: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.6399 - accuracy: 0.3542 - val_loss: 1.5763 - val_accuracy: 0.3194\n",
            "Epoch 83/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.5954 - accuracy: 0.3296\n",
            "Epoch 00083: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5801 - accuracy: 0.3333 - val_loss: 1.4272 - val_accuracy: 0.3889\n",
            "Epoch 84/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 1.5110 - accuracy: 0.3882\n",
            "Epoch 00084: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5218 - accuracy: 0.3750 - val_loss: 1.8735 - val_accuracy: 0.3056\n",
            "Epoch 85/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.5085 - accuracy: 0.3750\n",
            "Epoch 00085: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.5197 - accuracy: 0.3715 - val_loss: 1.6010 - val_accuracy: 0.2917\n",
            "Epoch 86/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 1.5331 - accuracy: 0.3736\n",
            "Epoch 00086: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.5267 - accuracy: 0.3750 - val_loss: 1.4678 - val_accuracy: 0.3056\n",
            "Epoch 87/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.4896 - accuracy: 0.3798\n",
            "Epoch 00087: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4620 - accuracy: 0.3889 - val_loss: 1.6156 - val_accuracy: 0.3472\n",
            "Epoch 88/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.3841 - accuracy: 0.4246\n",
            "Epoch 00088: val_accuracy did not improve from 0.40278\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.4091 - accuracy: 0.4236 - val_loss: 1.7420 - val_accuracy: 0.4028\n",
            "Epoch 89/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.3678 - accuracy: 0.4087\n",
            "Epoch 00089: val_accuracy improved from 0.40278 to 0.47222, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3909 - accuracy: 0.3958 - val_loss: 1.2760 - val_accuracy: 0.4722\n",
            "Epoch 90/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 1.3503 - accuracy: 0.4479\n",
            "Epoch 00090: val_accuracy did not improve from 0.47222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3503 - accuracy: 0.4479 - val_loss: 1.2964 - val_accuracy: 0.4583\n",
            "Epoch 91/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.3910 - accuracy: 0.4337\n",
            "Epoch 00091: val_accuracy did not improve from 0.47222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3831 - accuracy: 0.4375 - val_loss: 1.2631 - val_accuracy: 0.4722\n",
            "Epoch 92/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.4335 - accuracy: 0.3770\n",
            "Epoch 00092: val_accuracy did not improve from 0.47222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3979 - accuracy: 0.4028 - val_loss: 1.8211 - val_accuracy: 0.3472\n",
            "Epoch 93/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.3215 - accuracy: 0.4733\n",
            "Epoch 00093: val_accuracy did not improve from 0.47222\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3629 - accuracy: 0.4618 - val_loss: 1.3403 - val_accuracy: 0.4583\n",
            "Epoch 94/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.3925 - accuracy: 0.3929\n",
            "Epoch 00094: val_accuracy improved from 0.47222 to 0.50000, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.3752 - accuracy: 0.4167 - val_loss: 1.2631 - val_accuracy: 0.5000\n",
            "Epoch 95/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.3583 - accuracy: 0.4583\n",
            "Epoch 00095: val_accuracy did not improve from 0.50000\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.3401 - accuracy: 0.4618 - val_loss: 1.5321 - val_accuracy: 0.4583\n",
            "Epoch 96/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.2941 - accuracy: 0.4625\n",
            "Epoch 00096: val_accuracy did not improve from 0.50000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.3087 - accuracy: 0.4618 - val_loss: 1.3145 - val_accuracy: 0.3333\n",
            "Epoch 97/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 1.2172 - accuracy: 0.4912\n",
            "Epoch 00097: val_accuracy did not improve from 0.50000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2150 - accuracy: 0.4896 - val_loss: 1.5808 - val_accuracy: 0.3611\n",
            "Epoch 98/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 1.2696 - accuracy: 0.4188\n",
            "Epoch 00098: val_accuracy improved from 0.50000 to 0.55556, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2459 - accuracy: 0.4444 - val_loss: 1.0531 - val_accuracy: 0.5556\n",
            "Epoch 99/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 1.2317 - accuracy: 0.4912\n",
            "Epoch 00099: val_accuracy did not improve from 0.55556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2366 - accuracy: 0.4896 - val_loss: 1.3284 - val_accuracy: 0.5278\n",
            "Epoch 100/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.2432 - accuracy: 0.5119\n",
            "Epoch 00100: val_accuracy did not improve from 0.55556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2450 - accuracy: 0.5069 - val_loss: 1.2672 - val_accuracy: 0.4306\n",
            "Epoch 101/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.2138 - accuracy: 0.4897\n",
            "Epoch 00101: val_accuracy did not improve from 0.55556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2018 - accuracy: 0.4965 - val_loss: 1.4307 - val_accuracy: 0.4444\n",
            "Epoch 102/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.1961 - accuracy: 0.5301\n",
            "Epoch 00102: val_accuracy improved from 0.55556 to 0.61111, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1926 - accuracy: 0.5278 - val_loss: 1.3046 - val_accuracy: 0.6111\n",
            "Epoch 103/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.2515 - accuracy: 0.5041\n",
            "Epoch 00103: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.2652 - accuracy: 0.4792 - val_loss: 1.3904 - val_accuracy: 0.3889\n",
            "Epoch 104/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.0021 - accuracy: 0.5969\n",
            "Epoch 00104: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0192 - accuracy: 0.5868 - val_loss: 2.3449 - val_accuracy: 0.2361\n",
            "Epoch 105/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.1484 - accuracy: 0.4980\n",
            "Epoch 00105: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.1121 - accuracy: 0.5278 - val_loss: 1.2132 - val_accuracy: 0.5556\n",
            "Epoch 106/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.1813 - accuracy: 0.5417\n",
            "Epoch 00106: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1794 - accuracy: 0.5347 - val_loss: 1.1525 - val_accuracy: 0.5694\n",
            "Epoch 107/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.1860 - accuracy: 0.5100\n",
            "Epoch 00107: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.1987 - accuracy: 0.4861 - val_loss: 1.1011 - val_accuracy: 0.5556\n",
            "Epoch 108/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 1.0730 - accuracy: 0.5208\n",
            "Epoch 00108: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.0740 - accuracy: 0.5278 - val_loss: 1.9270 - val_accuracy: 0.5000\n",
            "Epoch 109/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 1.1308 - accuracy: 0.5967\n",
            "Epoch 00109: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.1434 - accuracy: 0.5799 - val_loss: 0.9770 - val_accuracy: 0.5833\n",
            "Epoch 110/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 1.0746 - accuracy: 0.5556\n",
            "Epoch 00110: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.0746 - accuracy: 0.5556 - val_loss: 1.7801 - val_accuracy: 0.4306\n",
            "Epoch 111/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 1.0753 - accuracy: 0.5674\n",
            "Epoch 00111: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.0811 - accuracy: 0.5660 - val_loss: 2.0866 - val_accuracy: 0.3056\n",
            "Epoch 112/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.1453 - accuracy: 0.5325\n",
            "Epoch 00112: val_accuracy did not improve from 0.61111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.1772 - accuracy: 0.5243 - val_loss: 1.3797 - val_accuracy: 0.4583\n",
            "Epoch 113/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.1486 - accuracy: 0.5691\n",
            "Epoch 00113: val_accuracy improved from 0.61111 to 0.66667, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.0959 - accuracy: 0.5799 - val_loss: 1.2315 - val_accuracy: 0.6667\n",
            "Epoch 114/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 1.0574 - accuracy: 0.5785\n",
            "Epoch 00114: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1123 - accuracy: 0.5660 - val_loss: 0.9452 - val_accuracy: 0.5694\n",
            "Epoch 115/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 1.1246 - accuracy: 0.5660\n",
            "Epoch 00115: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1246 - accuracy: 0.5660 - val_loss: 1.2249 - val_accuracy: 0.4306\n",
            "Epoch 116/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 1.1060 - accuracy: 0.5814\n",
            "Epoch 00116: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0617 - accuracy: 0.5903 - val_loss: 1.1534 - val_accuracy: 0.5417\n",
            "Epoch 117/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.0574 - accuracy: 0.5913\n",
            "Epoch 00117: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0793 - accuracy: 0.5938 - val_loss: 0.8874 - val_accuracy: 0.6667\n",
            "Epoch 118/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.9717 - accuracy: 0.6057\n",
            "Epoch 00118: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.0117 - accuracy: 0.5972 - val_loss: 1.1637 - val_accuracy: 0.4583\n",
            "Epoch 119/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.9857 - accuracy: 0.6246\n",
            "Epoch 00119: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9881 - accuracy: 0.6250 - val_loss: 1.2266 - val_accuracy: 0.6111\n",
            "Epoch 120/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.9551 - accuracy: 0.5993\n",
            "Epoch 00120: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9780 - accuracy: 0.6042 - val_loss: 1.1492 - val_accuracy: 0.5833\n",
            "Epoch 121/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 1.0859 - accuracy: 0.5823\n",
            "Epoch 00121: val_accuracy did not improve from 0.66667\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 1.0909 - accuracy: 0.5694 - val_loss: 1.0970 - val_accuracy: 0.5278\n",
            "Epoch 122/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.0243 - accuracy: 0.5813\n",
            "Epoch 00122: val_accuracy improved from 0.66667 to 0.80556, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9695 - accuracy: 0.6076 - val_loss: 0.6718 - val_accuracy: 0.8056\n",
            "Epoch 123/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.9587 - accuracy: 0.6304\n",
            "Epoch 00123: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9783 - accuracy: 0.6215 - val_loss: 2.0412 - val_accuracy: 0.3472\n",
            "Epoch 124/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.9265 - accuracy: 0.6466\n",
            "Epoch 00124: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9417 - accuracy: 0.6285 - val_loss: 1.2225 - val_accuracy: 0.5000\n",
            "Epoch 125/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.9669 - accuracy: 0.6382\n",
            "Epoch 00125: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9892 - accuracy: 0.6285 - val_loss: 0.9049 - val_accuracy: 0.6806\n",
            "Epoch 126/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 1.0255 - accuracy: 0.6142\n",
            "Epoch 00126: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9990 - accuracy: 0.6250 - val_loss: 0.9220 - val_accuracy: 0.5694\n",
            "Epoch 127/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.9663 - accuracy: 0.5984\n",
            "Epoch 00127: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9409 - accuracy: 0.6007 - val_loss: 2.9753 - val_accuracy: 0.2639\n",
            "Epoch 128/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 1.0400 - accuracy: 0.6301\n",
            "Epoch 00128: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0164 - accuracy: 0.6389 - val_loss: 0.6881 - val_accuracy: 0.7778\n",
            "Epoch 129/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.9805 - accuracy: 0.6250\n",
            "Epoch 00129: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9984 - accuracy: 0.6111 - val_loss: 0.8819 - val_accuracy: 0.5972\n",
            "Epoch 130/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.9039 - accuracy: 0.6466\n",
            "Epoch 00130: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8565 - accuracy: 0.6597 - val_loss: 1.8057 - val_accuracy: 0.4861\n",
            "Epoch 131/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.9898 - accuracy: 0.6420\n",
            "Epoch 00131: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9499 - accuracy: 0.6458 - val_loss: 1.5004 - val_accuracy: 0.6806\n",
            "Epoch 132/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.9393 - accuracy: 0.6792\n",
            "Epoch 00132: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9229 - accuracy: 0.6701 - val_loss: 0.8154 - val_accuracy: 0.6944\n",
            "Epoch 133/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.9395 - accuracy: 0.6581\n",
            "Epoch 00133: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9609 - accuracy: 0.6389 - val_loss: 0.9891 - val_accuracy: 0.6389\n",
            "Epoch 134/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.9342 - accuracy: 0.6379\n",
            "Epoch 00134: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9788 - accuracy: 0.6354 - val_loss: 0.9390 - val_accuracy: 0.6389\n",
            "Epoch 135/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.9105 - accuracy: 0.6706\n",
            "Epoch 00135: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9141 - accuracy: 0.6701 - val_loss: 2.6204 - val_accuracy: 0.1944\n",
            "Epoch 136/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.9461 - accuracy: 0.6270\n",
            "Epoch 00136: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9325 - accuracy: 0.6181 - val_loss: 1.0996 - val_accuracy: 0.6528\n",
            "Epoch 137/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.9520 - accuracy: 0.6458\n",
            "Epoch 00137: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9520 - accuracy: 0.6458 - val_loss: 0.8053 - val_accuracy: 0.7361\n",
            "Epoch 138/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.9769 - accuracy: 0.6078\n",
            "Epoch 00138: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9510 - accuracy: 0.6181 - val_loss: 1.8535 - val_accuracy: 0.3611\n",
            "Epoch 139/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.9981 - accuracy: 0.5801\n",
            "Epoch 00139: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9545 - accuracy: 0.5972 - val_loss: 0.6738 - val_accuracy: 0.6806\n",
            "Epoch 140/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.8550 - accuracy: 0.6707\n",
            "Epoch 00140: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9311 - accuracy: 0.6562 - val_loss: 1.1680 - val_accuracy: 0.5000\n",
            "Epoch 141/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8367 - accuracy: 0.6749\n",
            "Epoch 00141: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8103 - accuracy: 0.6806 - val_loss: 0.6312 - val_accuracy: 0.7083\n",
            "Epoch 142/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 1.0027 - accuracy: 0.6333\n",
            "Epoch 00142: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0352 - accuracy: 0.6250 - val_loss: 1.1213 - val_accuracy: 0.6667\n",
            "Epoch 143/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.8493 - accuracy: 0.6772\n",
            "Epoch 00143: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8464 - accuracy: 0.6771 - val_loss: 1.2111 - val_accuracy: 0.5417\n",
            "Epoch 144/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.9225 - accuracy: 0.6296\n",
            "Epoch 00144: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9685 - accuracy: 0.6146 - val_loss: 0.5404 - val_accuracy: 0.7778\n",
            "Epoch 145/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.8674 - accuracy: 0.6707\n",
            "Epoch 00145: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8811 - accuracy: 0.6701 - val_loss: 0.7310 - val_accuracy: 0.7361\n",
            "Epoch 146/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8443 - accuracy: 0.6955\n",
            "Epoch 00146: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8378 - accuracy: 0.6910 - val_loss: 1.3729 - val_accuracy: 0.4583\n",
            "Epoch 147/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.9470 - accuracy: 0.6344\n",
            "Epoch 00147: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9351 - accuracy: 0.6389 - val_loss: 2.3372 - val_accuracy: 0.2917\n",
            "Epoch 148/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.9180 - accuracy: 0.6771\n",
            "Epoch 00148: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9180 - accuracy: 0.6771 - val_loss: 0.8671 - val_accuracy: 0.6250\n",
            "Epoch 149/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.7644 - accuracy: 0.7065\n",
            "Epoch 00149: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7634 - accuracy: 0.7049 - val_loss: 2.5431 - val_accuracy: 0.4306\n",
            "Epoch 150/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.9219 - accuracy: 0.6502\n",
            "Epoch 00150: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8979 - accuracy: 0.6597 - val_loss: 1.2170 - val_accuracy: 0.5972\n",
            "Epoch 151/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.9549 - accuracy: 0.6626\n",
            "Epoch 00151: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8833 - accuracy: 0.6771 - val_loss: 0.7215 - val_accuracy: 0.7361\n",
            "Epoch 152/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.9820 - accuracy: 0.6154\n",
            "Epoch 00152: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9203 - accuracy: 0.6319 - val_loss: 0.7896 - val_accuracy: 0.6667\n",
            "Epoch 153/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8221 - accuracy: 0.6417\n",
            "Epoch 00153: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8801 - accuracy: 0.6250 - val_loss: 0.5982 - val_accuracy: 0.7500\n",
            "Epoch 154/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.8495 - accuracy: 0.7092\n",
            "Epoch 00154: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8363 - accuracy: 0.7153 - val_loss: 1.0136 - val_accuracy: 0.5556\n",
            "Epoch 155/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7950 - accuracy: 0.7057\n",
            "Epoch 00155: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7904 - accuracy: 0.7083 - val_loss: 1.0184 - val_accuracy: 0.5694\n",
            "Epoch 156/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7969 - accuracy: 0.7021\n",
            "Epoch 00156: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7825 - accuracy: 0.7083 - val_loss: 0.7819 - val_accuracy: 0.7917\n",
            "Epoch 157/250\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.9186 - accuracy: 0.7119\n",
            "Epoch 00157: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8782 - accuracy: 0.7257 - val_loss: 1.4495 - val_accuracy: 0.5000\n",
            "Epoch 158/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.9041 - accuracy: 0.6739\n",
            "Epoch 00158: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8968 - accuracy: 0.6771 - val_loss: 0.6378 - val_accuracy: 0.6944\n",
            "Epoch 159/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.8347 - accuracy: 0.6854\n",
            "Epoch 00159: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8468 - accuracy: 0.6840 - val_loss: 1.4003 - val_accuracy: 0.5278\n",
            "Epoch 160/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.8754 - accuracy: 0.6701\n",
            "Epoch 00160: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8754 - accuracy: 0.6701 - val_loss: 1.2379 - val_accuracy: 0.5278\n",
            "Epoch 161/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.9066 - accuracy: 0.6912\n",
            "Epoch 00161: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9074 - accuracy: 0.6875 - val_loss: 0.8502 - val_accuracy: 0.7500\n",
            "Epoch 162/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.8608 - accuracy: 0.6846\n",
            "Epoch 00162: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8682 - accuracy: 0.6806 - val_loss: 1.0766 - val_accuracy: 0.5694\n",
            "Epoch 163/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.7470 - accuracy: 0.7118\n",
            "Epoch 00163: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7470 - accuracy: 0.7118 - val_loss: 0.7280 - val_accuracy: 0.6806\n",
            "Epoch 164/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.8624 - accuracy: 0.7057\n",
            "Epoch 00164: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8619 - accuracy: 0.7083 - val_loss: 1.6595 - val_accuracy: 0.5417\n",
            "Epoch 165/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.9033 - accuracy: 0.7074\n",
            "Epoch 00165: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9112 - accuracy: 0.7118 - val_loss: 1.5599 - val_accuracy: 0.4306\n",
            "Epoch 166/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.8625 - accuracy: 0.7191\n",
            "Epoch 00166: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9014 - accuracy: 0.7049 - val_loss: 0.9635 - val_accuracy: 0.5833\n",
            "Epoch 167/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.8162 - accuracy: 0.7016\n",
            "Epoch 00167: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8092 - accuracy: 0.7014 - val_loss: 1.1308 - val_accuracy: 0.5833\n",
            "Epoch 168/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.8639 - accuracy: 0.7168\n",
            "Epoch 00168: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8619 - accuracy: 0.7188 - val_loss: 0.6650 - val_accuracy: 0.7222\n",
            "Epoch 169/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.8344 - accuracy: 0.6667\n",
            "Epoch 00169: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8150 - accuracy: 0.6736 - val_loss: 1.3285 - val_accuracy: 0.5417\n",
            "Epoch 170/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.7419 - accuracy: 0.7125\n",
            "Epoch 00170: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8347 - accuracy: 0.6910 - val_loss: 0.5934 - val_accuracy: 0.7222\n",
            "Epoch 171/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7230 - accuracy: 0.7637\n",
            "Epoch 00171: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6978 - accuracy: 0.7604 - val_loss: 0.9938 - val_accuracy: 0.6667\n",
            "Epoch 172/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.7907 - accuracy: 0.7548\n",
            "Epoch 00172: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7735 - accuracy: 0.7465 - val_loss: 0.5580 - val_accuracy: 0.7500\n",
            "Epoch 173/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.9152 - accuracy: 0.6897\n",
            "Epoch 00173: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9278 - accuracy: 0.6944 - val_loss: 0.6862 - val_accuracy: 0.7639\n",
            "Epoch 174/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.7318 - accuracy: 0.7479\n",
            "Epoch 00174: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7671 - accuracy: 0.7292 - val_loss: 1.2292 - val_accuracy: 0.6389\n",
            "Epoch 175/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7713 - accuracy: 0.7070\n",
            "Epoch 00175: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7764 - accuracy: 0.7014 - val_loss: 0.5000 - val_accuracy: 0.7778\n",
            "Epoch 176/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.8139 - accuracy: 0.6818\n",
            "Epoch 00176: val_accuracy did not improve from 0.80556\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7956 - accuracy: 0.6840 - val_loss: 0.9579 - val_accuracy: 0.6111\n",
            "Epoch 177/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.8129 - accuracy: 0.7460\n",
            "Epoch 00177: val_accuracy improved from 0.80556 to 0.86111, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7786 - accuracy: 0.7604 - val_loss: 0.3854 - val_accuracy: 0.8611\n",
            "Epoch 178/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7395 - accuracy: 0.7270\n",
            "Epoch 00178: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7447 - accuracy: 0.7222 - val_loss: 0.6927 - val_accuracy: 0.7639\n",
            "Epoch 179/250\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.9538 - accuracy: 0.7121\n",
            "Epoch 00179: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9485 - accuracy: 0.7083 - val_loss: 0.7429 - val_accuracy: 0.7500\n",
            "Epoch 180/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.6568 - accuracy: 0.7173\n",
            "Epoch 00180: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.7326 - val_loss: 1.5415 - val_accuracy: 0.5694\n",
            "Epoch 181/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.7954 - accuracy: 0.6992\n",
            "Epoch 00181: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8339 - accuracy: 0.6979 - val_loss: 0.9217 - val_accuracy: 0.6806\n",
            "Epoch 182/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.7793 - accuracy: 0.7590\n",
            "Epoch 00182: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7869 - accuracy: 0.7535 - val_loss: 0.6941 - val_accuracy: 0.7500\n",
            "Epoch 183/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.8105 - accuracy: 0.6838\n",
            "Epoch 00183: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8016 - accuracy: 0.6944 - val_loss: 0.9931 - val_accuracy: 0.6806\n",
            "Epoch 184/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 1.0489 - accuracy: 0.6883\n",
            "Epoch 00184: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0153 - accuracy: 0.6979 - val_loss: 0.5186 - val_accuracy: 0.8056\n",
            "Epoch 185/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8658 - accuracy: 0.6917\n",
            "Epoch 00185: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8606 - accuracy: 0.7083 - val_loss: 0.5721 - val_accuracy: 0.7639\n",
            "Epoch 186/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.7730 - accuracy: 0.7542\n",
            "Epoch 00186: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7811 - accuracy: 0.7465 - val_loss: 1.2745 - val_accuracy: 0.5417\n",
            "Epoch 187/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.7047 - accuracy: 0.7446\n",
            "Epoch 00187: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7018 - accuracy: 0.7396 - val_loss: 1.2116 - val_accuracy: 0.7222\n",
            "Epoch 188/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.9020 - accuracy: 0.6463\n",
            "Epoch 00188: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8392 - accuracy: 0.6667 - val_loss: 0.5009 - val_accuracy: 0.7361\n",
            "Epoch 189/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.7570 - accuracy: 0.7083\n",
            "Epoch 00189: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7570 - accuracy: 0.7083 - val_loss: 0.7876 - val_accuracy: 0.7361\n",
            "Epoch 190/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.8692 - accuracy: 0.7458\n",
            "Epoch 00190: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8123 - accuracy: 0.7708 - val_loss: 1.1602 - val_accuracy: 0.6667\n",
            "Epoch 191/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7341 - accuracy: 0.7057\n",
            "Epoch 00191: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7416 - accuracy: 0.7049 - val_loss: 1.3691 - val_accuracy: 0.5833\n",
            "Epoch 192/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.7999 - accuracy: 0.7153\n",
            "Epoch 00192: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7999 - accuracy: 0.7153 - val_loss: 1.6452 - val_accuracy: 0.5000\n",
            "Epoch 193/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7701 - accuracy: 0.7679\n",
            "Epoch 00193: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7820 - accuracy: 0.7604 - val_loss: 0.9100 - val_accuracy: 0.7083\n",
            "Epoch 194/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.7955 - accuracy: 0.7326\n",
            "Epoch 00194: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7955 - accuracy: 0.7326 - val_loss: 0.8815 - val_accuracy: 0.6806\n",
            "Epoch 195/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.7431\n",
            "Epoch 00195: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6449 - accuracy: 0.7431 - val_loss: 1.5027 - val_accuracy: 0.5417\n",
            "Epoch 196/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.7443 - accuracy: 0.7302\n",
            "Epoch 00196: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7844 - accuracy: 0.7257 - val_loss: 0.4667 - val_accuracy: 0.7639\n",
            "Epoch 197/250\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.9104 - accuracy: 0.6935\n",
            "Epoch 00197: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8899 - accuracy: 0.6944 - val_loss: 0.8893 - val_accuracy: 0.6667\n",
            "Epoch 198/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.8745 - accuracy: 0.7054\n",
            "Epoch 00198: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8321 - accuracy: 0.7188 - val_loss: 0.8928 - val_accuracy: 0.7500\n",
            "Epoch 199/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.7587 - accuracy: 0.7607\n",
            "Epoch 00199: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7830 - accuracy: 0.7535 - val_loss: 0.8571 - val_accuracy: 0.7361\n",
            "Epoch 200/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.8817 - accuracy: 0.7240\n",
            "Epoch 00200: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8664 - accuracy: 0.7292 - val_loss: 0.7855 - val_accuracy: 0.6944\n",
            "Epoch 201/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.8200 - accuracy: 0.7576\n",
            "Epoch 00201: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8512 - accuracy: 0.7361 - val_loss: 0.7492 - val_accuracy: 0.7222\n",
            "Epoch 202/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7467 - accuracy: 0.7057\n",
            "Epoch 00202: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7394 - accuracy: 0.7083 - val_loss: 0.9597 - val_accuracy: 0.6944\n",
            "Epoch 203/250\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.7236 - accuracy: 0.7529\n",
            "Epoch 00203: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8105 - accuracy: 0.7292 - val_loss: 0.6362 - val_accuracy: 0.7639\n",
            "Epoch 204/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.7453 - accuracy: 0.7566\n",
            "Epoch 00204: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7589 - accuracy: 0.7465 - val_loss: 0.9617 - val_accuracy: 0.7083\n",
            "Epoch 205/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.8456 - accuracy: 0.7333\n",
            "Epoch 00205: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8384 - accuracy: 0.7361 - val_loss: 1.0179 - val_accuracy: 0.5833\n",
            "Epoch 206/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7052 - accuracy: 0.6835\n",
            "Epoch 00206: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7871 - accuracy: 0.6875 - val_loss: 0.6021 - val_accuracy: 0.8194\n",
            "Epoch 207/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.6703 - accuracy: 0.7542\n",
            "Epoch 00207: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.7535 - val_loss: 0.9663 - val_accuracy: 0.5417\n",
            "Epoch 208/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.8695 - accuracy: 0.7163\n",
            "Epoch 00208: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8920 - accuracy: 0.7153 - val_loss: 1.4630 - val_accuracy: 0.5833\n",
            "Epoch 209/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.8783 - accuracy: 0.6877\n",
            "Epoch 00209: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8752 - accuracy: 0.6875 - val_loss: 0.9424 - val_accuracy: 0.6111\n",
            "Epoch 210/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7464 - accuracy: 0.7173\n",
            "Epoch 00210: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7428 - accuracy: 0.7188 - val_loss: 0.9952 - val_accuracy: 0.5556\n",
            "Epoch 211/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.7220 - accuracy: 0.7333\n",
            "Epoch 00211: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7344 - accuracy: 0.7292 - val_loss: 0.4285 - val_accuracy: 0.8194\n",
            "Epoch 212/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7436 - accuracy: 0.7411\n",
            "Epoch 00212: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7358 - accuracy: 0.7465 - val_loss: 0.5729 - val_accuracy: 0.7917\n",
            "Epoch 213/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.7064 - accuracy: 0.7375\n",
            "Epoch 00213: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7179 - accuracy: 0.7326 - val_loss: 0.3559 - val_accuracy: 0.8194\n",
            "Epoch 214/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.7041 - accuracy: 0.7428\n",
            "Epoch 00214: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7422 - accuracy: 0.7257 - val_loss: 0.4105 - val_accuracy: 0.8333\n",
            "Epoch 215/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7238 - accuracy: 0.7595\n",
            "Epoch 00215: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7170 - accuracy: 0.7535 - val_loss: 0.8538 - val_accuracy: 0.7222\n",
            "Epoch 216/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.8782 - accuracy: 0.7215\n",
            "Epoch 00216: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8323 - accuracy: 0.7222 - val_loss: 0.9893 - val_accuracy: 0.5694\n",
            "Epoch 217/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.7380 - accuracy: 0.7404\n",
            "Epoch 00217: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7518 - accuracy: 0.7361 - val_loss: 0.7028 - val_accuracy: 0.7639\n",
            "Epoch 218/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.6901 - accuracy: 0.7350\n",
            "Epoch 00218: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.7292 - val_loss: 1.0129 - val_accuracy: 0.6667\n",
            "Epoch 219/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.8074 - accuracy: 0.7431\n",
            "Epoch 00219: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8074 - accuracy: 0.7431 - val_loss: 1.0062 - val_accuracy: 0.7083\n",
            "Epoch 220/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.7268 - accuracy: 0.7222\n",
            "Epoch 00220: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7268 - accuracy: 0.7222 - val_loss: 1.2241 - val_accuracy: 0.7222\n",
            "Epoch 221/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.7456 - accuracy: 0.7350\n",
            "Epoch 00221: val_accuracy improved from 0.86111 to 0.87500, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7037 - accuracy: 0.7396 - val_loss: 0.3355 - val_accuracy: 0.8750\n",
            "Epoch 222/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.8003 - accuracy: 0.7599\n",
            "Epoch 00222: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7824 - accuracy: 0.7639 - val_loss: 0.7618 - val_accuracy: 0.6944\n",
            "Epoch 223/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7849 - accuracy: 0.7553\n",
            "Epoch 00223: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7926 - accuracy: 0.7431 - val_loss: 0.7831 - val_accuracy: 0.8056\n",
            "Epoch 224/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.8339 - accuracy: 0.7480\n",
            "Epoch 00224: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9176 - accuracy: 0.7222 - val_loss: 1.2980 - val_accuracy: 0.7639\n",
            "Epoch 225/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.6929 - accuracy: 0.7359\n",
            "Epoch 00225: val_accuracy improved from 0.87500 to 0.88889, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.7396 - val_loss: 0.3483 - val_accuracy: 0.8889\n",
            "Epoch 226/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.7615 - accuracy: 0.7602\n",
            "Epoch 00226: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8014 - accuracy: 0.7535 - val_loss: 1.4659 - val_accuracy: 0.7500\n",
            "Epoch 227/250\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.8264 - accuracy: 0.7464\n",
            "Epoch 00227: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8082 - accuracy: 0.7500 - val_loss: 1.7410 - val_accuracy: 0.5972\n",
            "Epoch 228/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.6380 - accuracy: 0.7730\n",
            "Epoch 00228: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.6395 - accuracy: 0.7743 - val_loss: 0.5826 - val_accuracy: 0.8472\n",
            "Epoch 229/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.7720 - accuracy: 0.7349\n",
            "Epoch 00229: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7780 - accuracy: 0.7361 - val_loss: 1.3062 - val_accuracy: 0.7500\n",
            "Epoch 230/250\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.7430 - accuracy: 0.7590\n",
            "Epoch 00230: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7630 - accuracy: 0.7535 - val_loss: 0.5376 - val_accuracy: 0.7917\n",
            "Epoch 231/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.7687 - accuracy: 0.7692\n",
            "Epoch 00231: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6651 - accuracy: 0.8021 - val_loss: 1.3046 - val_accuracy: 0.6667\n",
            "Epoch 232/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.8214 - accuracy: 0.7527\n",
            "Epoch 00232: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8101 - accuracy: 0.7535 - val_loss: 2.0442 - val_accuracy: 0.4444\n",
            "Epoch 233/250\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.7532 - accuracy: 0.7350\n",
            "Epoch 00233: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8768 - accuracy: 0.7222 - val_loss: 0.8245 - val_accuracy: 0.7222\n",
            "Epoch 234/250\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.7630 - accuracy: 0.7708\n",
            "Epoch 00234: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7630 - accuracy: 0.7708 - val_loss: 1.0312 - val_accuracy: 0.7361\n",
            "Epoch 235/250\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.6639 - accuracy: 0.7698\n",
            "Epoch 00235: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.6984 - accuracy: 0.7604 - val_loss: 1.4624 - val_accuracy: 0.5833\n",
            "Epoch 236/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.7107 - accuracy: 0.7708\n",
            "Epoch 00236: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7147 - accuracy: 0.7708 - val_loss: 0.9923 - val_accuracy: 0.6389\n",
            "Epoch 237/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.8658 - accuracy: 0.7679\n",
            "Epoch 00237: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9045 - accuracy: 0.7326 - val_loss: 1.2157 - val_accuracy: 0.6528\n",
            "Epoch 238/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7354 - accuracy: 0.7595\n",
            "Epoch 00238: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7739 - accuracy: 0.7465 - val_loss: 0.7026 - val_accuracy: 0.7500\n",
            "Epoch 239/250\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.7665 - accuracy: 0.7599\n",
            "Epoch 00239: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8014 - accuracy: 0.7569 - val_loss: 1.2601 - val_accuracy: 0.6667\n",
            "Epoch 240/250\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7028 - accuracy: 0.7791\n",
            "Epoch 00240: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7276 - accuracy: 0.7708 - val_loss: 1.1369 - val_accuracy: 0.7500\n",
            "Epoch 241/250\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.8238 - accuracy: 0.7444\n",
            "Epoch 00241: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7864 - accuracy: 0.7569 - val_loss: 0.8251 - val_accuracy: 0.7778\n",
            "Epoch 242/250\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.7536 - accuracy: 0.7215\n",
            "Epoch 00242: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7618 - accuracy: 0.7118 - val_loss: 0.8080 - val_accuracy: 0.7222\n",
            "Epoch 243/250\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.7328 - accuracy: 0.7273\n",
            "Epoch 00243: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7348 - accuracy: 0.7361 - val_loss: 0.9228 - val_accuracy: 0.6806\n",
            "Epoch 244/250\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.7371 - accuracy: 0.7561\n",
            "Epoch 00244: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8205 - accuracy: 0.7326 - val_loss: 0.5724 - val_accuracy: 0.7500\n",
            "Epoch 245/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.7427 - accuracy: 0.7474\n",
            "Epoch 00245: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7370 - accuracy: 0.7500 - val_loss: 1.2120 - val_accuracy: 0.5833\n",
            "Epoch 246/250\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8182 - accuracy: 0.7509\n",
            "Epoch 00246: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.9322 - accuracy: 0.7396 - val_loss: 0.4784 - val_accuracy: 0.8333\n",
            "Epoch 247/250\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.7090 - accuracy: 0.7378\n",
            "Epoch 00247: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7155 - accuracy: 0.7326 - val_loss: 0.8724 - val_accuracy: 0.7500\n",
            "Epoch 248/250\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.6859 - accuracy: 0.7542\n",
            "Epoch 00248: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7598 - accuracy: 0.7465 - val_loss: 0.8009 - val_accuracy: 0.7917\n",
            "Epoch 249/250\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.9270 - accuracy: 0.7298\n",
            "Epoch 00249: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9207 - accuracy: 0.7292 - val_loss: 0.5759 - val_accuracy: 0.7500\n",
            "Epoch 250/250\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7972 - accuracy: 0.7447\n",
            "Epoch 00250: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7860 - accuracy: 0.7500 - val_loss: 0.4770 - val_accuracy: 0.8194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1sUpso8dlkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763b54e3-7097-4065-820a-22625fdcb4ba"
      },
      "source": [
        "#training carefully\n",
        "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "stop = EarlyStopping(monitor='val_accuracy',  patience=70, verbose=0, mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=25, verbose=1,\n",
        "    factor=0.5,\n",
        "    min_lr = 0.00001,\n",
        "    cooldown=0\n",
        ")\n",
        "checkpoint = [ModelCheckpoint('model_best.hdf5',monitor='val_accuracy', \n",
        "        save_best_only=True, \n",
        "        mode='max',\n",
        "        verbose=1)\n",
        "]\n",
        "hist=model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[checkpoint,reduce_lr,stop],epochs=500, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.7535\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75000, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.7535 - val_loss: 0.9487 - val_accuracy: 0.7500\n",
            "Epoch 2/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.7712 - accuracy: 0.7333\n",
            "Epoch 00002: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7893 - accuracy: 0.7326 - val_loss: 0.8018 - val_accuracy: 0.7361\n",
            "Epoch 3/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.6309 - accuracy: 0.7912\n",
            "Epoch 00003: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6297 - accuracy: 0.7882 - val_loss: 1.2463 - val_accuracy: 0.7500\n",
            "Epoch 4/500\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.7321 - accuracy: 0.7474\n",
            "Epoch 00004: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7267 - accuracy: 0.7500 - val_loss: 2.2544 - val_accuracy: 0.5417\n",
            "Epoch 5/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.7920 - accuracy: 0.7303\n",
            "Epoch 00005: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8103 - accuracy: 0.7292 - val_loss: 0.8746 - val_accuracy: 0.6667\n",
            "Epoch 6/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.8474 - accuracy: 0.7341\n",
            "Epoch 00006: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8828 - accuracy: 0.7431 - val_loss: 0.8348 - val_accuracy: 0.6667\n",
            "Epoch 7/500\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.7732 - accuracy: 0.7625\n",
            "Epoch 00007: val_accuracy did not improve from 0.75000\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7740 - accuracy: 0.7708 - val_loss: 1.1246 - val_accuracy: 0.6944\n",
            "Epoch 8/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.9694 - accuracy: 0.7116\n",
            "Epoch 00008: val_accuracy improved from 0.75000 to 0.86111, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0155 - accuracy: 0.7049 - val_loss: 0.3424 - val_accuracy: 0.8611\n",
            "Epoch 9/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.6121 - accuracy: 0.7715\n",
            "Epoch 00009: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5740 - accuracy: 0.7882 - val_loss: 0.9312 - val_accuracy: 0.7361\n",
            "Epoch 10/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.6948 - accuracy: 0.7841\n",
            "Epoch 00010: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.7847 - val_loss: 1.1719 - val_accuracy: 0.6806\n",
            "Epoch 11/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7957 - accuracy: 0.7558\n",
            "Epoch 00011: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8122 - accuracy: 0.7604 - val_loss: 0.8063 - val_accuracy: 0.7639\n",
            "Epoch 12/500\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.7546 - accuracy: 0.7660\n",
            "Epoch 00012: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7412 - accuracy: 0.7708 - val_loss: 0.5265 - val_accuracy: 0.8333\n",
            "Epoch 13/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.8564 - accuracy: 0.7103\n",
            "Epoch 00013: val_accuracy did not improve from 0.86111\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8124 - accuracy: 0.7257 - val_loss: 0.6363 - val_accuracy: 0.7917\n",
            "Epoch 14/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.6892 - accuracy: 0.7640\n",
            "Epoch 00014: val_accuracy improved from 0.86111 to 0.87500, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.6897 - accuracy: 0.7674 - val_loss: 0.3925 - val_accuracy: 0.8750\n",
            "Epoch 15/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.9312 - accuracy: 0.6951\n",
            "Epoch 00015: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8616 - accuracy: 0.7153 - val_loss: 1.8515 - val_accuracy: 0.5972\n",
            "Epoch 16/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.8499 - accuracy: 0.7603\n",
            "Epoch 00016: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8477 - accuracy: 0.7500 - val_loss: 0.6369 - val_accuracy: 0.7222\n",
            "Epoch 17/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.5846 - accuracy: 0.7922\n",
            "Epoch 00017: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5933 - accuracy: 0.7882 - val_loss: 1.4128 - val_accuracy: 0.5833\n",
            "Epoch 18/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.6925 - accuracy: 0.7132\n",
            "Epoch 00018: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6781 - accuracy: 0.7153 - val_loss: 0.5169 - val_accuracy: 0.8056\n",
            "Epoch 19/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7098 - accuracy: 0.7558\n",
            "Epoch 00019: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7023 - accuracy: 0.7500 - val_loss: 0.6666 - val_accuracy: 0.7917\n",
            "Epoch 20/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.9270 - accuracy: 0.6911\n",
            "Epoch 00020: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8612 - accuracy: 0.7188 - val_loss: 2.3270 - val_accuracy: 0.6944\n",
            "Epoch 21/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.8478 - accuracy: 0.7579\n",
            "Epoch 00021: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9389 - accuracy: 0.7465 - val_loss: 0.9940 - val_accuracy: 0.5972\n",
            "Epoch 22/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.6427 - accuracy: 0.7828\n",
            "Epoch 00022: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.7847 - val_loss: 1.0392 - val_accuracy: 0.6389\n",
            "Epoch 23/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.7539 - accuracy: 0.7529\n",
            "Epoch 00023: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7807 - accuracy: 0.7431 - val_loss: 1.2733 - val_accuracy: 0.7222\n",
            "Epoch 24/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.9656 - accuracy: 0.7132\n",
            "Epoch 00024: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9406 - accuracy: 0.7153 - val_loss: 1.1352 - val_accuracy: 0.6389\n",
            "Epoch 25/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7178 - accuracy: 0.7791\n",
            "Epoch 00025: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.7917 - val_loss: 0.6549 - val_accuracy: 0.7500\n",
            "Epoch 26/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 1.0758 - accuracy: 0.7424\n",
            "Epoch 00026: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.0480 - accuracy: 0.7431 - val_loss: 0.4336 - val_accuracy: 0.8194\n",
            "Epoch 27/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.6543 - accuracy: 0.7613\n",
            "Epoch 00027: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6536 - accuracy: 0.7569 - val_loss: 1.3173 - val_accuracy: 0.6528\n",
            "Epoch 28/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.7444 - accuracy: 0.7399\n",
            "Epoch 00028: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7450 - accuracy: 0.7292 - val_loss: 0.6736 - val_accuracy: 0.7778\n",
            "Epoch 29/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.9107 - accuracy: 0.7538\n",
            "Epoch 00029: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8841 - accuracy: 0.7604 - val_loss: 0.8811 - val_accuracy: 0.8056\n",
            "Epoch 30/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.8911 - accuracy: 0.7481\n",
            "Epoch 00030: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8407 - accuracy: 0.7569 - val_loss: 0.9234 - val_accuracy: 0.7639\n",
            "Epoch 31/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.9310 - accuracy: 0.7179\n",
            "Epoch 00031: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9709 - accuracy: 0.7153 - val_loss: 0.9725 - val_accuracy: 0.7361\n",
            "Epoch 32/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.9857 - accuracy: 0.7197\n",
            "Epoch 00032: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9571 - accuracy: 0.7222 - val_loss: 0.5462 - val_accuracy: 0.8056\n",
            "Epoch 33/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.7694 - accuracy: 0.7421\n",
            "Epoch 00033: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8075 - accuracy: 0.7361 - val_loss: 0.6817 - val_accuracy: 0.7222\n",
            "Epoch 34/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.7486 - accuracy: 0.7603\n",
            "Epoch 00034: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7345 - accuracy: 0.7569 - val_loss: 0.4934 - val_accuracy: 0.8056\n",
            "Epoch 35/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.7460 - accuracy: 0.7630\n",
            "Epoch 00035: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7671 - accuracy: 0.7500 - val_loss: 0.6535 - val_accuracy: 0.7222\n",
            "Epoch 36/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.6375 - accuracy: 0.7593\n",
            "Epoch 00036: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6980 - accuracy: 0.7431 - val_loss: 1.3642 - val_accuracy: 0.7083\n",
            "Epoch 37/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.8518 - accuracy: 0.7857\n",
            "Epoch 00037: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.8023 - accuracy: 0.7917 - val_loss: 1.4397 - val_accuracy: 0.6250\n",
            "Epoch 38/500\n",
            "75/96 [======================>.......] - ETA: 0s - loss: 0.7455 - accuracy: 0.7511\n",
            "Epoch 00038: val_accuracy did not improve from 0.87500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7509 - accuracy: 0.7396 - val_loss: 0.6035 - val_accuracy: 0.7639\n",
            "Epoch 39/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.7667 - accuracy: 0.7395\n",
            "Epoch 00039: val_accuracy improved from 0.87500 to 0.88889, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8065 - accuracy: 0.7292 - val_loss: 0.3215 - val_accuracy: 0.8889\n",
            "Epoch 40/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7234 - accuracy: 0.7829\n",
            "Epoch 00040: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7377 - accuracy: 0.7674 - val_loss: 0.5959 - val_accuracy: 0.7917\n",
            "Epoch 41/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.7103 - accuracy: 0.7961\n",
            "Epoch 00041: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7345 - accuracy: 0.7847 - val_loss: 0.8849 - val_accuracy: 0.6111\n",
            "Epoch 42/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.6842 - accuracy: 0.8106\n",
            "Epoch 00042: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.8021 - val_loss: 0.5245 - val_accuracy: 0.8611\n",
            "Epoch 43/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.8926 - accuracy: 0.7586\n",
            "Epoch 00043: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8370 - accuracy: 0.7708 - val_loss: 0.9755 - val_accuracy: 0.7639\n",
            "Epoch 44/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.7306 - accuracy: 0.7349\n",
            "Epoch 00044: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8464 - accuracy: 0.7222 - val_loss: 0.9294 - val_accuracy: 0.6250\n",
            "Epoch 45/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8221 - accuracy: 0.7490\n",
            "Epoch 00045: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7835 - accuracy: 0.7569 - val_loss: 1.7362 - val_accuracy: 0.5139\n",
            "Epoch 46/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.8114 - accuracy: 0.7395\n",
            "Epoch 00046: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8673 - accuracy: 0.7326 - val_loss: 0.8032 - val_accuracy: 0.8194\n",
            "Epoch 47/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.6466 - accuracy: 0.7843\n",
            "Epoch 00047: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7356 - accuracy: 0.7604 - val_loss: 0.8980 - val_accuracy: 0.7361\n",
            "Epoch 48/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.7941 - accuracy: 0.7341\n",
            "Epoch 00048: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7799 - accuracy: 0.7396 - val_loss: 0.6139 - val_accuracy: 0.7778\n",
            "Epoch 49/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.6950 - accuracy: 0.7309\n",
            "Epoch 00049: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7831 - accuracy: 0.7188 - val_loss: 1.5302 - val_accuracy: 0.6250\n",
            "Epoch 50/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.6104 - accuracy: 0.7674\n",
            "Epoch 00050: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6494 - accuracy: 0.7708 - val_loss: 1.3559 - val_accuracy: 0.6389\n",
            "Epoch 51/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.9293 - accuracy: 0.7159\n",
            "Epoch 00051: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9079 - accuracy: 0.7188 - val_loss: 0.7563 - val_accuracy: 0.7500\n",
            "Epoch 52/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.7054 - accuracy: 0.7927\n",
            "Epoch 00052: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7119 - accuracy: 0.7847 - val_loss: 0.8824 - val_accuracy: 0.7500\n",
            "Epoch 53/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8740 - accuracy: 0.7692\n",
            "Epoch 00053: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8413 - accuracy: 0.7778 - val_loss: 1.0828 - val_accuracy: 0.7083\n",
            "Epoch 54/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.6647 - accuracy: 0.7430\n",
            "Epoch 00054: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.7207 - accuracy: 0.7292 - val_loss: 0.3561 - val_accuracy: 0.8472\n",
            "Epoch 55/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.8387 - accuracy: 0.7656\n",
            "Epoch 00055: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8409 - accuracy: 0.7639 - val_loss: 0.6889 - val_accuracy: 0.7361\n",
            "Epoch 56/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.7303 - accuracy: 0.7576\n",
            "Epoch 00056: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8147 - accuracy: 0.7326 - val_loss: 0.3656 - val_accuracy: 0.8333\n",
            "Epoch 57/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.7505 - accuracy: 0.7597\n",
            "Epoch 00057: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8202 - accuracy: 0.7396 - val_loss: 0.5292 - val_accuracy: 0.7917\n",
            "Epoch 58/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.5374 - accuracy: 0.8101\n",
            "Epoch 00058: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.8021 - val_loss: 2.0708 - val_accuracy: 0.6667\n",
            "Epoch 59/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.1895 - accuracy: 0.7738\n",
            "Epoch 00059: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 1.1560 - accuracy: 0.7639 - val_loss: 1.6764 - val_accuracy: 0.6389\n",
            "Epoch 60/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.6720 - accuracy: 0.7453\n",
            "Epoch 00060: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.6371 - accuracy: 0.7604 - val_loss: 2.1258 - val_accuracy: 0.5694\n",
            "Epoch 61/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 1.0304 - accuracy: 0.7222\n",
            "Epoch 00061: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.9703 - accuracy: 0.7396 - val_loss: 1.3025 - val_accuracy: 0.7083\n",
            "Epoch 62/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.8521 - accuracy: 0.7366\n",
            "Epoch 00062: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7907 - accuracy: 0.7535 - val_loss: 0.8625 - val_accuracy: 0.7500\n",
            "Epoch 63/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.7062 - accuracy: 0.7462\n",
            "Epoch 00063: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.7029 - accuracy: 0.7431 - val_loss: 1.5996 - val_accuracy: 0.7361\n",
            "Epoch 64/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.8821 - accuracy: 0.7333\n",
            "Epoch 00064: val_accuracy did not improve from 0.88889\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.8523 - accuracy: 0.7396 - val_loss: 1.3005 - val_accuracy: 0.6389\n",
            "Epoch 65/500\n",
            "89/96 [==========================>...] - ETA: 0s - loss: 0.4647 - accuracy: 0.8464\n",
            "Epoch 00065: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.8472 - val_loss: 0.3588 - val_accuracy: 0.8611\n",
            "Epoch 66/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3038 - accuracy: 0.8864\n",
            "Epoch 00066: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8750 - val_loss: 0.4020 - val_accuracy: 0.8611\n",
            "Epoch 67/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.3521 - accuracy: 0.8532\n",
            "Epoch 00067: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8715 - val_loss: 0.3048 - val_accuracy: 0.8750\n",
            "Epoch 68/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3417 - accuracy: 0.8582\n",
            "Epoch 00068: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8611 - val_loss: 1.5880 - val_accuracy: 0.6667\n",
            "Epoch 69/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.3444 - accuracy: 0.8807\n",
            "Epoch 00069: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3182 - accuracy: 0.8958 - val_loss: 0.3774 - val_accuracy: 0.8750\n",
            "Epoch 70/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2671 - accuracy: 0.8826\n",
            "Epoch 00070: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2751 - accuracy: 0.8750 - val_loss: 0.4974 - val_accuracy: 0.8333\n",
            "Epoch 71/500\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.3097 - accuracy: 0.8903\n",
            "Epoch 00071: val_accuracy did not improve from 0.88889\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.9028 - val_loss: 0.6982 - val_accuracy: 0.8333\n",
            "Epoch 72/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.3737 - accuracy: 0.8784\n",
            "Epoch 00072: val_accuracy improved from 0.88889 to 0.93056, saving model to model_best.hdf5\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8785 - val_loss: 0.2783 - val_accuracy: 0.9306\n",
            "Epoch 73/500\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.2751 - accuracy: 0.8658\n",
            "Epoch 00073: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.8819 - val_loss: 0.6890 - val_accuracy: 0.8194\n",
            "Epoch 74/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.2702 - accuracy: 0.9004\n",
            "Epoch 00074: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8889 - val_loss: 0.4780 - val_accuracy: 0.8611\n",
            "Epoch 75/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.3654 - accuracy: 0.8902\n",
            "Epoch 00075: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3859 - accuracy: 0.8785 - val_loss: 0.4436 - val_accuracy: 0.8472\n",
            "Epoch 76/500\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.8993\n",
            "Epoch 00076: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2791 - accuracy: 0.8993 - val_loss: 0.3245 - val_accuracy: 0.8889\n",
            "Epoch 77/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.2650 - accuracy: 0.8876\n",
            "Epoch 00077: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2954 - accuracy: 0.8785 - val_loss: 0.4906 - val_accuracy: 0.8750\n",
            "Epoch 78/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3006 - accuracy: 0.8889\n",
            "Epoch 00078: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8889 - val_loss: 0.4241 - val_accuracy: 0.8472\n",
            "Epoch 79/500\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.9176\n",
            "Epoch 00079: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.9132 - val_loss: 0.3296 - val_accuracy: 0.8889\n",
            "Epoch 80/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.3261 - accuracy: 0.8605\n",
            "Epoch 00080: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8611 - val_loss: 0.6519 - val_accuracy: 0.8472\n",
            "Epoch 81/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2633 - accuracy: 0.9015\n",
            "Epoch 00081: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.9062 - val_loss: 0.2729 - val_accuracy: 0.9167\n",
            "Epoch 82/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.3301 - accuracy: 0.8755\n",
            "Epoch 00082: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2947 - accuracy: 0.8924 - val_loss: 0.3653 - val_accuracy: 0.8750\n",
            "Epoch 83/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.2339 - accuracy: 0.8956\n",
            "Epoch 00083: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8785 - val_loss: 0.6391 - val_accuracy: 0.8333\n",
            "Epoch 84/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.3001 - accuracy: 0.8765\n",
            "Epoch 00084: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8750 - val_loss: 0.4578 - val_accuracy: 0.8611\n",
            "Epoch 85/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.2645 - accuracy: 0.8810\n",
            "Epoch 00085: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8854 - val_loss: 0.3847 - val_accuracy: 0.8889\n",
            "Epoch 86/500\n",
            "91/96 [===========================>..] - ETA: 0s - loss: 0.3006 - accuracy: 0.8828\n",
            "Epoch 00086: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8854 - val_loss: 0.5533 - val_accuracy: 0.8611\n",
            "Epoch 87/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.3403 - accuracy: 0.8704\n",
            "Epoch 00087: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8750 - val_loss: 0.8365 - val_accuracy: 0.8472\n",
            "Epoch 88/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.2240 - accuracy: 0.9106\n",
            "Epoch 00088: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.2501 - accuracy: 0.9028 - val_loss: 0.3873 - val_accuracy: 0.8889\n",
            "Epoch 89/500\n",
            "93/96 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9140\n",
            "Epoch 00089: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.9132 - val_loss: 0.3078 - val_accuracy: 0.9028\n",
            "Epoch 90/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.2861 - accuracy: 0.8810\n",
            "Epoch 00090: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2678 - accuracy: 0.8889 - val_loss: 0.6236 - val_accuracy: 0.8611\n",
            "Epoch 91/500\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.4131 - accuracy: 0.8462\n",
            "Epoch 00091: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8750 - val_loss: 0.7805 - val_accuracy: 0.8333\n",
            "Epoch 92/500\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.3572 - accuracy: 0.9114\n",
            "Epoch 00092: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.9028 - val_loss: 0.4105 - val_accuracy: 0.8472\n",
            "Epoch 93/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2675 - accuracy: 0.9129\n",
            "Epoch 00093: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.2618 - accuracy: 0.9132 - val_loss: 2.9712 - val_accuracy: 0.5139\n",
            "Epoch 94/500\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8715\n",
            "Epoch 00094: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.3162 - accuracy: 0.8715 - val_loss: 0.5851 - val_accuracy: 0.8472\n",
            "Epoch 95/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.2668 - accuracy: 0.9109\n",
            "Epoch 00095: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2732 - accuracy: 0.9062 - val_loss: 0.8922 - val_accuracy: 0.8056\n",
            "Epoch 96/500\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.3120 - accuracy: 0.8917\n",
            "Epoch 00096: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2714 - accuracy: 0.9097 - val_loss: 0.4644 - val_accuracy: 0.8472\n",
            "Epoch 97/500\n",
            "87/96 [==========================>...] - ETA: 0s - loss: 0.3335 - accuracy: 0.8659\n",
            "Epoch 00097: val_accuracy did not improve from 0.93056\n",
            "\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8785 - val_loss: 0.3761 - val_accuracy: 0.8750\n",
            "Epoch 98/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.2051 - accuracy: 0.9280\n",
            "Epoch 00098: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1964 - accuracy: 0.9306 - val_loss: 0.3201 - val_accuracy: 0.8750\n",
            "Epoch 99/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1671 - accuracy: 0.9470\n",
            "Epoch 00099: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1738 - accuracy: 0.9410 - val_loss: 0.4324 - val_accuracy: 0.8889\n",
            "Epoch 100/500\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.9306\n",
            "Epoch 00100: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1727 - accuracy: 0.9306 - val_loss: 0.5400 - val_accuracy: 0.8750\n",
            "Epoch 101/500\n",
            "88/96 [==========================>...] - ETA: 0s - loss: 0.1577 - accuracy: 0.9280\n",
            "Epoch 00101: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.9340 - val_loss: 0.2912 - val_accuracy: 0.9028\n",
            "Epoch 102/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1919 - accuracy: 0.9197\n",
            "Epoch 00102: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9132 - val_loss: 0.3702 - val_accuracy: 0.8889\n",
            "Epoch 103/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1916 - accuracy: 0.9317\n",
            "Epoch 00103: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9410 - val_loss: 0.4540 - val_accuracy: 0.8889\n",
            "Epoch 104/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1584 - accuracy: 0.9357\n",
            "Epoch 00104: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9340 - val_loss: 0.4560 - val_accuracy: 0.9028\n",
            "Epoch 105/500\n",
            "77/96 [=======================>......] - ETA: 0s - loss: 0.1846 - accuracy: 0.9264\n",
            "Epoch 00105: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9271 - val_loss: 0.4042 - val_accuracy: 0.8472\n",
            "Epoch 106/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.1396 - accuracy: 0.9612\n",
            "Epoch 00106: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9444 - val_loss: 0.2944 - val_accuracy: 0.9167\n",
            "Epoch 107/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.1970 - accuracy: 0.9177\n",
            "Epoch 00107: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9236 - val_loss: 0.2743 - val_accuracy: 0.9167\n",
            "Epoch 108/500\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.1844 - accuracy: 0.9208\n",
            "Epoch 00108: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1783 - accuracy: 0.9167 - val_loss: 0.4636 - val_accuracy: 0.9028\n",
            "Epoch 109/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.1723 - accuracy: 0.9228\n",
            "Epoch 00109: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9271 - val_loss: 0.4435 - val_accuracy: 0.9167\n",
            "Epoch 110/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.1351 - accuracy: 0.9472\n",
            "Epoch 00110: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 0.9410 - val_loss: 0.5876 - val_accuracy: 0.8333\n",
            "Epoch 111/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1709 - accuracy: 0.9444\n",
            "Epoch 00111: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1616 - accuracy: 0.9444 - val_loss: 0.3525 - val_accuracy: 0.9167\n",
            "Epoch 112/500\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.1318 - accuracy: 0.9625\n",
            "Epoch 00112: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9514 - val_loss: 0.7756 - val_accuracy: 0.8750\n",
            "Epoch 113/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.2863 - accuracy: 0.9365\n",
            "Epoch 00113: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.9306 - val_loss: 0.3342 - val_accuracy: 0.9167\n",
            "Epoch 114/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1616 - accuracy: 0.9317\n",
            "Epoch 00114: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9271 - val_loss: 0.4515 - val_accuracy: 0.9028\n",
            "Epoch 115/500\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.1855 - accuracy: 0.9333\n",
            "Epoch 00115: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9340 - val_loss: 0.3349 - val_accuracy: 0.9167\n",
            "Epoch 116/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1597 - accuracy: 0.9518\n",
            "Epoch 00116: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9479 - val_loss: 0.5479 - val_accuracy: 0.8472\n",
            "Epoch 117/500\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9326\n",
            "Epoch 00117: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1681 - accuracy: 0.9340 - val_loss: 0.3579 - val_accuracy: 0.8750\n",
            "Epoch 118/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.1775 - accuracy: 0.9300\n",
            "Epoch 00118: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1627 - accuracy: 0.9375 - val_loss: 0.5353 - val_accuracy: 0.8611\n",
            "Epoch 119/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.1755 - accuracy: 0.9268\n",
            "Epoch 00119: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9236 - val_loss: 0.3261 - val_accuracy: 0.8889\n",
            "Epoch 120/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1878 - accuracy: 0.9237\n",
            "Epoch 00120: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1740 - accuracy: 0.9271 - val_loss: 0.4491 - val_accuracy: 0.8472\n",
            "Epoch 121/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1364 - accuracy: 0.9438\n",
            "Epoch 00121: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9444 - val_loss: 0.7022 - val_accuracy: 0.8750\n",
            "Epoch 122/500\n",
            "85/96 [=========================>....] - ETA: 0s - loss: 0.1836 - accuracy: 0.9294\n",
            "Epoch 00122: val_accuracy did not improve from 0.93056\n",
            "\n",
            "Epoch 00122: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9306 - val_loss: 0.3577 - val_accuracy: 0.8750\n",
            "Epoch 123/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.1217 - accuracy: 0.9588\n",
            "Epoch 00123: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1192 - accuracy: 0.9549 - val_loss: 0.3518 - val_accuracy: 0.9167\n",
            "Epoch 124/500\n",
            "80/96 [========================>.....] - ETA: 0s - loss: 0.1491 - accuracy: 0.9375\n",
            "Epoch 00124: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9444 - val_loss: 0.3749 - val_accuracy: 0.8750\n",
            "Epoch 125/500\n",
            "90/96 [===========================>..] - ETA: 0s - loss: 0.1117 - accuracy: 0.9593\n",
            "Epoch 00125: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.9583 - val_loss: 0.3564 - val_accuracy: 0.9028\n",
            "Epoch 126/500\n",
            "75/96 [======================>.......] - ETA: 0s - loss: 0.1244 - accuracy: 0.9422\n",
            "Epoch 00126: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9444 - val_loss: 0.4362 - val_accuracy: 0.8750\n",
            "Epoch 127/500\n",
            "86/96 [=========================>....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9574\n",
            "Epoch 00127: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1297 - accuracy: 0.9549 - val_loss: 0.4592 - val_accuracy: 0.8750\n",
            "Epoch 128/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.1352 - accuracy: 0.9383\n",
            "Epoch 00128: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9479 - val_loss: 0.3694 - val_accuracy: 0.8889\n",
            "Epoch 129/500\n",
            "79/96 [=======================>......] - ETA: 0s - loss: 0.1432 - accuracy: 0.9494\n",
            "Epoch 00129: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1316 - accuracy: 0.9514 - val_loss: 0.4239 - val_accuracy: 0.8611\n",
            "Epoch 130/500\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.1362 - accuracy: 0.9530\n",
            "Epoch 00130: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1408 - accuracy: 0.9479 - val_loss: 0.4732 - val_accuracy: 0.8611\n",
            "Epoch 131/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9565\n",
            "Epoch 00131: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9549 - val_loss: 0.3825 - val_accuracy: 0.8750\n",
            "Epoch 132/500\n",
            "95/96 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9439\n",
            "Epoch 00132: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1330 - accuracy: 0.9444 - val_loss: 0.3368 - val_accuracy: 0.8750\n",
            "Epoch 133/500\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9504\n",
            "Epoch 00133: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.9479 - val_loss: 0.4542 - val_accuracy: 0.8750\n",
            "Epoch 134/500\n",
            "92/96 [===========================>..] - ETA: 0s - loss: 0.1161 - accuracy: 0.9565\n",
            "Epoch 00134: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1181 - accuracy: 0.9549 - val_loss: 0.4267 - val_accuracy: 0.8472\n",
            "Epoch 135/500\n",
            "94/96 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9574\n",
            "Epoch 00135: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9549 - val_loss: 0.4265 - val_accuracy: 0.8750\n",
            "Epoch 136/500\n",
            "96/96 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9410\n",
            "Epoch 00136: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 4ms/step - loss: 0.1440 - accuracy: 0.9410 - val_loss: 0.4267 - val_accuracy: 0.8611\n",
            "Epoch 137/500\n",
            "81/96 [========================>.....] - ETA: 0s - loss: 0.1326 - accuracy: 0.9383\n",
            "Epoch 00137: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1276 - accuracy: 0.9444 - val_loss: 0.5359 - val_accuracy: 0.9167\n",
            "Epoch 138/500\n",
            "83/96 [========================>.....] - ETA: 0s - loss: 0.1331 - accuracy: 0.9478\n",
            "Epoch 00138: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.9514 - val_loss: 0.4241 - val_accuracy: 0.8611\n",
            "Epoch 139/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.1185 - accuracy: 0.9593\n",
            "Epoch 00139: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1160 - accuracy: 0.9583 - val_loss: 0.3821 - val_accuracy: 0.8611\n",
            "Epoch 140/500\n",
            "78/96 [=======================>......] - ETA: 0s - loss: 0.1220 - accuracy: 0.9487\n",
            "Epoch 00140: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9479 - val_loss: 0.4340 - val_accuracy: 0.8889\n",
            "Epoch 141/500\n",
            "82/96 [========================>.....] - ETA: 0s - loss: 0.1262 - accuracy: 0.9553\n",
            "Epoch 00141: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9479 - val_loss: 0.5185 - val_accuracy: 0.8611\n",
            "Epoch 142/500\n",
            "84/96 [=========================>....] - ETA: 0s - loss: 0.1158 - accuracy: 0.9484\n",
            "Epoch 00142: val_accuracy did not improve from 0.93056\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9479 - val_loss: 0.4376 - val_accuracy: 0.8750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQZz5YXCuY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c91d3799-21f7-4671-e713-e95945f01568"
      },
      "source": [
        "#plot learning curve\n",
        "plt.title('Train Accuracy vs Val Accuracy')\n",
        "plt.plot(hist.history['accuracy'], label='Train Accuracy', color='black')\n",
        "plt.plot(hist.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXxc0ZzL5AT"
      },
      "source": [
        "x = vid[['size','bitrate','size_diff', 'bitrate_diff']]\n",
        "y = vid.Cls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H94RyPxDIpU2"
      },
      "source": [
        "#predict\n",
        "model.load_weights('model_best.hdf5')\n",
        "model.compile(optimizer='RMSprop',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "pr = model.predict(x, batch_size=3, verbose=0, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRFLZh5aI4sf"
      },
      "source": [
        "pp = pr.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy0J9LcDI5za"
      },
      "source": [
        "yp = []\n",
        "for i in pp:\n",
        "  yp.append(1100+i*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lazGPcapJ2ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2b7644-f802-4efc-e8e3-0075439096ff"
      },
      "source": [
        "print(len(yp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz8Cwy8FJ4UT"
      },
      "source": [
        "yt = []\n",
        "for i in y:\n",
        "  yt.append(1100+i*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veCuTd5-KVU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f90c87e-2369-47d3-90be-5a977742eb4f"
      },
      "source": [
        "print(yt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBEudwyFtDgd"
      },
      "source": [
        "#load prediction data\r\n",
        "issue = pd.read_csv(\"/content/received.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yBxFtasyltA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "2549fbe0-3f5c-425a-f089-483593d33506"
      },
      "source": [
        "issue.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>FileSize</th>\n",
              "      <th>BitRate</th>\n",
              "      <th>size_diff</th>\n",
              "      <th>B_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.mp4</td>\n",
              "      <td>33.9</td>\n",
              "      <td>1388</td>\n",
              "      <td>7.3</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.mp4</td>\n",
              "      <td>35.2</td>\n",
              "      <td>1441</td>\n",
              "      <td>6.0</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.mp4</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1473</td>\n",
              "      <td>5.2</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.mp4</td>\n",
              "      <td>35.9</td>\n",
              "      <td>1469</td>\n",
              "      <td>5.3</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.mp4</td>\n",
              "      <td>36.0</td>\n",
              "      <td>1473</td>\n",
              "      <td>5.2</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    name  FileSize  BitRate  size_diff  B_diff\n",
              "0  0.mp4      33.9     1388        7.3     168\n",
              "1  1.mp4      35.2     1441        6.0     115\n",
              "2  2.mp4      36.0     1473        5.2      83\n",
              "3  3.mp4      35.9     1469        5.3      87\n",
              "4  4.mp4      36.0     1473        5.2      83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoeRnqb6ypLO"
      },
      "source": [
        "X_issue = issue[['FileSize','BitRate','size_diff', 'B_diff']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYPBNQtstAxd"
      },
      "source": [
        "pr = model.predict(X_issue, batch_size=3, verbose=0, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-ek1n-yztr"
      },
      "source": [
        "pp = pr.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypJx-GeTy0nE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75292a05-02bb-47ab-a6ba-c7f9e531afbc"
      },
      "source": [
        "pp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 2, 2, 0, 1, 4, 5, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSwJfQk5y4vF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fe394cf-3971-4d04-875d-30c473c0a2c4"
      },
      "source": [
        "yp = []\n",
        "for i in pp:\n",
        "  yp.append(1100+i*100)\n",
        "yp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1200, 1300, 1300, 1300, 1300, 1100, 1200, 1500, 1600, 1300]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}