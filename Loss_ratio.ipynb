{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loss ratio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFINkj8AGPIy",
        "outputId": "752c4568-a9a1-4177-c058-16948f7af5ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td4tbXS5Geqm",
        "outputId": "082281ec-413e-436f-d3ca-e8ad81113eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahrd1i0NGkf1",
        "outputId": "b709e51c-c528-4d9e-de76-c448e8d45e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "#load the dataset\n",
        "vid = pd.read_csv(\"/content/SframePSNR52.csv\")\n",
        "#show the data\n",
        "vid.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lossclass</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Framecount</th>\n",
              "      <th>42</th>\n",
              "      <th>41</th>\n",
              "      <th>40</th>\n",
              "      <th>39</th>\n",
              "      <th>42/F</th>\n",
              "      <th>41/F</th>\n",
              "      <th>40/F</th>\n",
              "      <th>39/F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1100</td>\n",
              "      <td>6622</td>\n",
              "      <td>6074</td>\n",
              "      <td>6045</td>\n",
              "      <td>5818</td>\n",
              "      <td>5709</td>\n",
              "      <td>0.917246</td>\n",
              "      <td>0.912866</td>\n",
              "      <td>0.878587</td>\n",
              "      <td>0.862126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1100</td>\n",
              "      <td>6622</td>\n",
              "      <td>6048</td>\n",
              "      <td>5861</td>\n",
              "      <td>5793</td>\n",
              "      <td>5681</td>\n",
              "      <td>0.913319</td>\n",
              "      <td>0.885080</td>\n",
              "      <td>0.874811</td>\n",
              "      <td>0.857898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1100</td>\n",
              "      <td>6622</td>\n",
              "      <td>6159</td>\n",
              "      <td>6128</td>\n",
              "      <td>5912</td>\n",
              "      <td>5815</td>\n",
              "      <td>0.930082</td>\n",
              "      <td>0.925400</td>\n",
              "      <td>0.892782</td>\n",
              "      <td>0.878133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1100</td>\n",
              "      <td>6622</td>\n",
              "      <td>5883</td>\n",
              "      <td>5837</td>\n",
              "      <td>5708</td>\n",
              "      <td>5626</td>\n",
              "      <td>0.888402</td>\n",
              "      <td>0.881456</td>\n",
              "      <td>0.861975</td>\n",
              "      <td>0.849592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1100</td>\n",
              "      <td>6622</td>\n",
              "      <td>6092</td>\n",
              "      <td>5903</td>\n",
              "      <td>5855</td>\n",
              "      <td>5744</td>\n",
              "      <td>0.919964</td>\n",
              "      <td>0.891423</td>\n",
              "      <td>0.884174</td>\n",
              "      <td>0.867412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   lossclass  Condition  Framecount  ...      41/F      40/F      39/F\n",
              "0          0       1100        6622  ...  0.912866  0.878587  0.862126\n",
              "1          1       1100        6622  ...  0.885080  0.874811  0.857898\n",
              "2          2       1100        6622  ...  0.925400  0.892782  0.878133\n",
              "3          3       1100        6622  ...  0.881456  0.861975  0.849592\n",
              "4          4       1100        6622  ...  0.891423  0.884174  0.867412\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAtXAQdLPlNQ"
      },
      "source": [
        "train = vid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXUooT0CKnN1"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train[['42/F','41/F','40/F','39/F','Condition']] = scaler.fit_transform(train[['42/F','41/F','40/F','39/F','Condition']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VI9Jn5eVMAG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = train[['42/F','41/F','40/F','39/F','Condition']]\n",
        "y = train.lossclass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyuwialJIVPZ",
        "outputId": "946546f3-799f-41b0-b72e-45b4817c9636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split as split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.15)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(255, 5)\n",
            "(255,)\n",
            "(45, 5)\n",
            "(45,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9GbAVtTHGcg",
        "outputId": "dce44b2d-a77b-406d-9cb8-f02f3eb05fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "\n",
        "optimizer = 'RMSProp'\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(128,input_dim=5,activation='relu',name=\"d1\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128,activation='relu',name=\"d2\"))\n",
        "model.add(Dropout(0.3))\n",
        "'''model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.5))'''\n",
        "'''model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))'''\n",
        "model.add(Dense(6,activation='softmax',name=\"o\"))\n",
        "\n",
        "model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "d1 (Dense)                   (None, 128)               768       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "d2 (Dense)                   (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "o (Dense)                    (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 18,054\n",
            "Trainable params: 18,054\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkmva1dZLQ1g",
        "outputId": "7b872b5f-15ac-49a7-8832-40185416089b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = [\n",
        "    ModelCheckpoint('model_best.hdf5',\n",
        "        monitor='val_accuracy', \n",
        "        save_best_only=True, \n",
        "        mode='max',\n",
        "        verbose=1)\n",
        "]\n",
        "hist=model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[checkpoint],epochs=100, batch_size=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "59/85 [===================>..........] - ETA: 0s - loss: 1.8187 - accuracy: 0.1582\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.08889, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.8136 - accuracy: 0.1843 - val_loss: 1.8168 - val_accuracy: 0.0889\n",
            "Epoch 2/100\n",
            "59/85 [===================>..........] - ETA: 0s - loss: 1.7629 - accuracy: 0.2147\n",
            "Epoch 00002: val_accuracy improved from 0.08889 to 0.15556, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.7876 - accuracy: 0.2078 - val_loss: 1.8071 - val_accuracy: 0.1556\n",
            "Epoch 3/100\n",
            "78/85 [==========================>...] - ETA: 0s - loss: 1.7693 - accuracy: 0.2051\n",
            "Epoch 00003: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.7720 - accuracy: 0.2000 - val_loss: 1.8318 - val_accuracy: 0.1556\n",
            "Epoch 4/100\n",
            "75/85 [=========================>....] - ETA: 0s - loss: 1.7463 - accuracy: 0.2178\n",
            "Epoch 00004: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7440 - accuracy: 0.2275 - val_loss: 1.8009 - val_accuracy: 0.0889\n",
            "Epoch 5/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.7558 - accuracy: 0.2115\n",
            "Epoch 00005: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.7443 - accuracy: 0.2157 - val_loss: 1.7879 - val_accuracy: 0.0889\n",
            "Epoch 6/100\n",
            "74/85 [=========================>....] - ETA: 0s - loss: 1.7539 - accuracy: 0.2027\n",
            "Epoch 00006: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7475 - accuracy: 0.2078 - val_loss: 1.8088 - val_accuracy: 0.0889\n",
            "Epoch 7/100\n",
            "63/85 [=====================>........] - ETA: 0s - loss: 1.7393 - accuracy: 0.1746\n",
            "Epoch 00007: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.7417 - accuracy: 0.1922 - val_loss: 1.7826 - val_accuracy: 0.0889\n",
            "Epoch 8/100\n",
            "58/85 [===================>..........] - ETA: 0s - loss: 1.7274 - accuracy: 0.2069\n",
            "Epoch 00008: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.7095 - accuracy: 0.2353 - val_loss: 1.7818 - val_accuracy: 0.0889\n",
            "Epoch 9/100\n",
            "56/85 [==================>...........] - ETA: 0s - loss: 1.7008 - accuracy: 0.2262\n",
            "Epoch 00009: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.7130 - accuracy: 0.2314 - val_loss: 1.8392 - val_accuracy: 0.1333\n",
            "Epoch 10/100\n",
            "79/85 [==========================>...] - ETA: 0s - loss: 1.7046 - accuracy: 0.2025\n",
            "Epoch 00010: val_accuracy did not improve from 0.15556\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6986 - accuracy: 0.2078 - val_loss: 1.7856 - val_accuracy: 0.1556\n",
            "Epoch 11/100\n",
            "71/85 [========================>.....] - ETA: 0s - loss: 1.7386 - accuracy: 0.2019\n",
            "Epoch 00011: val_accuracy improved from 0.15556 to 0.17778, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7182 - accuracy: 0.2118 - val_loss: 1.7933 - val_accuracy: 0.1778\n",
            "Epoch 12/100\n",
            "84/85 [============================>.] - ETA: 0s - loss: 1.7025 - accuracy: 0.1825\n",
            "Epoch 00012: val_accuracy did not improve from 0.17778\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6996 - accuracy: 0.1843 - val_loss: 1.7490 - val_accuracy: 0.1111\n",
            "Epoch 13/100\n",
            "64/85 [=====================>........] - ETA: 0s - loss: 1.6801 - accuracy: 0.2552\n",
            "Epoch 00013: val_accuracy did not improve from 0.17778\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7008 - accuracy: 0.2235 - val_loss: 1.7548 - val_accuracy: 0.1333\n",
            "Epoch 14/100\n",
            "72/85 [========================>.....] - ETA: 0s - loss: 1.6767 - accuracy: 0.2778\n",
            "Epoch 00014: val_accuracy did not improve from 0.17778\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6757 - accuracy: 0.2706 - val_loss: 1.7343 - val_accuracy: 0.1556\n",
            "Epoch 15/100\n",
            "69/85 [=======================>......] - ETA: 0s - loss: 1.6647 - accuracy: 0.2512\n",
            "Epoch 00015: val_accuracy did not improve from 0.17778\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6844 - accuracy: 0.2392 - val_loss: 1.7198 - val_accuracy: 0.1333\n",
            "Epoch 16/100\n",
            "76/85 [=========================>....] - ETA: 0s - loss: 1.6790 - accuracy: 0.2544\n",
            "Epoch 00016: val_accuracy did not improve from 0.17778\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6791 - accuracy: 0.2471 - val_loss: 1.6906 - val_accuracy: 0.1111\n",
            "Epoch 17/100\n",
            "68/85 [=======================>......] - ETA: 0s - loss: 1.6830 - accuracy: 0.2696\n",
            "Epoch 00017: val_accuracy improved from 0.17778 to 0.20000, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6859 - accuracy: 0.2549 - val_loss: 1.7292 - val_accuracy: 0.2000\n",
            "Epoch 18/100\n",
            "73/85 [========================>.....] - ETA: 0s - loss: 1.6571 - accuracy: 0.2648\n",
            "Epoch 00018: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6580 - accuracy: 0.2510 - val_loss: 1.7057 - val_accuracy: 0.1333\n",
            "Epoch 19/100\n",
            "79/85 [==========================>...] - ETA: 0s - loss: 1.6318 - accuracy: 0.2321\n",
            "Epoch 00019: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6341 - accuracy: 0.2314 - val_loss: 1.6896 - val_accuracy: 0.1333\n",
            "Epoch 20/100\n",
            "80/85 [===========================>..] - ETA: 0s - loss: 1.6550 - accuracy: 0.2625\n",
            "Epoch 00020: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.6421 - accuracy: 0.2549 - val_loss: 1.7476 - val_accuracy: 0.1556\n",
            "Epoch 21/100\n",
            "59/85 [===================>..........] - ETA: 0s - loss: 1.6547 - accuracy: 0.2090\n",
            "Epoch 00021: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.6359 - accuracy: 0.2196 - val_loss: 1.6778 - val_accuracy: 0.1111\n",
            "Epoch 22/100\n",
            "70/85 [=======================>......] - ETA: 0s - loss: 1.6197 - accuracy: 0.2667\n",
            "Epoch 00022: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6253 - accuracy: 0.2588 - val_loss: 1.7521 - val_accuracy: 0.1111\n",
            "Epoch 23/100\n",
            "72/85 [========================>.....] - ETA: 0s - loss: 1.6297 - accuracy: 0.2222\n",
            "Epoch 00023: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6247 - accuracy: 0.2235 - val_loss: 1.6756 - val_accuracy: 0.1556\n",
            "Epoch 24/100\n",
            "72/85 [========================>.....] - ETA: 0s - loss: 1.6067 - accuracy: 0.3148\n",
            "Epoch 00024: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6252 - accuracy: 0.3137 - val_loss: 1.6361 - val_accuracy: 0.1333\n",
            "Epoch 25/100\n",
            "82/85 [===========================>..] - ETA: 0s - loss: 1.5872 - accuracy: 0.2886\n",
            "Epoch 00025: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.5975 - accuracy: 0.2863 - val_loss: 1.6220 - val_accuracy: 0.1111\n",
            "Epoch 26/100\n",
            "70/85 [=======================>......] - ETA: 0s - loss: 1.6352 - accuracy: 0.2857\n",
            "Epoch 00026: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6493 - accuracy: 0.2706 - val_loss: 1.6390 - val_accuracy: 0.1333\n",
            "Epoch 27/100\n",
            "70/85 [=======================>......] - ETA: 0s - loss: 1.6044 - accuracy: 0.2571\n",
            "Epoch 00027: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.5848 - accuracy: 0.2667 - val_loss: 1.5937 - val_accuracy: 0.1111\n",
            "Epoch 28/100\n",
            "44/85 [==============>...............] - ETA: 0s - loss: 1.5808 - accuracy: 0.2121\n",
            "Epoch 00028: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.6151 - accuracy: 0.2275 - val_loss: 1.6676 - val_accuracy: 0.1556\n",
            "Epoch 29/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5974 - accuracy: 0.3133\n",
            "Epoch 00029: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.6009 - accuracy: 0.2980 - val_loss: 1.6168 - val_accuracy: 0.1556\n",
            "Epoch 30/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.6110 - accuracy: 0.2756\n",
            "Epoch 00030: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5880 - accuracy: 0.2588 - val_loss: 1.6842 - val_accuracy: 0.1556\n",
            "Epoch 31/100\n",
            "48/85 [===============>..............] - ETA: 0s - loss: 1.5956 - accuracy: 0.2361\n",
            "Epoch 00031: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5905 - accuracy: 0.2392 - val_loss: 1.6008 - val_accuracy: 0.1111\n",
            "Epoch 32/100\n",
            "55/85 [==================>...........] - ETA: 0s - loss: 1.5612 - accuracy: 0.2970\n",
            "Epoch 00032: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5744 - accuracy: 0.2980 - val_loss: 1.5889 - val_accuracy: 0.1111\n",
            "Epoch 33/100\n",
            "46/85 [===============>..............] - ETA: 0s - loss: 1.5755 - accuracy: 0.3333\n",
            "Epoch 00033: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5629 - accuracy: 0.2902 - val_loss: 1.5956 - val_accuracy: 0.1778\n",
            "Epoch 34/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5376 - accuracy: 0.2867\n",
            "Epoch 00034: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5920 - accuracy: 0.2784 - val_loss: 1.5934 - val_accuracy: 0.1111\n",
            "Epoch 35/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5568 - accuracy: 0.3533\n",
            "Epoch 00035: val_accuracy did not improve from 0.20000\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5475 - accuracy: 0.3294 - val_loss: 1.6892 - val_accuracy: 0.1333\n",
            "Epoch 36/100\n",
            "55/85 [==================>...........] - ETA: 0s - loss: 1.6058 - accuracy: 0.2606\n",
            "Epoch 00036: val_accuracy improved from 0.20000 to 0.22222, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5820 - accuracy: 0.2510 - val_loss: 1.6275 - val_accuracy: 0.2222\n",
            "Epoch 37/100\n",
            "49/85 [================>.............] - ETA: 0s - loss: 1.5682 - accuracy: 0.2993    \n",
            "Epoch 00037: val_accuracy did not improve from 0.22222\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5866 - accuracy: 0.2510 - val_loss: 1.6203 - val_accuracy: 0.1556\n",
            "Epoch 38/100\n",
            "46/85 [===============>..............] - ETA: 0s - loss: 1.5966 - accuracy: 0.2681\n",
            "Epoch 00038: val_accuracy improved from 0.22222 to 0.24444, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.5707 - accuracy: 0.2902 - val_loss: 1.5909 - val_accuracy: 0.2444\n",
            "Epoch 39/100\n",
            "85/85 [==============================] - ETA: 0s - loss: 1.5535 - accuracy: 0.2941\n",
            "Epoch 00039: val_accuracy did not improve from 0.24444\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.5535 - accuracy: 0.2941 - val_loss: 1.6054 - val_accuracy: 0.1778\n",
            "Epoch 40/100\n",
            "54/85 [==================>...........] - ETA: 0s - loss: 1.5291 - accuracy: 0.3210    \n",
            "Epoch 00040: val_accuracy did not improve from 0.24444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5488 - accuracy: 0.3137 - val_loss: 1.5808 - val_accuracy: 0.1778\n",
            "Epoch 41/100\n",
            "43/85 [==============>...............] - ETA: 0s - loss: 1.5290 - accuracy: 0.2868\n",
            "Epoch 00041: val_accuracy improved from 0.24444 to 0.35556, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.5331 - accuracy: 0.2941 - val_loss: 1.5730 - val_accuracy: 0.3556\n",
            "Epoch 42/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5145 - accuracy: 0.2418    \n",
            "Epoch 00042: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5014 - accuracy: 0.2627 - val_loss: 1.5809 - val_accuracy: 0.3111\n",
            "Epoch 43/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5853 - accuracy: 0.2200    \n",
            "Epoch 00043: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5559 - accuracy: 0.2431 - val_loss: 1.5839 - val_accuracy: 0.2444\n",
            "Epoch 44/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.4871 - accuracy: 0.3141\n",
            "Epoch 00044: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5289 - accuracy: 0.2941 - val_loss: 1.7020 - val_accuracy: 0.2000\n",
            "Epoch 45/100\n",
            "44/85 [==============>...............] - ETA: 0s - loss: 1.5161 - accuracy: 0.3182\n",
            "Epoch 00045: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5383 - accuracy: 0.3020 - val_loss: 1.5806 - val_accuracy: 0.2222\n",
            "Epoch 46/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.5541 - accuracy: 0.2500    \n",
            "Epoch 00046: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5434 - accuracy: 0.2196 - val_loss: 1.5554 - val_accuracy: 0.3556\n",
            "Epoch 47/100\n",
            "45/85 [==============>...............] - ETA: 0s - loss: 1.5769 - accuracy: 0.2667\n",
            "Epoch 00047: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5564 - accuracy: 0.2588 - val_loss: 1.5933 - val_accuracy: 0.2222\n",
            "Epoch 48/100\n",
            "46/85 [===============>..............] - ETA: 0s - loss: 1.5323 - accuracy: 0.2681    \n",
            "Epoch 00048: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5507 - accuracy: 0.2745 - val_loss: 1.5708 - val_accuracy: 0.2889\n",
            "Epoch 49/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5730 - accuracy: 0.2810\n",
            "Epoch 00049: val_accuracy did not improve from 0.35556\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5440 - accuracy: 0.3098 - val_loss: 1.6754 - val_accuracy: 0.2667\n",
            "Epoch 50/100\n",
            "57/85 [===================>..........] - ETA: 0s - loss: 1.5118 - accuracy: 0.2982    \n",
            "Epoch 00050: val_accuracy improved from 0.35556 to 0.37778, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5293 - accuracy: 0.2824 - val_loss: 1.5756 - val_accuracy: 0.3778\n",
            "Epoch 51/100\n",
            "47/85 [===============>..............] - ETA: 0s - loss: 1.5736 - accuracy: 0.3404\n",
            "Epoch 00051: val_accuracy did not improve from 0.37778\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5238 - accuracy: 0.3333 - val_loss: 1.5821 - val_accuracy: 0.2889\n",
            "Epoch 52/100\n",
            "49/85 [================>.............] - ETA: 0s - loss: 1.5053 - accuracy: 0.3469\n",
            "Epoch 00052: val_accuracy did not improve from 0.37778\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5364 - accuracy: 0.3216 - val_loss: 1.6313 - val_accuracy: 0.2667\n",
            "Epoch 53/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5388 - accuracy: 0.2941    \n",
            "Epoch 00053: val_accuracy did not improve from 0.37778\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5189 - accuracy: 0.2824 - val_loss: 1.6084 - val_accuracy: 0.3333\n",
            "Epoch 54/100\n",
            "46/85 [===============>..............] - ETA: 0s - loss: 1.5733 - accuracy: 0.2754\n",
            "Epoch 00054: val_accuracy did not improve from 0.37778\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5443 - accuracy: 0.2627 - val_loss: 1.5744 - val_accuracy: 0.3111\n",
            "Epoch 55/100\n",
            "55/85 [==================>...........] - ETA: 0s - loss: 1.5137 - accuracy: 0.2909    \n",
            "Epoch 00055: val_accuracy did not improve from 0.37778\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5065 - accuracy: 0.3098 - val_loss: 1.8196 - val_accuracy: 0.1556\n",
            "Epoch 56/100\n",
            "48/85 [===============>..............] - ETA: 0s - loss: 1.5375 - accuracy: 0.2917\n",
            "Epoch 00056: val_accuracy did not improve from 0.37778\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5208 - accuracy: 0.2863 - val_loss: 1.6438 - val_accuracy: 0.2889\n",
            "Epoch 57/100\n",
            "55/85 [==================>...........] - ETA: 0s - loss: 1.5004 - accuracy: 0.3030\n",
            "Epoch 00057: val_accuracy improved from 0.37778 to 0.44444, saving model to model_best.hdf5\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5169 - accuracy: 0.3020 - val_loss: 1.5369 - val_accuracy: 0.4444\n",
            "Epoch 58/100\n",
            "73/85 [========================>.....] - ETA: 0s - loss: 1.4830 - accuracy: 0.3242\n",
            "Epoch 00058: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.4919 - accuracy: 0.3137 - val_loss: 1.5404 - val_accuracy: 0.3778\n",
            "Epoch 59/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5124 - accuracy: 0.2941\n",
            "Epoch 00059: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5134 - accuracy: 0.3294 - val_loss: 1.9760 - val_accuracy: 0.1333\n",
            "Epoch 60/100\n",
            "57/85 [===================>..........] - ETA: 0s - loss: 1.5117 - accuracy: 0.3099\n",
            "Epoch 00060: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5345 - accuracy: 0.3059 - val_loss: 1.6819 - val_accuracy: 0.2000\n",
            "Epoch 61/100\n",
            "49/85 [================>.............] - ETA: 0s - loss: 1.5512 - accuracy: 0.2925\n",
            "Epoch 00061: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5222 - accuracy: 0.2824 - val_loss: 1.7141 - val_accuracy: 0.2000\n",
            "Epoch 62/100\n",
            "54/85 [==================>...........] - ETA: 0s - loss: 1.5530 - accuracy: 0.2716\n",
            "Epoch 00062: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5374 - accuracy: 0.2980 - val_loss: 1.5832 - val_accuracy: 0.2667\n",
            "Epoch 63/100\n",
            "47/85 [===============>..............] - ETA: 0s - loss: 1.5403 - accuracy: 0.3050\n",
            "Epoch 00063: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5177 - accuracy: 0.2745 - val_loss: 1.6401 - val_accuracy: 0.2667\n",
            "Epoch 64/100\n",
            "53/85 [=================>............] - ETA: 0s - loss: 1.5868 - accuracy: 0.2767    \n",
            "Epoch 00064: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5475 - accuracy: 0.2902 - val_loss: 1.6116 - val_accuracy: 0.2889\n",
            "Epoch 65/100\n",
            "47/85 [===============>..............] - ETA: 0s - loss: 1.5499 - accuracy: 0.3121\n",
            "Epoch 00065: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5271 - accuracy: 0.3098 - val_loss: 1.5798 - val_accuracy: 0.3111\n",
            "Epoch 66/100\n",
            "45/85 [==============>...............] - ETA: 0s - loss: 1.4826 - accuracy: 0.3481\n",
            "Epoch 00066: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5198 - accuracy: 0.2941 - val_loss: 1.6822 - val_accuracy: 0.2889\n",
            "Epoch 67/100\n",
            "55/85 [==================>...........] - ETA: 0s - loss: 1.4909 - accuracy: 0.2848    \n",
            "Epoch 00067: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5166 - accuracy: 0.2745 - val_loss: 1.6234 - val_accuracy: 0.2667\n",
            "Epoch 68/100\n",
            "54/85 [==================>...........] - ETA: 0s - loss: 1.5702 - accuracy: 0.3333\n",
            "Epoch 00068: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5323 - accuracy: 0.3255 - val_loss: 1.7593 - val_accuracy: 0.1333\n",
            "Epoch 69/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.5851 - accuracy: 0.2564\n",
            "Epoch 00069: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5311 - accuracy: 0.2941 - val_loss: 1.6108 - val_accuracy: 0.2889\n",
            "Epoch 70/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.5528 - accuracy: 0.2372\n",
            "Epoch 00070: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5596 - accuracy: 0.2471 - val_loss: 1.6354 - val_accuracy: 0.2667\n",
            "Epoch 71/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5259 - accuracy: 0.3007    \n",
            "Epoch 00071: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5259 - accuracy: 0.3059 - val_loss: 1.6005 - val_accuracy: 0.3556\n",
            "Epoch 72/100\n",
            "57/85 [===================>..........] - ETA: 0s - loss: 1.4639 - accuracy: 0.2807\n",
            "Epoch 00072: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5147 - accuracy: 0.2941 - val_loss: 1.6333 - val_accuracy: 0.3111\n",
            "Epoch 73/100\n",
            "53/85 [=================>............] - ETA: 0s - loss: 1.4998 - accuracy: 0.2893\n",
            "Epoch 00073: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5137 - accuracy: 0.2941 - val_loss: 1.6181 - val_accuracy: 0.2889\n",
            "Epoch 74/100\n",
            "41/85 [=============>................] - ETA: 0s - loss: 1.5000 - accuracy: 0.3415\n",
            "Epoch 00074: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5214 - accuracy: 0.3137 - val_loss: 1.8340 - val_accuracy: 0.1333\n",
            "Epoch 75/100\n",
            "44/85 [==============>...............] - ETA: 0s - loss: 1.4915 - accuracy: 0.3333\n",
            "Epoch 00075: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5064 - accuracy: 0.3059 - val_loss: 1.6201 - val_accuracy: 0.4000\n",
            "Epoch 76/100\n",
            "49/85 [================>.............] - ETA: 0s - loss: 1.4882 - accuracy: 0.3197\n",
            "Epoch 00076: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4984 - accuracy: 0.2863 - val_loss: 1.6934 - val_accuracy: 0.2444\n",
            "Epoch 77/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5103 - accuracy: 0.2941    \n",
            "Epoch 00077: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5227 - accuracy: 0.2706 - val_loss: 1.6389 - val_accuracy: 0.2667\n",
            "Epoch 78/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.4861 - accuracy: 0.3268    \n",
            "Epoch 00078: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4917 - accuracy: 0.3255 - val_loss: 1.6482 - val_accuracy: 0.3111\n",
            "Epoch 79/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.5054 - accuracy: 0.3397    \n",
            "Epoch 00079: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5189 - accuracy: 0.3176 - val_loss: 1.6919 - val_accuracy: 0.3111\n",
            "Epoch 80/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5473 - accuracy: 0.3137\n",
            "Epoch 00080: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5065 - accuracy: 0.3059 - val_loss: 1.8158 - val_accuracy: 0.1556\n",
            "Epoch 81/100\n",
            "49/85 [================>.............] - ETA: 0s - loss: 1.4997 - accuracy: 0.2721    \n",
            "Epoch 00081: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4978 - accuracy: 0.2863 - val_loss: 1.6023 - val_accuracy: 0.3556\n",
            "Epoch 82/100\n",
            "48/85 [===============>..............] - ETA: 0s - loss: 1.5153 - accuracy: 0.2986    \n",
            "Epoch 00082: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 1.4836 - accuracy: 0.3098 - val_loss: 1.8671 - val_accuracy: 0.1556\n",
            "Epoch 83/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5206 - accuracy: 0.3067\n",
            "Epoch 00083: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4950 - accuracy: 0.3059 - val_loss: 1.6353 - val_accuracy: 0.2667\n",
            "Epoch 84/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5861 - accuracy: 0.2600    \n",
            "Epoch 00084: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5272 - accuracy: 0.2784 - val_loss: 1.6049 - val_accuracy: 0.3111\n",
            "Epoch 85/100\n",
            "55/85 [==================>...........] - ETA: 0s - loss: 1.4902 - accuracy: 0.3091\n",
            "Epoch 00085: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4977 - accuracy: 0.3020 - val_loss: 1.5843 - val_accuracy: 0.3556\n",
            "Epoch 86/100\n",
            "54/85 [==================>...........] - ETA: 0s - loss: 1.5165 - accuracy: 0.3086\n",
            "Epoch 00086: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4999 - accuracy: 0.2980 - val_loss: 1.6135 - val_accuracy: 0.4000\n",
            "Epoch 87/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5192 - accuracy: 0.2933\n",
            "Epoch 00087: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5090 - accuracy: 0.3059 - val_loss: 1.6408 - val_accuracy: 0.2667\n",
            "Epoch 88/100\n",
            "48/85 [===============>..............] - ETA: 0s - loss: 1.4858 - accuracy: 0.3611\n",
            "Epoch 00088: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5234 - accuracy: 0.3137 - val_loss: 1.6407 - val_accuracy: 0.3111\n",
            "Epoch 89/100\n",
            "48/85 [===============>..............] - ETA: 0s - loss: 1.4633 - accuracy: 0.2917\n",
            "Epoch 00089: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4922 - accuracy: 0.2824 - val_loss: 1.7084 - val_accuracy: 0.2667\n",
            "Epoch 90/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5602 - accuracy: 0.2800    \n",
            "Epoch 00090: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5240 - accuracy: 0.2941 - val_loss: 1.6198 - val_accuracy: 0.3778\n",
            "Epoch 91/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.4621 - accuracy: 0.3595\n",
            "Epoch 00091: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5106 - accuracy: 0.3608 - val_loss: 1.7313 - val_accuracy: 0.2889\n",
            "Epoch 92/100\n",
            "50/85 [================>.............] - ETA: 0s - loss: 1.5308 - accuracy: 0.2733    \n",
            "Epoch 00092: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4964 - accuracy: 0.3059 - val_loss: 1.7074 - val_accuracy: 0.3333\n",
            "Epoch 93/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.5186 - accuracy: 0.2949\n",
            "Epoch 00093: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5159 - accuracy: 0.2863 - val_loss: 1.6671 - val_accuracy: 0.2889\n",
            "Epoch 94/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.4617 - accuracy: 0.2436\n",
            "Epoch 00094: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5087 - accuracy: 0.2549 - val_loss: 1.7734 - val_accuracy: 0.1778\n",
            "Epoch 95/100\n",
            "54/85 [==================>...........] - ETA: 0s - loss: 1.4567 - accuracy: 0.3086\n",
            "Epoch 00095: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5016 - accuracy: 0.2902 - val_loss: 1.6564 - val_accuracy: 0.2444\n",
            "Epoch 96/100\n",
            "53/85 [=================>............] - ETA: 0s - loss: 1.4944 - accuracy: 0.3270    \n",
            "Epoch 00096: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5038 - accuracy: 0.2980 - val_loss: 1.6494 - val_accuracy: 0.2889\n",
            "Epoch 97/100\n",
            "45/85 [==============>...............] - ETA: 0s - loss: 1.5233 - accuracy: 0.2593\n",
            "Epoch 00097: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4968 - accuracy: 0.2745 - val_loss: 1.6218 - val_accuracy: 0.2667\n",
            "Epoch 98/100\n",
            "51/85 [=================>............] - ETA: 0s - loss: 1.5447 - accuracy: 0.3137\n",
            "Epoch 00098: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.4892 - accuracy: 0.3255 - val_loss: 1.6341 - val_accuracy: 0.4000\n",
            "Epoch 99/100\n",
            "52/85 [=================>............] - ETA: 0s - loss: 1.5380 - accuracy: 0.2949\n",
            "Epoch 00099: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5180 - accuracy: 0.3098 - val_loss: 1.7891 - val_accuracy: 0.1556\n",
            "Epoch 100/100\n",
            "55/85 [==================>...........] - ETA: 0s - loss: 1.5079 - accuracy: 0.2970    \n",
            "Epoch 00100: val_accuracy did not improve from 0.44444\n",
            "85/85 [==============================] - 0s 1ms/step - loss: 1.5447 - accuracy: 0.2863 - val_loss: 1.7161 - val_accuracy: 0.2889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdNiYQ3vYOm7",
        "outputId": "4410afb3-c6b3-4085-d97f-f0d60789dc6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
        "'''model.load_weights('model_best.hdf5')\n",
        "model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])'''\n",
        "stop = EarlyStopping(monitor='val_accuracy',  patience=80, verbose=0, mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=25, verbose=1,\n",
        "    factor=0.5,\n",
        "    min_lr = 0.00001,\n",
        "    cooldown=0\n",
        ")\n",
        "checkpoint = [ModelCheckpoint('model_best.hdf5',monitor='val_accuracy', \n",
        "        save_best_only=True, \n",
        "        mode='max',\n",
        "        verbose=1)\n",
        "]\n",
        "hist=model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[checkpoint,reduce_lr,stop],epochs=500, batch_size=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.5273 - accuracy: 0.2936\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.13333, saving model to model_best.hdf5\n",
            "128/128 [==============================] - 0s 3ms/step - loss: 1.5331 - accuracy: 0.2941 - val_loss: 1.9023 - val_accuracy: 0.1333\n",
            "Epoch 2/500\n",
            "101/128 [======================>.......] - ETA: 0s - loss: 1.5427 - accuracy: 0.2871\n",
            "Epoch 00002: val_accuracy improved from 0.13333 to 0.28889, saving model to model_best.hdf5\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5289 - accuracy: 0.2863 - val_loss: 1.6570 - val_accuracy: 0.2889\n",
            "Epoch 3/500\n",
            "126/128 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.2897\n",
            "Epoch 00003: val_accuracy did not improve from 0.28889\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5251 - accuracy: 0.2863 - val_loss: 1.6783 - val_accuracy: 0.2667\n",
            "Epoch 4/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.5102 - accuracy: 0.3125\n",
            "Epoch 00004: val_accuracy did not improve from 0.28889\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4976 - accuracy: 0.3098 - val_loss: 2.2214 - val_accuracy: 0.1556\n",
            "Epoch 5/500\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 1.5032 - accuracy: 0.3186\n",
            "Epoch 00005: val_accuracy did not improve from 0.28889\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5261 - accuracy: 0.3020 - val_loss: 1.7629 - val_accuracy: 0.2667\n",
            "Epoch 6/500\n",
            "106/128 [=======================>......] - ETA: 0s - loss: 1.5376 - accuracy: 0.3019\n",
            "Epoch 00006: val_accuracy did not improve from 0.28889\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5285 - accuracy: 0.2980 - val_loss: 1.7242 - val_accuracy: 0.2444\n",
            "Epoch 7/500\n",
            "113/128 [=========================>....] - ETA: 0s - loss: 1.5204 - accuracy: 0.3053\n",
            "Epoch 00007: val_accuracy did not improve from 0.28889\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5486 - accuracy: 0.2941 - val_loss: 1.7183 - val_accuracy: 0.2889\n",
            "Epoch 8/500\n",
            "113/128 [=========================>....] - ETA: 0s - loss: 1.5455 - accuracy: 0.2965\n",
            "Epoch 00008: val_accuracy did not improve from 0.28889\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5393 - accuracy: 0.2941 - val_loss: 1.6739 - val_accuracy: 0.2889\n",
            "Epoch 9/500\n",
            "107/128 [========================>.....] - ETA: 0s - loss: 1.5384 - accuracy: 0.2804\n",
            "Epoch 00009: val_accuracy did not improve from 0.28889\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5646 - accuracy: 0.2824 - val_loss: 1.6284 - val_accuracy: 0.2889\n",
            "Epoch 10/500\n",
            "111/128 [=========================>....] - ETA: 0s - loss: 1.5658 - accuracy: 0.3198\n",
            "Epoch 00010: val_accuracy improved from 0.28889 to 0.35556, saving model to model_best.hdf5\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5454 - accuracy: 0.3098 - val_loss: 1.6305 - val_accuracy: 0.3556\n",
            "Epoch 11/500\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 1.5137 - accuracy: 0.2857\n",
            "Epoch 00011: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5102 - accuracy: 0.2824 - val_loss: 1.8540 - val_accuracy: 0.0889\n",
            "Epoch 12/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.4799 - accuracy: 0.2981\n",
            "Epoch 00012: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5220 - accuracy: 0.2824 - val_loss: 1.6677 - val_accuracy: 0.2889\n",
            "Epoch 13/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.5054 - accuracy: 0.2706\n",
            "Epoch 00013: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5172 - accuracy: 0.2667 - val_loss: 1.6441 - val_accuracy: 0.3333\n",
            "Epoch 14/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.5478 - accuracy: 0.3173\n",
            "Epoch 00014: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5238 - accuracy: 0.3412 - val_loss: 1.6952 - val_accuracy: 0.3556\n",
            "Epoch 15/500\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 1.5264 - accuracy: 0.3571\n",
            "Epoch 00015: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5214 - accuracy: 0.3412 - val_loss: 1.9475 - val_accuracy: 0.1111\n",
            "Epoch 16/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.5035 - accuracy: 0.3091\n",
            "Epoch 00016: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5448 - accuracy: 0.2980 - val_loss: 1.6926 - val_accuracy: 0.2889\n",
            "Epoch 17/500\n",
            "113/128 [=========================>....] - ETA: 0s - loss: 1.5282 - accuracy: 0.2832\n",
            "Epoch 00017: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5256 - accuracy: 0.2824 - val_loss: 1.7208 - val_accuracy: 0.2667\n",
            "Epoch 18/500\n",
            "124/128 [============================>.] - ETA: 0s - loss: 1.5123 - accuracy: 0.3024\n",
            "Epoch 00018: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5227 - accuracy: 0.3059 - val_loss: 1.7343 - val_accuracy: 0.2222\n",
            "Epoch 19/500\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 1.4913 - accuracy: 0.3367\n",
            "Epoch 00019: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5096 - accuracy: 0.3216 - val_loss: 1.7530 - val_accuracy: 0.2444\n",
            "Epoch 20/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.5044 - accuracy: 0.3269\n",
            "Epoch 00020: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5102 - accuracy: 0.3255 - val_loss: 1.8259 - val_accuracy: 0.2000\n",
            "Epoch 21/500\n",
            "114/128 [=========================>....] - ETA: 0s - loss: 1.5099 - accuracy: 0.2807\n",
            "Epoch 00021: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5117 - accuracy: 0.3059 - val_loss: 1.7656 - val_accuracy: 0.3111\n",
            "Epoch 22/500\n",
            "115/128 [=========================>....] - ETA: 0s - loss: 1.5425 - accuracy: 0.3043\n",
            "Epoch 00022: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5428 - accuracy: 0.2980 - val_loss: 1.9584 - val_accuracy: 0.0667\n",
            "Epoch 23/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.5239 - accuracy: 0.3211\n",
            "Epoch 00023: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5157 - accuracy: 0.3216 - val_loss: 1.9292 - val_accuracy: 0.1111\n",
            "Epoch 24/500\n",
            " 96/128 [=====================>........] - ETA: 0s - loss: 1.5039 - accuracy: 0.3698\n",
            "Epoch 00024: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5304 - accuracy: 0.3333 - val_loss: 1.7323 - val_accuracy: 0.2889\n",
            "Epoch 25/500\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 1.5214 - accuracy: 0.3235\n",
            "Epoch 00025: val_accuracy did not improve from 0.35556\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5303 - accuracy: 0.3137 - val_loss: 1.6571 - val_accuracy: 0.3111\n",
            "Epoch 26/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.5448 - accuracy: 0.3028\n",
            "Epoch 00026: val_accuracy improved from 0.35556 to 0.37778, saving model to model_best.hdf5\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5433 - accuracy: 0.3137 - val_loss: 1.6562 - val_accuracy: 0.3778\n",
            "Epoch 27/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.5378 - accuracy: 0.3077\n",
            "Epoch 00027: val_accuracy did not improve from 0.37778\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5440 - accuracy: 0.3059 - val_loss: 1.7354 - val_accuracy: 0.2000\n",
            "Epoch 28/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.5071 - accuracy: 0.2524\n",
            "Epoch 00028: val_accuracy did not improve from 0.37778\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5268 - accuracy: 0.2588 - val_loss: 1.7039 - val_accuracy: 0.2889\n",
            "Epoch 29/500\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 1.5519 - accuracy: 0.3010\n",
            "Epoch 00029: val_accuracy did not improve from 0.37778\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5305 - accuracy: 0.2980 - val_loss: 1.8025 - val_accuracy: 0.3111\n",
            "Epoch 30/500\n",
            "112/128 [=========================>....] - ETA: 0s - loss: 1.5197 - accuracy: 0.3170\n",
            "Epoch 00030: val_accuracy did not improve from 0.37778\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5116 - accuracy: 0.3451 - val_loss: 1.7890 - val_accuracy: 0.2667\n",
            "Epoch 31/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.5114 - accuracy: 0.3303\n",
            "Epoch 00031: val_accuracy did not improve from 0.37778\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5498 - accuracy: 0.3098 - val_loss: 1.8434 - val_accuracy: 0.2444\n",
            "Epoch 32/500\n",
            "107/128 [========================>.....] - ETA: 0s - loss: 1.4955 - accuracy: 0.3178\n",
            "Epoch 00032: val_accuracy did not improve from 0.37778\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5061 - accuracy: 0.3098 - val_loss: 1.8964 - val_accuracy: 0.2000\n",
            "Epoch 33/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.5595 - accuracy: 0.2407\n",
            "Epoch 00033: val_accuracy improved from 0.37778 to 0.42222, saving model to model_best.hdf5\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5424 - accuracy: 0.2549 - val_loss: 1.6984 - val_accuracy: 0.4222\n",
            "Epoch 34/500\n",
            "114/128 [=========================>....] - ETA: 0s - loss: 1.5124 - accuracy: 0.3070\n",
            "Epoch 00034: val_accuracy did not improve from 0.42222\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5283 - accuracy: 0.2980 - val_loss: 1.7336 - val_accuracy: 0.3111\n",
            "Epoch 35/500\n",
            "100/128 [======================>.......] - ETA: 0s - loss: 1.5233 - accuracy: 0.3050\n",
            "Epoch 00035: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5363 - accuracy: 0.3098 - val_loss: 1.8619 - val_accuracy: 0.2000\n",
            "Epoch 36/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.5455 - accuracy: 0.2870\n",
            "Epoch 00036: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5169 - accuracy: 0.3098 - val_loss: 1.7068 - val_accuracy: 0.2889\n",
            "Epoch 37/500\n",
            "101/128 [======================>.......] - ETA: 0s - loss: 1.4916 - accuracy: 0.3564\n",
            "Epoch 00037: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5148 - accuracy: 0.3333 - val_loss: 1.7144 - val_accuracy: 0.3333\n",
            "Epoch 38/500\n",
            "115/128 [=========================>....] - ETA: 0s - loss: 1.4580 - accuracy: 0.3174\n",
            "Epoch 00038: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4586 - accuracy: 0.3098 - val_loss: 1.7999 - val_accuracy: 0.2889\n",
            "Epoch 39/500\n",
            "115/128 [=========================>....] - ETA: 0s - loss: 1.4578 - accuracy: 0.3739\n",
            "Epoch 00039: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5059 - accuracy: 0.3490 - val_loss: 1.7428 - val_accuracy: 0.2444\n",
            "Epoch 40/500\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 1.5100 - accuracy: 0.3627\n",
            "Epoch 00040: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4911 - accuracy: 0.3451 - val_loss: 1.6627 - val_accuracy: 0.3556\n",
            "Epoch 41/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.4630 - accuracy: 0.3857\n",
            "Epoch 00041: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4954 - accuracy: 0.3490 - val_loss: 1.8428 - val_accuracy: 0.2444\n",
            "Epoch 42/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.4990 - accuracy: 0.3227\n",
            "Epoch 00042: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4914 - accuracy: 0.3294 - val_loss: 1.6556 - val_accuracy: 0.3333\n",
            "Epoch 43/500\n",
            "114/128 [=========================>....] - ETA: 0s - loss: 1.4778 - accuracy: 0.3640\n",
            "Epoch 00043: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4868 - accuracy: 0.3451 - val_loss: 1.7220 - val_accuracy: 0.3111\n",
            "Epoch 44/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.4437 - accuracy: 0.3413\n",
            "Epoch 00044: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4715 - accuracy: 0.3294 - val_loss: 1.7718 - val_accuracy: 0.2889\n",
            "Epoch 45/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.5043 - accuracy: 0.3227\n",
            "Epoch 00045: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4985 - accuracy: 0.3098 - val_loss: 1.7592 - val_accuracy: 0.2889\n",
            "Epoch 46/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.5445 - accuracy: 0.3221\n",
            "Epoch 00046: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5142 - accuracy: 0.3333 - val_loss: 1.7485 - val_accuracy: 0.3111\n",
            "Epoch 47/500\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 1.4842 - accuracy: 0.3333\n",
            "Epoch 00047: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4825 - accuracy: 0.3176 - val_loss: 1.7756 - val_accuracy: 0.2889\n",
            "Epoch 48/500\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 1.5177 - accuracy: 0.3107\n",
            "Epoch 00048: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5301 - accuracy: 0.2980 - val_loss: 1.7370 - val_accuracy: 0.3333\n",
            "Epoch 49/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.4973 - accuracy: 0.3045\n",
            "Epoch 00049: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4936 - accuracy: 0.2980 - val_loss: 1.7415 - val_accuracy: 0.2889\n",
            "Epoch 50/500\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 1.5246 - accuracy: 0.2913\n",
            "Epoch 00050: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5041 - accuracy: 0.3059 - val_loss: 1.7234 - val_accuracy: 0.2889\n",
            "Epoch 51/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.5325 - accuracy: 0.3227\n",
            "Epoch 00051: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5286 - accuracy: 0.3333 - val_loss: 1.6915 - val_accuracy: 0.3111\n",
            "Epoch 52/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4834 - accuracy: 0.3349\n",
            "Epoch 00052: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4861 - accuracy: 0.3373 - val_loss: 1.7669 - val_accuracy: 0.2667\n",
            "Epoch 53/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.4982 - accuracy: 0.3380\n",
            "Epoch 00053: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4942 - accuracy: 0.3255 - val_loss: 1.8372 - val_accuracy: 0.2444\n",
            "Epoch 54/500\n",
            "113/128 [=========================>....] - ETA: 0s - loss: 1.4845 - accuracy: 0.3230\n",
            "Epoch 00054: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5143 - accuracy: 0.3176 - val_loss: 1.8060 - val_accuracy: 0.2667\n",
            "Epoch 55/500\n",
            "125/128 [============================>.] - ETA: 0s - loss: 1.5098 - accuracy: 0.2960\n",
            "Epoch 00055: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5098 - accuracy: 0.2941 - val_loss: 1.8414 - val_accuracy: 0.2667\n",
            "Epoch 56/500\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 1.4802 - accuracy: 0.3010\n",
            "Epoch 00056: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4779 - accuracy: 0.3020 - val_loss: 1.7356 - val_accuracy: 0.2667\n",
            "Epoch 57/500\n",
            "100/128 [======================>.......] - ETA: 0s - loss: 1.4555 - accuracy: 0.3850\n",
            "Epoch 00057: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4932 - accuracy: 0.3412 - val_loss: 1.8999 - val_accuracy: 0.2222\n",
            "Epoch 58/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.5170 - accuracy: 0.3102\n",
            "Epoch 00058: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4881 - accuracy: 0.3176 - val_loss: 1.7247 - val_accuracy: 0.3778\n",
            "Epoch 59/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4987 - accuracy: 0.3119\n",
            "Epoch 00059: val_accuracy did not improve from 0.42222\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4773 - accuracy: 0.3216 - val_loss: 1.7129 - val_accuracy: 0.4000\n",
            "Epoch 60/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4799 - accuracy: 0.3165\n",
            "Epoch 00060: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4746 - accuracy: 0.3255 - val_loss: 1.8205 - val_accuracy: 0.2667\n",
            "Epoch 61/500\n",
            "113/128 [=========================>....] - ETA: 0s - loss: 1.4537 - accuracy: 0.3142\n",
            "Epoch 00061: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4659 - accuracy: 0.3098 - val_loss: 1.7789 - val_accuracy: 0.2889\n",
            "Epoch 62/500\n",
            "107/128 [========================>.....] - ETA: 0s - loss: 1.4833 - accuracy: 0.3037\n",
            "Epoch 00062: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5210 - accuracy: 0.3137 - val_loss: 1.7661 - val_accuracy: 0.3111\n",
            "Epoch 63/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.4550 - accuracy: 0.3702\n",
            "Epoch 00063: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4619 - accuracy: 0.3686 - val_loss: 1.7975 - val_accuracy: 0.2889\n",
            "Epoch 64/500\n",
            "107/128 [========================>.....] - ETA: 0s - loss: 1.4943 - accuracy: 0.3131\n",
            "Epoch 00064: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4743 - accuracy: 0.3333 - val_loss: 1.7623 - val_accuracy: 0.2889\n",
            "Epoch 65/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4860 - accuracy: 0.3257\n",
            "Epoch 00065: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4665 - accuracy: 0.3294 - val_loss: 1.8631 - val_accuracy: 0.2667\n",
            "Epoch 66/500\n",
            " 97/128 [=====================>........] - ETA: 0s - loss: 1.3919 - accuracy: 0.3660\n",
            "Epoch 00066: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4850 - accuracy: 0.3255 - val_loss: 1.8433 - val_accuracy: 0.2667\n",
            "Epoch 67/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.4650 - accuracy: 0.3287\n",
            "Epoch 00067: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4680 - accuracy: 0.3373 - val_loss: 1.8053 - val_accuracy: 0.2889\n",
            "Epoch 68/500\n",
            "111/128 [=========================>....] - ETA: 0s - loss: 1.4722 - accuracy: 0.3198\n",
            "Epoch 00068: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4715 - accuracy: 0.3176 - val_loss: 1.7421 - val_accuracy: 0.2889\n",
            "Epoch 69/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.4757 - accuracy: 0.3750\n",
            "Epoch 00069: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4930 - accuracy: 0.3686 - val_loss: 1.7980 - val_accuracy: 0.2889\n",
            "Epoch 70/500\n",
            "101/128 [======================>.......] - ETA: 0s - loss: 1.4730 - accuracy: 0.3366\n",
            "Epoch 00070: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4742 - accuracy: 0.3216 - val_loss: 1.7277 - val_accuracy: 0.3556\n",
            "Epoch 71/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.4633 - accuracy: 0.3048\n",
            "Epoch 00071: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4734 - accuracy: 0.3098 - val_loss: 1.7886 - val_accuracy: 0.3111\n",
            "Epoch 72/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.4517 - accuracy: 0.3238\n",
            "Epoch 00072: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5018 - accuracy: 0.3216 - val_loss: 1.7516 - val_accuracy: 0.3111\n",
            "Epoch 73/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.4910 - accuracy: 0.3190\n",
            "Epoch 00073: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4832 - accuracy: 0.3294 - val_loss: 1.7392 - val_accuracy: 0.3111\n",
            "Epoch 74/500\n",
            "123/128 [===========================>..] - ETA: 0s - loss: 1.5015 - accuracy: 0.3374\n",
            "Epoch 00074: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4975 - accuracy: 0.3294 - val_loss: 1.7443 - val_accuracy: 0.3111\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.4582 - accuracy: 0.3137\n",
            "Epoch 00075: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4582 - accuracy: 0.3137 - val_loss: 1.7823 - val_accuracy: 0.2889\n",
            "Epoch 76/500\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 1.4794 - accuracy: 0.3578\n",
            "Epoch 00076: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4646 - accuracy: 0.3647 - val_loss: 1.7676 - val_accuracy: 0.3111\n",
            "Epoch 77/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4740 - accuracy: 0.3073\n",
            "Epoch 00077: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4929 - accuracy: 0.2980 - val_loss: 1.7729 - val_accuracy: 0.2889\n",
            "Epoch 78/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.5419 - accuracy: 0.2982\n",
            "Epoch 00078: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5091 - accuracy: 0.3020 - val_loss: 1.7264 - val_accuracy: 0.3333\n",
            "Epoch 79/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.4741 - accuracy: 0.3364\n",
            "Epoch 00079: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4808 - accuracy: 0.3294 - val_loss: 1.7661 - val_accuracy: 0.2889\n",
            "Epoch 80/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4286 - accuracy: 0.3440\n",
            "Epoch 00080: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4516 - accuracy: 0.3294 - val_loss: 1.7590 - val_accuracy: 0.2889\n",
            "Epoch 81/500\n",
            "106/128 [=======================>......] - ETA: 0s - loss: 1.4658 - accuracy: 0.3396\n",
            "Epoch 00081: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4852 - accuracy: 0.3059 - val_loss: 1.8379 - val_accuracy: 0.2444\n",
            "Epoch 82/500\n",
            "112/128 [=========================>....] - ETA: 0s - loss: 1.4828 - accuracy: 0.3482\n",
            "Epoch 00082: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4767 - accuracy: 0.3412 - val_loss: 1.8229 - val_accuracy: 0.3111\n",
            "Epoch 83/500\n",
            "107/128 [========================>.....] - ETA: 0s - loss: 1.5320 - accuracy: 0.2710\n",
            "Epoch 00083: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5266 - accuracy: 0.2784 - val_loss: 1.7254 - val_accuracy: 0.3111\n",
            "Epoch 84/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4770 - accuracy: 0.2752\n",
            "Epoch 00084: val_accuracy did not improve from 0.42222\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4632 - accuracy: 0.2980 - val_loss: 1.7548 - val_accuracy: 0.2889\n",
            "Epoch 85/500\n",
            " 97/128 [=====================>........] - ETA: 0s - loss: 1.4268 - accuracy: 0.3247\n",
            "Epoch 00085: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4647 - accuracy: 0.3176 - val_loss: 1.7339 - val_accuracy: 0.3111\n",
            "Epoch 86/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4425 - accuracy: 0.3303\n",
            "Epoch 00086: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4742 - accuracy: 0.3333 - val_loss: 1.7049 - val_accuracy: 0.2889\n",
            "Epoch 87/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.4880 - accuracy: 0.2981\n",
            "Epoch 00087: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4717 - accuracy: 0.2941 - val_loss: 1.7473 - val_accuracy: 0.2889\n",
            "Epoch 88/500\n",
            "113/128 [=========================>....] - ETA: 0s - loss: 1.4658 - accuracy: 0.3319\n",
            "Epoch 00088: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4697 - accuracy: 0.3176 - val_loss: 1.7446 - val_accuracy: 0.3111\n",
            "Epoch 89/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.4304 - accuracy: 0.3667\n",
            "Epoch 00089: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4488 - accuracy: 0.3490 - val_loss: 1.7736 - val_accuracy: 0.2889\n",
            "Epoch 90/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.4080 - accuracy: 0.3558\n",
            "Epoch 00090: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4686 - accuracy: 0.3412 - val_loss: 1.7560 - val_accuracy: 0.2889\n",
            "Epoch 91/500\n",
            "114/128 [=========================>....] - ETA: 0s - loss: 1.4227 - accuracy: 0.3377\n",
            "Epoch 00091: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4416 - accuracy: 0.3333 - val_loss: 1.7865 - val_accuracy: 0.2667\n",
            "Epoch 92/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.4684 - accuracy: 0.3227\n",
            "Epoch 00092: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4645 - accuracy: 0.3176 - val_loss: 1.7495 - val_accuracy: 0.2667\n",
            "Epoch 93/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.4669 - accuracy: 0.3519\n",
            "Epoch 00093: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4753 - accuracy: 0.3529 - val_loss: 1.7445 - val_accuracy: 0.2889\n",
            "Epoch 94/500\n",
            "113/128 [=========================>....] - ETA: 0s - loss: 1.4257 - accuracy: 0.3274\n",
            "Epoch 00094: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4389 - accuracy: 0.3255 - val_loss: 1.7412 - val_accuracy: 0.2889\n",
            "Epoch 95/500\n",
            "115/128 [=========================>....] - ETA: 0s - loss: 1.3998 - accuracy: 0.3217\n",
            "Epoch 00095: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4377 - accuracy: 0.3176 - val_loss: 1.8063 - val_accuracy: 0.2667\n",
            "Epoch 96/500\n",
            "101/128 [======================>.......] - ETA: 0s - loss: 1.4820 - accuracy: 0.3465\n",
            "Epoch 00096: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4561 - accuracy: 0.3529 - val_loss: 1.7422 - val_accuracy: 0.2889\n",
            "Epoch 97/500\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 1.4377 - accuracy: 0.3704\n",
            "Epoch 00097: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4780 - accuracy: 0.3412 - val_loss: 1.7584 - val_accuracy: 0.2889\n",
            "Epoch 98/500\n",
            "127/128 [============================>.] - ETA: 0s - loss: 1.4537 - accuracy: 0.3189\n",
            "Epoch 00098: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4504 - accuracy: 0.3216 - val_loss: 1.8288 - val_accuracy: 0.2444\n",
            "Epoch 99/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.4272 - accuracy: 0.3381\n",
            "Epoch 00099: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4343 - accuracy: 0.3333 - val_loss: 1.7559 - val_accuracy: 0.2889\n",
            "Epoch 100/500\n",
            "111/128 [=========================>....] - ETA: 0s - loss: 1.4763 - accuracy: 0.3514\n",
            "Epoch 00100: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4839 - accuracy: 0.3608 - val_loss: 1.7441 - val_accuracy: 0.2889\n",
            "Epoch 101/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.4596 - accuracy: 0.3606\n",
            "Epoch 00101: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4591 - accuracy: 0.3529 - val_loss: 1.7391 - val_accuracy: 0.3111\n",
            "Epoch 102/500\n",
            " 95/128 [=====================>........] - ETA: 0s - loss: 1.4520 - accuracy: 0.3421\n",
            "Epoch 00102: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4340 - accuracy: 0.3569 - val_loss: 1.7729 - val_accuracy: 0.2667\n",
            "Epoch 103/500\n",
            "112/128 [=========================>....] - ETA: 0s - loss: 1.4362 - accuracy: 0.3170\n",
            "Epoch 00103: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4496 - accuracy: 0.3098 - val_loss: 1.8109 - val_accuracy: 0.2889\n",
            "Epoch 104/500\n",
            "114/128 [=========================>....] - ETA: 0s - loss: 1.4109 - accuracy: 0.3421\n",
            "Epoch 00104: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4585 - accuracy: 0.3373 - val_loss: 1.7408 - val_accuracy: 0.3111\n",
            "Epoch 105/500\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 1.4080 - accuracy: 0.3318\n",
            "Epoch 00105: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4336 - accuracy: 0.3255 - val_loss: 1.7669 - val_accuracy: 0.2889\n",
            "Epoch 106/500\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 1.5145 - accuracy: 0.3365\n",
            "Epoch 00106: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.5029 - accuracy: 0.3255 - val_loss: 1.7739 - val_accuracy: 0.2889\n",
            "Epoch 107/500\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 1.4717 - accuracy: 0.3431\n",
            "Epoch 00107: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4537 - accuracy: 0.3529 - val_loss: 1.7587 - val_accuracy: 0.2889\n",
            "Epoch 108/500\n",
            "107/128 [========================>.....] - ETA: 0s - loss: 1.4312 - accuracy: 0.3738\n",
            "Epoch 00108: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4438 - accuracy: 0.3529 - val_loss: 1.7683 - val_accuracy: 0.2889\n",
            "Epoch 109/500\n",
            "107/128 [========================>.....] - ETA: 0s - loss: 1.4406 - accuracy: 0.3972\n",
            "Epoch 00109: val_accuracy did not improve from 0.42222\n",
            "\n",
            "Epoch 00109: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4593 - accuracy: 0.3765 - val_loss: 1.7980 - val_accuracy: 0.2889\n",
            "Epoch 110/500\n",
            " 99/128 [======================>.......] - ETA: 0s - loss: 1.4497 - accuracy: 0.3586\n",
            "Epoch 00110: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4653 - accuracy: 0.3216 - val_loss: 1.7801 - val_accuracy: 0.2889\n",
            "Epoch 111/500\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 1.5200 - accuracy: 0.2952\n",
            "Epoch 00111: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4871 - accuracy: 0.3020 - val_loss: 1.7734 - val_accuracy: 0.2889\n",
            "Epoch 112/500\n",
            "111/128 [=========================>....] - ETA: 0s - loss: 1.4446 - accuracy: 0.3243\n",
            "Epoch 00112: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4832 - accuracy: 0.3098 - val_loss: 1.7601 - val_accuracy: 0.2889\n",
            "Epoch 113/500\n",
            "109/128 [========================>.....] - ETA: 0s - loss: 1.4295 - accuracy: 0.3532\n",
            "Epoch 00113: val_accuracy did not improve from 0.42222\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 1.4328 - accuracy: 0.3529 - val_loss: 1.7723 - val_accuracy: 0.2667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnQdoM40apWq",
        "outputId": "e1206e1e-3f99-4df2-9f15-0616d35d761a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.title('Train Accuracy vs Val Accuracy')\n",
        "plt.plot(hist.history['accuracy'], label='Train Accuracy', color='black')\n",
        "plt.plot(hist.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKv9DT3KLTvr"
      },
      "source": [
        "pr = model.predict(X, batch_size=3, verbose=0, steps=None)\n",
        "pp = pr.argmax(axis=1)\n",
        "yp = []\n",
        "for i in pp:\n",
        "  if i == 0:\n",
        "    yp.append(0.001)\n",
        "  elif i == 1:\n",
        "    yp.append(0.01)\n",
        "  elif i == 2:\n",
        "    yp.append(0.025)\n",
        "  elif i == 3:\n",
        "    yp.append(0.05)\n",
        "  elif i == 4:\n",
        "    yp.append(0.1)\n",
        "  else:\n",
        "    yp.append(0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqLVy0XIWMSB"
      },
      "source": [
        "yt = []\n",
        "for i in y:\n",
        "  if i == 0:\n",
        "    yt.append(0.001)\n",
        "  elif i == 1:\n",
        "    yt.append(0.01)\n",
        "  elif i == 2:\n",
        "    yt.append(0.025)\n",
        "  elif i == 3:\n",
        "    yt.append(0.05)\n",
        "  elif i == 4:\n",
        "    yt.append(0.1)\n",
        "  else:\n",
        "    yt.append(0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKHH1Zim0dtf",
        "outputId": "df7c0ce1-ea42-45a3-dd9c-baf202886c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(yt, yp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSQpbss501K-"
      },
      "source": [
        "issue = pd.read_csv(\"/content/issueframePSNRs.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i6JT3as1UQb",
        "outputId": "38109728-4cbb-44c0-e90d-fae78cf82bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "issue.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>name</th>\n",
              "      <th>Condition</th>\n",
              "      <th>Frame</th>\n",
              "      <th>42</th>\n",
              "      <th>41</th>\n",
              "      <th>40</th>\n",
              "      <th>39</th>\n",
              "      <th>42/F</th>\n",
              "      <th>41/F</th>\n",
              "      <th>40/F</th>\n",
              "      <th>39/F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>L:/Videodata/issue/received/0.mp4</td>\n",
              "      <td>1200</td>\n",
              "      <td>6114</td>\n",
              "      <td>5901</td>\n",
              "      <td>5872</td>\n",
              "      <td>5860</td>\n",
              "      <td>5851</td>\n",
              "      <td>0.965162</td>\n",
              "      <td>0.960419</td>\n",
              "      <td>0.958456</td>\n",
              "      <td>0.956984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>L:/Videodata/issue/received/1.mp4</td>\n",
              "      <td>1300</td>\n",
              "      <td>6114</td>\n",
              "      <td>5243</td>\n",
              "      <td>5100</td>\n",
              "      <td>5050</td>\n",
              "      <td>5032</td>\n",
              "      <td>0.857540</td>\n",
              "      <td>0.834151</td>\n",
              "      <td>0.825973</td>\n",
              "      <td>0.823029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>L:/Videodata/issue/received/2.mp4</td>\n",
              "      <td>1300</td>\n",
              "      <td>6114</td>\n",
              "      <td>6069</td>\n",
              "      <td>6068</td>\n",
              "      <td>6064</td>\n",
              "      <td>6062</td>\n",
              "      <td>0.992640</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>0.991822</td>\n",
              "      <td>0.991495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>L:/Videodata/issue/received/3.mp4</td>\n",
              "      <td>1300</td>\n",
              "      <td>6114</td>\n",
              "      <td>5729</td>\n",
              "      <td>5686</td>\n",
              "      <td>5668</td>\n",
              "      <td>5649</td>\n",
              "      <td>0.937030</td>\n",
              "      <td>0.929997</td>\n",
              "      <td>0.927053</td>\n",
              "      <td>0.923945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>L:/Videodata/issue/received/4.mp4</td>\n",
              "      <td>1300</td>\n",
              "      <td>6114</td>\n",
              "      <td>6069</td>\n",
              "      <td>6068</td>\n",
              "      <td>6064</td>\n",
              "      <td>6062</td>\n",
              "      <td>0.992640</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>0.991822</td>\n",
              "      <td>0.991495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                               name  ...      40/F      39/F\n",
              "0           0  L:/Videodata/issue/received/0.mp4  ...  0.958456  0.956984\n",
              "1           1  L:/Videodata/issue/received/1.mp4  ...  0.825973  0.823029\n",
              "2           2  L:/Videodata/issue/received/2.mp4  ...  0.991822  0.991495\n",
              "3           3  L:/Videodata/issue/received/3.mp4  ...  0.927053  0.923945\n",
              "4           4  L:/Videodata/issue/received/4.mp4  ...  0.991822  0.991495\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvgt5RQ1Wx5"
      },
      "source": [
        "X_issue = issue[['42/F','41/F','40/F','39/F','Condition']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8J6gwcX3K5C",
        "outputId": "324cd04f-7fed-4771-e116-f5be57343bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_issue[['42/F','41/F','40/F','39/F','Condition']] = scaler.fit_transform(X_issue[['42/F','41/F','40/F','39/F','Condition']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[item] = s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvyiuegQ1chW"
      },
      "source": [
        "pr = model.predict(X_issue, batch_size=3, verbose=0, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqWVJPgX1kNi",
        "outputId": "5017f462-5448-4903-df04-ef9b9ad31768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pp = pr.argmax(axis=1)\n",
        "pp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 5, 0, 5, 5, 5, 5, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbWWUDHe1k_9",
        "outputId": "50e419f3-3f6d-4794-9294-60a103de9464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yp = []\n",
        "for i in pp:\n",
        "  if i == 0:\n",
        "    yp.append(0.001)\n",
        "  elif i == 1:\n",
        "    yp.append(0.01)\n",
        "  elif i == 2:\n",
        "    yp.append(0.025)\n",
        "  elif i == 3:\n",
        "    yp.append(0.05)\n",
        "  elif i == 4:\n",
        "    yp.append(0.1)\n",
        "  else:\n",
        "    yp.append(0.25)\n",
        "yp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.001, 0.001, 0.25, 0.001, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    }
  ]
}